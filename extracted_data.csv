Advisor,Author,Date,Abstract,URL,Discipline,Keywords,Title,Type
"Stephen Gilbert,Vasant G. Honavar","Bonansea, Lucas",2018-08-11T14:21:52.000,"<p>The increasing number of new and complex computer-based applications has generated a need for a more natural interface between human users and computer-based applications. This problem can be solved by using hand gestures, one of the most natural means of communication between human beings. The difficulty in deploying a computer vision-based gesture application in a non-controlled environment can be solved by using new hardware which can capture 3D information. However, researchers and others still need complete solutions to perform reliable gesture recognition in such an environment.</p>
<p>This paper presents a complete solution for the one-hand 3D gesture recognition problem, implements a solution, and proves its reliability. The solution is complete because it focuses both on the 3D gesture recognition and on understanding the scene being presented (so the user does not need to inform the system that he or she is about to initiate a new gesture). The selected approach models the gestures as a sequence of hand poses. This reduces the problem to one of recognizing the series of hand poses and building the gestures from this information. Additionally, the need to perform the gesture recognition in real time resulted in using a simple feature set that makes the required processing as streamlined as possible.</p>
<p>Finally, the hand gesture recognition system proposed here was successfully implemented in two applications, one developed by a completely independent team and one developed as part of this research. The latter effort resulted in a device driver that adds 3D gestures to an open-source, platform-independent multi-touch framework called Sparsh-UI</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25035,Computer Sciences,"3D Gesture,Hand Recognition,HCI,SVM,ZCam",3D Hand gesture recognition using a ZCam and an SVM-SMO classifier,thesis
N/A,"Qing, Fanghui",2020-11-13T19:29:36.000,"<p>Prolonging network lifetime and retaining maximum communication fidelity are important to many applications of ad-hoc wireless sensor networks. Many energy-efficient communication protocols have been proposed to allow as many sensors as possible to be in idling. Typically, these techniques reduce energy consumption by minimizing the number of transmission packets and the size of each packet. However, recent researches have shown that energy consumed by the sensors in idling state is not negligible. In this research, we address this problem with a novel Backbone-based Communication Scheduling (BCS) technique. This scheme reduces the idling energy dissipation by keeping only a small set of sensors active at any time and leaving the rest of them in sleeping. The active sensors form a communication backbone that maintains the communication fidelity of the entire network. The backbone nodes are rotated with a highly efficient backbone election algorithm to balance the energy consumption of the sensors in the whole network. Our simulations results show that the proposed scheme can significantly extend the network lifetime without compromising the communication fidelity.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97141,N/A,Computer science,A backbone-based communication scheduling scheme for wireless sensor networks,thesis
"Baskar Ganapathysubramanian,Soumik Sarkar","Chawla, Vikas",2018-08-11T12:08:10.000,"<p>Machine learning has become a popular technology that has not only turbo-charged the existing problems in the AI but it has also emerged as the powerful toolkit to solve some of the interesting problems across the various interdisciplinary domains.</p>
<p>The availability of food is the biggest problem of the 21st century and many experts have raised their concerns as we continue to see a rise in the global human population. There have been many efforts in this direction which include but not limited to improvement in the seeds quality, good management practices, prior knowledge about the expected yield, etc.</p>
<p>In this work, we propose a data-driven approach that is ‘gray box’ i.e. that seamlessly utilizes expert knowledge in constructing a statistical network model for corn yield forecasting. Our multivariate gray box model is developed on Bayesian network analysis to build a Directed</p>
<p>Acyclic Graph (DAG) between predictors and yield. Starting from a complete graph connecting various carefully chosen variables and yield, expert knowledge is used to prune or strengthen edges connecting variables. Subsequently, the structure (connectivity and edge weights) of the DAG that maximizes the likelihood of observing the training data is identified via optimization. We curated an extensive set of historical data (1948 − 2012) for each of the 99 counties in Iowa as data to train the model. We discuss preliminary results, and specifically focus on (a) the structure of the learned network and how it corroborates with known trends, and (b) how partial information still produces reasonable predictions (predictions with gappy data), and show that incorporating the missing information improves predictions.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/30076,"Agriculture,Computer Sciences,Plant Sciences,Statistics and Probability","Bayesian network,Corn yield prediction,Expert knowledge,Historical yield data,Machine learning,Probabilistic graphical model",A Bayesian network approach to county-level corn yield prediction using historical data and expert knowledge,thesis
N/A,"Teng-amnuay, Yunyong",2018-08-15T08:08:21.000,"<p>The problem of concurrency control in distributed databases is very complex. As a result, a great number of control algorithms have been proposed in recent years. This research is aimed at the development of a viable categorization scheme for these various algorithms. The scheme is based on the theoretical concept of serializability, but is qualitative in nature. An important class of serializable execution sequences, conflict-preserving-serializable, leads to the identification of fundamental attributes common to all algorithms included in this study. These attributes serve as the underlying philosophy for the categorization scheme. Combined with the two logical approaches of prevention and correction of nonserializability, the result is a flexible and extensive categorization scheme which accounts for all algorithms studied and suggests the possibility of new algorithms.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/80644,Computer Sciences,Computer science,A categorization scheme for concurrency control protocols in distributed databases,dissertation
N/A,"Kwinn, Kathryn",2018-08-15T10:06:31.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/79397,Computer Sciences,Computer science,A comparison of automata-theoretic and algebraic approaches to tree transduction and use of algebraic tree transducers in semantic-preserving translations,dissertation
N/A,"Dantuluri, Varma",2020-11-22T06:44:13.000,"<p>Static architecture learning algorithms have the architecture fixed. The amount of learning and generalization does depend on the architecture trained on. Therefore static architecture algorithms require a careful selection of architecture before training. But how good, an architecture is, is not known until after the training is done. To overcome this shortcoming of static architecture learning algorithms, a Cost Function Based Dynamic Node Architecture Algorithm (DNAC) is proposed, which dynamically changes the architecture of the network and finds the one with high learning and generalization capability. The DNAC algorithm starts with a minimal architecture and stops at a good architecture for the data to be learned. The Scaled Conjugate Gradient Algorithm (Moller, 1993) is used as the underlying learning algorithm. The RMS error on a cross validation set is used as the measure of generalization. A computationally simple nodal and layer importance functions are proposed. The results on two benchmark problems and a real world problem are presented.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/98526,,Computer science,A cost function based dynamic node architecture algorithm,thesis
"Zhang, Zhu,Tian, Jin,Tavanapong, Wallapak,Li, Qi,Bao, Forrest","Gupta, Amulya",N/A,"Availability of immense amount of unstructured text has surged the demand for intelligent natural language processing (NLP) systems. Specifically, these systems are expected to represent, extract, transfer and utilize knowledge in order to be successful on language related problems. In this work, we have directed our efforts to explore various forms of knowledge, especially linguistic and latent structures, available for a NLP system.
Additionally, we investigate the significance of aforementioned structures in various language relevant tasks ranging from highly researched semantic matching to less explored abstract text summarization, and many more.

In chapter 2, we study the interaction between explicit linguistic structures and implicit structure revealed by attention mechanisms. This investigation on various semantic matching datasets, especially question-answering, display significant patterns of substitutability between the two types of structures (explicit and implicit), and may affect other modalities. As a result, we develop a multi-view progressive attention mechanism which is general enough to operate on various linguistic structures of text.

Moving further, to represent and extract latent structure from a collection of documents, we propose a family of vector-quantization-based topic models (VQ-TMs) in Chapter 3. Specifically, VQ-TMs consider dense and global topic embeddings which are homomorphic to word embeddings. Moreover, the family exploits vector-quantization (VQ) technique to promote discreteness in the overall architecture. Lastly, the learned topics are successfully transferred to a different task of code-generation.

VQ-TMs exploit VQ technique while learning topics representation. Although VQ method aids in learning discrete topic embeddings, yet, the method is built upon a geometric intuition. Moreover, VQ-TMs utilizes the learned topic representations as reference knowledge in a downstream task. Some natural questions arise. Is it possible to exploit probabilistic intuition to infuse discreteness while learning topics representation? Additionally, can the learned topics be leveraged to control sentence generation and augment the performance in aspect-aware item recommendation? In order to push the envelope further, we explore these ideas in Chapter 4.",https://dr.lib.iastate.edu/handle/20.500.12876/nrQBKLWz,Computer science,"artificial intelligence,deep learning,machine learning,natural language processing,nlp,structure nlp",A deep learning odyssey on structured natural language processing,dissertation
Leslie Miller,"Alabsi, Mohammed",2018-08-23T08:51:48.000,"<p>During the last few years, the number and size of biological databases have increased dramatically. Existence of these databases has been significant to recent biological research, providing biologist with an increasingly large and valuable set of data. Still, current biological databases have some shortcomings. A single data source does not usually have all the data a biologist needs. Also, biological databases are normally subject specific, containing data about a certain biological discipline. Hence, providing biologists with a single source of data gathered from multiple sources and covering many subjects would provide them with the breadth and depth of data needed for many biological studies. Such source should also include the needed tools to conduct necessary analytical studies on the integrated data.;A system model is introduced that gathers integrated data from multiple diverse biological databases, enables biology labs to share their data with each other, and provides support for analytical tools relevant to biological research of users. External tools can also be attached to the platform using the notion of plug-ins. The design of the system is provided and the implementation details are explained.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/68207,"Bioinformatics,Computer Sciences",Computer science,A distributed system for integrating and sharing biology data and tools,thesis
"Carl K. Chang,Andrew S. Miner","Jiang, Hsin-yi",2018-08-11T16:28:46.000,"<p>Genetic Algorithms (GAs) have been gradually identified as an optimization-problem solver for certain classes of real-world applications. As GAs are increasingly utilized, a foundational</p>
<p>study on how well GAs can perform with respect to varying problem domains becomes crucial. Yet, none of the prevalent theoretical studies are built upon the linkage between the theory and application of GAs. This dissertation introduces a methodology for estimating the</p>
<p>applicability of a GA configuration for an arbitrary optimization problem based on run-time data. More specifically, this work analyzes the convergence behavior within a finite number of generations for each GA run through the estimation of the trace of the transition matrix of the</p>
<p>corresponding Markov chain from run-time data. The analytical and empirical results show that the methodology is helpful for evaluating the applicability of GAs to optimization problems. Through the methodology, the number of generations needed for empirical convergence</p>
<p>with respect to a fitness function (or a problem) can be estimated. The proposed methodology entails an evaluation metric and connects theory to application of GAs, for estimating the applicability of a GA to a problem. The methodology is demonstrated through a case study on evolutionary testing.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25290,Computer Sciences,"Applicability,Evaluation Metric,Genetic Algorithm,Markov Chain,Optimization,Trace",A framework for estimating the applicability of GAs for real-world optimization problems,dissertation
"Wallpak Tavanapong,Johnny Wong","Saggi, Anand",2018-08-11T13:18:25.000,"<p>Fast Forward Motion Pictures Expert Group (FFmpeg) is a well-known, high performance, cross platform open source library for recording, streaming, and playback of video and audio in various formats, namely, Motion Pictures Expert Group (MPEG), H.264, Audio Video Interleave (AVI), just to name a few. With FFmpeg current licensing options, it is also suitable for both open source and commercial software development. FFmpeg contains over 100 open source codecs for video encoding and decoding.</p>
<p>Given the complexities of MPEG standards, FFmpeg still lacks a framework for (1) seeking to a particular image frame in a video, which is needed for accurate annotation at the frame level for applications in fields such as medical domain, digital communications and commercial video broadcasting and (2) motion vectors extraction for analysis of motion patterns in video content. Most importantly, FFmpeg code base is not well documented, which has raised a significant difficulty for developing an extension.</p>
<p>As our contributions, we extended FFmpeg code base to include new APIs and libraries support accurate frame-level seek, motion vector extraction, and MPEG-2 video encoding/decoding. We documented FFmpeg MPEG-2 codec to facilitate future software development. We evaluated the performance of our implementation against a high-performance third-party commercial software development kit on videos captured from television broadcasts and from endoscopy procedures. To evaluate the usability of our libraries, we integrated them with some commercial applications. In the following sections, we will discuss our software architecture, important implementation details, performance evaluation results, and lessons learned.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25961,Computer Sciences,"Encoding,FFmpeg,Frame Accurate Seek,Motion Vector Extraction,MPEG2,Video Playback",A framework for multimedia playback and analysis of MPEG-2 videos with FFmpeg,thesis
Josy M. Reyes Ylamo,"Reyes Alamo, Jose",2018-08-11T11:43:00.000,"<p>The Service Oriented Computing (SOC) paradigm, defines services as software artifacts whose implementations are separated from their specifications. Application developers rely on services to simplify the design, reduce the development time and cost. Within the SOC paradigm, different Service Oriented Architectures (SOAs) have been developed. These different SOAs provide platform independence, programming-language independence, defined standards, and network support. Even when different SOAs follow the same SOC principles, in practice it is difficult to compose services from heterogeneous architectures. Automatic the process of composition of services from heterogeneous SOAs is not a trivial task.</p>
<p>Current composition tools usually focus on a single SOA, while others do not provide mechanisms for ensuring safety of composite services and their interactions. Given that some services might perform critical operations or manage sensitive data, defining safety for services and checking for compliance is crucial. This work proposes and workflow specification language for composite services that is SOA-independent. It also presents a framework for automatic composition of services of heterogeneous SOAs, supporting web services (WS) and OSGi services as an example. It integrates formal software analysis methods to ensure the safety of composite services and their interactions. Experiments are conducted to study the performance of the composite service generated automatically by the framework with composite services using current composition methods. We use as an example a smart home composite service for the management of medicines, deployed in a regular and in a resource-constrained network environment.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25651,Computer Sciences,"Formal Methods,Healthcare IT,Requirements Engineering,Service Oriented Architecture,Smart Homes,Software Engineering",A framework for safe composition of heterogeneous SOA services in a pervasive computing environment with resource constraints,dissertation
Samik Basu,"Gunasekharan, Maheedhar",2018-08-11T11:59:57.000,"<p>Over the past decades, cyber attacks have grown in frequency as well as in sophistication.</p>
<p>Often, they elude the counter-measures that are in place due to inadequate expert man-power</p>
<p>that is necessary to manually deploy the correct responses and maintain systems being compromised.</p>
<p>We present a decision support framework to aid in timely deployment and maintenance</p>
<p>of effective responses when intrusive or malicious behavior is detected.</p>
<p>The support framework has two specific objectives: to identify the best set of responses given</p>
<p>the knowledge of the attack and the system being protected; and to identify the minimal set of</p>
<p>responses that must be deployed. While appropriateness of responses is of utmost importance</p>
<p>to safeguard systems from attacks, minimality in the number of responses, an important factor</p>
<p>from the deployment and maintainability perspective, has often been discarded. Our framework</p>
<p>leverages National Vulnerability Database as a source for information about the attacks, relies</p>
<p>on the pre-specified expert knowledge about the responses that can adequately stop attack</p>
<p>and takes into considerations the impact of an attack as well as responses on the system being</p>
<p>protected in terms of well-studied CIA (Confidentiality, Integrity and Availability) vector.</p>
<p>We utilize Trade-off Enhanced Conditional Preference Network (TCP-net) to qualitatively</p>
<p>represent and reason about the CIA priorities of the expert and model the problem of identifying</p>
<p>minimal set of most effective responses into a search problem. The choice of TCP-net stems</p>
<p>from the fact that the CIA priorities are typically qualitative in nature and it has been proven</p>
<p>that quantification of priorities that are inherently qualitative can result in incorrect and often</p>
<p>unexplainable results due to seemingly small perturbations in quantitative measures. Our TCPnet</p>
<p>based computation can generate provably optimal solution where optimality corresponds</p>
<p>to minimality of selected responses. While optimality is an important factor, the necessity</p>
<p>for computing the solution efficiently cannot be overstated, particularly in the context where</p>
<p>timeliness in response deployment is equally important. We investigate and evaluate several</p>
<p>ix</p>
<p>heuristics with the goal of searching part of the potentially large solution space and compute</p>
<p>a solution that is ”close” to the optimal solution. We discuss the relative advantages and</p>
<p>disadvantages of each heuristic, and present a specific one that is efficient in computing the</p>
<p>optimal solution.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29494,Computer Sciences,"Formal security methods,Intrusion response systems,Security and privacy",A Framework for Selecting the Minimal Set of Preferred Responses to Counter Detected Intrusions,thesis
N/A,"White, Ralph",2018-08-15T13:04:58.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/78511,Computer Sciences,Computer science,A graph model for coordinating systems of tasks,dissertation
N/A,"Gieseler, Charles",2020-11-09T01:09:19.000,"<p>This work details a machine learning tool developed to support computational, agent-based simulation research in the social sciences. Specifically, the Java Reinforcement Learning Module (JReLM) is a platform for implementing reinforcement learning algorithms for use in agent-based simulations. The module was designed for use with the Recursive Porous Agent Simulation Toolkit (Repast), an agent-based simulation platform popular in computational social science research. Background, architecture, and implementation of JReLM are discussed within. This includes explanation of pre-implemented tools and algorithms available for immediate use in Repast simulations. In addition, an account of JReLM's use in an agent-based computational economics simulation is included as an illustrative application. Directions for further development and future use in ongoing agent-based computational economics work are discussed as well.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97918,,Computer science,A Java Reinforcement Learning Module for the Recursive Porous Agent Simulation Toolkit: facilitating study and experimentation with reinforcement learning in social science multi-agent simulations,thesis
N/A,"Elliott, Donald",2018-08-24T18:28:09.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/77403,Computer Sciences,"Programming languages (Electronic computers),Electronic digital computers--Design and construction,Simulation methods",A language to describe and simulate multiprocessor computer systems,dissertation
N/A,"Lee, Kyongryun",2020-06-17T02:39:14.000,"<p>In this thesis, we address the problem of estimating count queries on databases quickly, without accessing the database at query time. We accomplish that by building a model of the domain from the database in a preprocessing phase, and use this to answer count queries. The model we use is the Mixture Model of Bayesian Networks (MMBN), which effectively encodes the joint probability distribution of the domain. An MMBN is a weighted model with Bayesian Networks (BNs) as components. We describe how to learn an MMBN model from a database using an instance of the modified Expectation-Maximization (EM) algorithm, called EAM algorithm, and evaluate its accuracy on real and artificial data sets. Experimental results show that MMBNs can represent a data set satisfactorily and can approximate counts with the high degree of accuracy, without accessing the database.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/73133,,Computer science,A Mixture Model of Bayesian Networks,thesis
Shashi K. Gadia,"Tao, Jia",2018-08-24T20:06:34.000,"<p>A database consists of an object space that is information about objects in (some part of) the real world. Every object existing in the real world has properties, including an identity that uniquely distinguishes it from other objects. In a belief database multiple subjects are hypothesized. Subjects have varying beliefs about existence, identities, and other properties of objects. A multilevel security database is a belief database where the subjects form a hierarchy. The upper users can see the object space of lower users but the lower users cannot. In fact lower users may not even be aware of upper users. This paper presents a skeleton based model for storage and query of a multilevel security database. This model follows the parametric approach proposed by Gadia. As in all works in the parametric approach, the central focus of this framework is to support most natural query of data.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/81828,Computer Sciences,Computer science,A model for storage and query of cascading beliefs in multilevel security database,thesis
"Chang, Carl,Tian, Jin,Mitra, Simanta,Aduri, Pavankumar,Rajan, Hridesh","Sun, Peng",N/A,"Non-Functional Requirements (NFRs) have a great impact on all the downstream activities in a software's entire life-cycle. It is important to capture newly emerged NFRs for system evolution. NFRs are treated as quality goals in Goal Oriented  Requirements  Engineering. Under the concept of the goal, the interrelations among users, system agents, and scenarios are complex, so the quality factors related to users' concerns cannot be easily studied. In this study we distill human factor from the concept of goal and deliberately represent NFRs through contribution relations among human desires. By doing so, NFRs can be better exploited from humans' perspective.   We noticed that recent work in the area of requirements engineering shows that through the combinational use of  goal inference, user behavioral and system contextual data functional requirement can be elicited in the form of task-level alternative features. Our basic assumption is that there is a chance to extract new NFRs from user' mental states, specifically their desires, because the concepts of goal and desire are closely connected. A statistical model under the Situ framework is proposed to infer human desires of multiple levels of abstraction from contextual data. In our multi-layered desire inference method, we consider inference results and try to make sense of results with different levels of inference confidence. We ran a case study to show how to elicit users' new NFRs in three cases from the contributing relations among desires in different abstraction levels. To bridge desires and goals in requirements engineering and apply the elicited NFRs on the practical system evolution, we extended the i* framework to build the dependency relations between desire contributing relations and system tasks. Several implications of this work are also discussed.",https://dr.lib.iastate.edu/handle/20.500.12876/nrQBOV0z,Computer science,"Conditional Random Fields,Desires Inference,Nonfunctional Requirement,Software Engineering",A multi-layered desires based framework to detect evolving non-functional requirements of users,dissertation
Akhilesh Tyagi,"Krishnamurthy, Viswanath",2018-08-11T15:03:13.000,"<p>A novel thread scheduler design for polymorphic embedded systems</p>
<p>Abstract:</p>
<p>The ever-increasing complexity of current day embedded systems necessitates that these systems be adaptable and scalable to user demands. With the growing use of consumer electronic devices, embedded computing is steadily approaching the desktop computing trend. End users expect their consumer electronic devices to operate faster than before and offer support for a wide range of applications. In order to accommodate a broad range of user applications, the challenge is to come up with an efficient design for the embedded system scheduler. Hence the primary goal of the thesis is to design a thread scheduler for a polymorphic thread computing embedded system. This is the first ever novel attempt at designing a polymorphic thread scheduler as none of the existing or conventional schedulers have accounted for thread polymorphism. To summarize the thesis work, a dynamic thread scheduler for a Multiple Application, Multithreaded polymorphic system has been implemented with User satisfaction as its objective function. The sigmoid function helps to accurately model end user perception in an embedded system as opposed to the conventional systems where the objective is to maximize/minimize the performance metric such as performance, power, energy etc. The Polymorphic thread scheduler framework which operates in a dynamic environment with N multithreaded applications has been explained and evaluated. Randomly generated Application graphs are used to test the Polymorphic scheduler framework. The benefits obtained by using User Satisfaction as the objective function and the performance enhancements obtained using the novel thread scheduler are demonstrated clearly using the result graphs. The advantages of the proposed greedy thread scheduling algorithm are demonstrated by comparison against conventional thread scheduling approaches like First Come First Serve (FCFS) and priority scheduling schemes.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25537,Computer Sciences,"Computer Architecture,Embedded Systems,Operating Systems,Polymorphic Computing,Scheduler Design,User satisfaction",A Novel Thread Scheduler Design for Polymorphic Embedded Systems,thesis
Wensheng Zhang,"Fong, Michael",2018-08-11T12:31:27.000,"<p>RFID system has become a technology that many companies would like to adopt as it provides convenience to our society. Increasing competitiveness in the industry has reduced the cost of the technology, but it also raises considerable privacy concerns. Among many RFID applications that have made major impacts on various industries, the RFID-based authentication system has become a solution to resolve a number of issues, for instances, theft, compromised brand, lack of inventory control, supply chain inefficiencies, and etc. Many authentication protocols based on symmetric challenge-response scheme have been developed in order to ensure preservation of privacy. However, many of the these schemes cannot fully protect privacy in the presence of malicious readers or insider attacks.</p>
<p>In this thesis, we first investigated possible security and privacy threats that security engineers face from an RFID system. We then presented a new protocol to authenticate smart tags without exposing their private identities and activity patterns with resource-limited devices, such as RFID smart tags or wireless sensor nodes. We further analyzed the RFID system's security strength against various attacking scenarios, such as eavesdropping, collusive attack or tag-compromise situation, based on extensive experiments to validate the feasibility and security of our proposed solution.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25854,Computer Sciences,,A privacy-preserving authentication protocol for smart tags,thesis
Shashi K. Gadia,"Nair, Sunil",2018-08-23T16:25:22.000,"<p>In temporal database systems the time varying aspects of data are captured by time-stamping data values. Research in temporal databases has concentrated on developing models in which it is essential that all the information be known;In the present work a relational model for incomplete information is presented. The model allows incomplete temporal information to be stored, and provides a powerful, yet simple, algebra to query the incomplete information;The incomplete information model presented here generalizes a well-known model for temporal databases with complete information. The algebraic expressions in the model produce results that are reliable in the sense that they never report incorrect information. This is shown by introducing the notion of completions of relations and databases. It is also shown that except for certain cases of selection, if the definition of the operators were strengthened to give more information, we could obtain results that are not reliable. This result is obtained by introducing the concepts of extensions of relations and more informative relations;Update operations create, change, and changekey are defined. These operations allow the user to modify the state of the database to reflect changes in the real world, to correct errors in the database, and to increase the information content of incomplete objects as more information becomes available.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/63292,Computer Sciences,Computer science,A relational model for incomplete information in temporal databases,dissertation
Gary T. Leavens,"Cheon, Yoonsik",2018-08-24T19:58:14.000,"<p>The Java Modeling Language (JML) is a formal behavioral interface specification language (BISL) for Java. JML has many advances including specification-only declarations, specifications of interfaces, stateful interfaces, multiple inheritance of specifications, and behavioral subtyping. An approach to runtime assertion checking of JML assertions is presented and implemented as a JML compiler to translate Java programs annotated with JML specifications into Java bytecode. The compiled bytecode transparently checks JML specifications at runtime. The JML compiler supports separate and modular compilation. The approach brings programming benefits such as debugging and testing to BISLs and also helps programmers to use BISLs in their daily programming.;A set of translation rules are defined from JML expressions into assertion checking code. The translation rules handle various kinds of undefinedness, such as runtime exceptions and non-executable constructs, in such a way as to both satisfy the standard rules of logic and detect as many assertion violations as possible. The rules also support various forms of quantifiers. Specification-only declarations such as model fields, ghost fields, and model methods are translated into access methods; e.g., an access method for a model field is an abstraction function that calculates an abstract value from the program state. The specification state of a stateful interface, due to specification-only fields such as ghost fields, is represented as a separate assertion class. Thus, an object's specification state is distributed over the object itself and one assertion object for each interface that its class implements. Assertion checking is also distributed in that a subtype delegates the responsibility of checking inherited specifications to its supertypes (or the assertion classes of its superinterfaces). The delegation approach supports multiple inheritance, and is modular.;Finally, the effectiveness and practicality of runtime assertion checking is demonstrated by applying it to program testing. An approach is implemented that significantly automates unit testing. The key idea of the approach is to view interface specifications as test oracles and to use the runtime assertion checker as the decision procedure of the test oracles. The approach also shows that the runtime assertion checker can be an effective framework for developing specification-based tools.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/78385,Computer Sciences,Computer science,A runtime assertion checker for the Java Modeling Language,dissertation
"Jannesari, Ali,Zhang, Wensheng,Cai, Ying","Bengre, Vivek LN",N/A,"Many algorithms have been designed to break a large job into manageable/parallelizable bits. The smaller job chunks are structured as a directed acyclic graph (DAG) to link the dependencies together.  The challenge of such a framework is the complexity in scheduling each stage/node in a collection of jobs/DAGs.  This problem is especially pronounced for High-Frequency job arrivals, as selecting a node/stage and allocating executors to minimize the average job completion time(JCT) is an NP-hard problem.  Since the number of executors is always limited, having multiple executors assigned to one job when there is a stream of incoming jobs can worsen the average job completion time of a given job set.  This thesis shows that we can derive highly efficient policies by allocating a unit of computing power across executable stages/nodes in conjunction with a learned scheduling model.  We utilize reinforcement learning(RL) and graph neural networks(GNN) to learn patterns in complex job graphs and reduce overall job completion time.  Our experiments show up to 20% improvement in job completion time compared todecima[Mao et al. (2018)] and a tuned heuristic-based scheduler.  Our experiments also show better, more efficient resource allocation and utilization.",https://dr.lib.iastate.edu/handle/20.500.12876/KrZJRMbr,Computer science,"Apache Spark,Graph Neural Networks,High Frequency Job Arrival,Machine Learning,Optimization,Streaming",A scheduler for high frequency job arrival using graph neural networks,thesis
N/A,"Campbell, Jerry",2018-08-15T05:42:49.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/78748,Computer Sciences,Computer science,A software organization for the control of multiple processes,dissertation
"Jin Tian,Doug Jacobson,Shashi Gadia","Liu, Lexin",2018-08-22T17:05:01.000,"<p>Knowing the cause and effect is important to researchers who are interested in modeling the effects of actions. One commonly used method for modeling cause and effect is graphical model. Bayesian Network is a probabilistic graphical model for representing and reasoning uncertain knowledge. A common graphical causal model used by many researchers is a directed acyclic graph (DAG) with causal interpretation known as the causal Bayesian network (BN). Causal reasoning is the causal interpretation part of a causal Bayesian Network. They enable people to find meaningful order in events that might otherwise appear random and chaotic. Further more, they can even help people to plan and predict the future. We develop a software system, which is a set of tools to solve causal reasoning problems, such as to identify unconditional causal effects, to identify conditional causal effects and to find constraints in a causal Bayesian Networks with hidden variables.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/69068,Computer Sciences,Computer science,A software system for causal reasoning in causal Bayesian networks,thesis
"Gary T. Leavens,Krishna Rajan,Hridesh Rajan","Taylor, Kristina",2018-08-23T11:27:56.000,"<p>Design by contract specification languages help programmers write their intentions for a piece of code in a formal mathematical language.  Most programming languages do not have built-in syntax for such specifications, so many design by contract languages place specifications in comments.  The Java Modeling Language (JML) is one such specification language for Java that uses comments to specify contracts.  However, starting with version 5, Java has introduced annotations, a syntactical structure to place metadata in various places in the code.  This thesis proposes an initial design to writing JML contracts in the Java 5 annotation syntax and evaluates several criteria in the areas of specification languages and Java language design: whether these annotations are expressive enough to take advantage of annotation simplicity and tool support, and whether the annotation syntax is expressive enough to support handling a large specification language such as JML.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/68508,Computer Sciences,Computer science,A specification language design for the Java Modeling Language (JML) using Java 5 annotations,thesis
"Le, Wei,Mitra, Simanta,Zhang, Wensheng","Guo, Xiuyuan",N/A,"Static analysis is widely used for software assurance. However, static analysis tools can report
an overwhelming number of warnings, many of which are false positives. Applying the static
analysis to a new version can result in a large number of warnings that are only relevant to the
old version. Inspecting these warnings is time-consuming and can hinder developers from finding
new bugs in the new version. We report the challenges of cascading warnings generated from two
versions of programs. We investigated program differencing tools and extended them to perform
warning cascading automatically. Specifically, we used the textual-based diff tool, namely SCALe,
abstract syntax tree (AST) based diff tool, namely GumTree, and control flow graph (CFG) based
diff tool, namely Hydrogen. We reported our experience of applying these tools, and hopefully,
our findings can help developers understand of pros and cons of each approach. In our evaluation,
we used 96 pairs of benchmark programs for which we know ground-truth bugs and fixes as well
as 12 pairs of real-world open-source projects. Our tools and data are available at
https://github.com/WarningCas/WarningCascading_Data.",https://dr.lib.iastate.edu/handle/20.500.12876/GvqXLoLw,Computer science,"Software Engineer,Static Analysis,Warning Cascading",A study of static warning cascading tool,thesis
"Jannesari, Ali,Chang, Carl,Le, Wei","Phan, Hung Dang",N/A,"Neural Machine Translation (NMT) is the current trend approach in Natural Language Processing (NLP) to solve the problem of automatically inferring the content of target language given the source language. The ability of NMT is to learn deep knowledge inside languages by deep learning approaches. However, prior works show that NMT has its own drawbacks in NLP and in some research problems of Software Engineering (SE). In this work, we provide a hypothesis that SE corpus has inherent characteristics that NMT will confront challenges compared to the state-of-the-art translation engine based on Statistical Machine Translation. We introduce a problem which is significant in SE and has characteristics that challenges the ability of NMT to learn correct sequences, called Prefix Mapping. We implement and optimize the original SMT and NMT to mitigate those challenges. By the evaluation, we show that SMT outperforms NMT for this research problem, which provides potential directions to optimize the current NMT engines for specific classes of parallel corpus. By achieving the accuracy from 65% to 90% for code tokens generation of 1000 Github code corpus, we show the potential of using MT for code completion at token level.",https://dr.lib.iastate.edu/handle/20.500.12876/arY48LVv,Computer science,"Attention Model,Deep Learning,Language Model,Neural Machine Translation,Prefix Resolution,Statistical Machine Translation",A study on the performance of statistical machine translation and neural machine translation for prefix resolution in software engineering,thesis
Johnny S. Wong,"Wang, Xia",2018-08-11T18:22:15.000,"<p>Wireless mesh networking has emerged as a key technology to provide</p>
<p>wide-coverage broadband networking. It benefits both service</p>
<p>providers with low cost in network deployment, and end users with</p>
<p>ubiquitous access to the Internet from anywhere at anytime. Wireless</p>
<p>mesh networks are vulnerable to malicious attacks due to the nature</p>
<p>of wireless communication and the lack of centralized network</p>
<p>infrastructure. Meanwhile, the capacity of multi-radio multi-channel</p>
<p>communication, the need for heterogeneous network integration, and</p>
<p>the demand for multi-hop wireless communication often make</p>
<p>traditional security mechanisms inefficient or infeasible.</p>
<p>Therefore, wireless mesh networks pose new challenges and call for</p>
<p>more effective and applicable solutions.</p>
<p>In this work, we identify the requirement for a systematic security</p>
<p>framework to protect wireless mesh networks and provide a security</p>
<p>system with heterogeneity-aware intrusion prevention mechanism,</p>
<p>cross-layer based intrusion detection technique, and a generic</p>
<p>intrusion response model.</p>
<p>Our major contributions lie in the following: (1) We identify the</p>
<p>architecture heterogeneity of wireless mesh networks and proposed a</p>
<p>novel heterogeneity-aware group key management framework which</p>
<p>combines the logical key hierarchical technique together with the</p>
<p>localized threshold-based technique. (2) To leverage link-aware</p>
<p>routing characteristics, we present a cross-layer based anomaly</p>
<p>detection model which utilizes machine learning algorithms for</p>
<p>profile training and intrusion detection. (3) We address the</p>
<p>automatic intrusion response problem in wireless mesh network by</p>
<p>providing a generic response model to describe the dependency of</p>
<p>system services and resources. The dependency graph is later used</p>
<p>for damage cost assessment and response cost evaluation. (4) We</p>
<p>build a wireless mesh network testbed and implemented a system</p>
<p>prototype for intrusion detection system. Our simulation and</p>
<p>experiment results show that our solutions outperform existing ones</p>
<p>and are practical for wireless mesh networks in terms of</p>
<p>communication overhead and performance speed.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24935,Computer Sciences,"Group Key Management,Intrusion Detection,Intrusion Response,Network Security,Wireless Ad Hoc Networks,Wireless Mesh Network",A Systematic Security Approach in Wireless Mesh Networks,dissertation
"Rajan, Hridesh,Stevens, Clay,Dyer, Robert","Thomas, Deepak George",N/A,"Reinforcement Learning (RL) is increasingly being used for a variety of applications, some of which include safety-critical tasks. Therefore, it is important for developers to be aware of faults that occur when writing programs that leverage RL. This thesis conducts an empirical study on the faults associated with RL programs. Our work is tailored towards developers who use well-vetted frameworks or implementations of RL algorithms.

We conduct studies to identify relevant RL tags to mine StackExchange and a popular RL framework to mine GitHub. Then, we mine StackExchange and GitHub and after dropping unrelated posts using various automated techniques, we arrive at 2,787 posts, which we manually analyze. Finally, we manually label a portion of these posts to develop a taxonomy of RL faults.  

Our final taxonomy contains 8 categories of faults from 112 artifacts. We describe these categories and the corresponding child nodes in detail in this thesis. We compare our work against prior work done in this area by comparing each leaf node of theirs with our taxonomy.  

We present two promising future directions that could be derived from this work: Mutation Testing and Fault Localization. Both of these directions require a taxonomy of real RL faults, which our work provides.",https://dr.lib.iastate.edu/handle/20.500.12876/GvqXQkqw,"Computer science,Artificial intelligence","Fault Taxonomy,Reinforcement Learning",A taxonomy of reinforcement learning faults,thesis
Shashi K. Gadia,"Shetty, Soumya",2018-08-12T03:04:56.000,"<p>The use of B-trees for achieving good performance for updates and retrievals in databases is well-known. Many excellent implementations of B-trees are available as well. However it is difficult to find B-trees that are easily configured and deployed into experimental systems. We undertake an implementation of B-trees from scratch that specifically addresses configurability and deployablility issue. An XML file is used to store as well as document information such as page formats of the nodes of the B-trees and details about the nature of records and keys. The behavior of the tree is encapsulated by commands for creation of B-trees, insertions of records in the tree, and make retrievals via the tree. The XML based configuration together with commands make the deployment and functionality of the tree completely clear and straightforward.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25512,Computer Sciences,"B-tree,bulkloading,CanStoreX,indexing",A user configurable implementation of B-trees,thesis
Ali Jannesari,"Roghair, Jeremy",2020-06-26T19:59:59.000,"<p>Integration of reinforcement learning with unmanned aerial vehicles (UAVs) to achieve autonomous ﬂight has been an active research area in recent years. An important part focuses on collision detection and avoidance as a UAV navigates through an environment. In this thesis, we introduce a new variation of the Deep Q-Network (DQN) algorithm for UAV collision avoidance. Exploration with other variations of DQN for collision avoidance such as D3QN, are typically done through uniform sampling of actions, however, the challenge is environments inherently have sparse rewards resulting in many actions leading to redundant states. We focus on this problem of learning the dynamics of an unseen environment with sparse rewards more eﬃciently. To this end, we present an algorithm for improved exploration for UAVs. The approach is a guidance based method that uses a Bayesian Gaussian mixture model to compare previously seen states to a predicted next state in order to select the next action. Performance of these approaches was demonstrated in multiple simulation environments using Microsoft AirSim. The proposed algorithm demonstrates a two-fold improvement in average rewards compared to D3QN, after the ﬁrst 1000 training episodes.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/32167,,"collision avoidance,deep q-network,object detection,reinforcement learning,robotics,uav",A vision based DQN exploration algorithm for collision avoidance,thesis
"Mitra, Simanta,Prabhu, Gurpur,Chang, Carl K.","Pal, Saveri",N/A,"ABET accreditation is a certification of an educational program’s quality and ensures that a program continually meets specific criteria.  The process of creating an ABET accreditation report is  burdensome and time consuming. It requires several pieces of information from multiple sources to be collected manually and included in the report.  The current practice is error-prone and requires the effort of an entire faculty and administrative staff. While there have been previous efforts made to automate the evaluation of outcome assessments, little can be found on automating the data collection process. This work presents a web-based application software tool to automate data collection and report generation efforts in preparing an ABET accreditation document. Requirements for this application were gathered after several meetings with stakeholders of the ABET accreditation process. A combination of web crawling, data parsing and retrieving data from third-party applications such as Canvas LMS via API calls, have been used to gather information that is usually performed manually. Information collected include courses offered by the department, assignments of each course, student submissions, and their scores, among others.  The web application tool features an intuitive user interface that allows users to add courses and assignments manually or pull them from Canvas LMS, add outcomes to courses and map them to ABET outcomes, and generate course reports and tables with one click of a button. The tool has the potential to automate a large part of the accreditation process, reducing manual effort by a considerable amount.",https://dr.lib.iastate.edu/handle/20.500.12876/3wxaNaxv,Computer science,"ABET accreditation,automated data collection,automated report generation,Canvas LMS,web crawling,web user interface",A web application to assist in preparing for ABET accreditation,thesis
Shashi K. Gadia,"Narayanan, Valliappan",2018-08-11T17:12:06.000,"<p>This work focuses on a methodology to help bring many of our database artifacts and prototypes to reside on the top of a common workbench platform that leads to uniformity and removes overlap across different subsystems. A versatile command format has been developed to allow commands belonging to different subsystems to be interleaved in the same batch unambiguously. Through a collaborative effort carried on in parallel, existing GUIs (Graphical User Interfaces) have also been merged into a common, but simple GUI. The GUI executes a batch of commands.</p>
<p>Subsystem currently included are: a runner for SQL on a variety of database platforms, a runner for Quilt queries (Quilt is an early version of XQuery and runs on a platform called KWEELT), ElementalDB, an experimental database system used for instruction in a graduate database implementation course, our own XQuery engine which aims at handling data in terabyte range stored in our storage in paginated form using our pagination algorithm, a research prototype for NC94, an important spatiotemporal data set in agriculture, and a research prototype for a temporal database. The organization of the subsystems follows strict convention for ease of further development and maintenance. XML is used extensively by various subsystems. An XML based framework has been developed for benchmarking subsystems to make experiments completely repeatable at click of a button starting from creation of storage, loading of data sets, execution of commands, collecting performance data in XML-based logs to reporting using XQuery queries on the XML logs.</p>
<p>With a very small learning curve, the resulting workbench can be used by students, instructors, developers and researchers alike and managed easily.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24873,Computer Sciences,"Benchmarking,Commands,Database,Workbench,XML,XQuery",A workbench for advanced database implementation and benchmarking,thesis
"Li, Qi,Cai, Ying,Zhang, Wensheng","Song, Wenfei",N/A,"One of the biggest challenges in network flow detection is that the data distribution is extremely unbalanced, which leads to the inability of traditional unsupervised learning to find data features effectively, while semi-supervised/supervised learning methods require a large number of labeled samples to obtain enough information to study. Active learning can effectively reduce the cost of labeling through the process of ""training-labeling the most valuable samples-training"". Existing work pays more attention to the application of pool-based active learning in related scenarios, and mostly ignores the startup problem when there are no labeled samples in the initial condition. This report better simulates real-world application scenarios through special pool-stream input settings, focusing on solving the cold-start problem on extremely unbalanced datasets, and proposes a new initiative that combines uncertainty and marginal information for active learning selection strategies. We organize the rest of the report as following. In Chapter 1, we briefly discuss the background of active learning in the field of network flow detection, and in Chapter 2 we discuss existing related work. Chapter 3 presents the preliminary work, and Chapter 4 provides a formal definition of the problem. In Chapter 5, we give a concrete description of our work and present our work results in Chapter 6. At last, we make a conclusion of our work in Chapter 7.",https://dr.lib.iastate.edu/handle/20.500.12876/ywAbmxZv,Computer science,"active learning,Gaussian Mixture Model,imbalance,semi-supervised",Active Learning with semi-supervised Gaussian Mixture Model on Network Flow Detection,thesis
Carl K Chang,"Rahman, Mohammed",2021-01-16T18:24:29.000,"<p>Activities of Daily Living (ADL) can give us information about an individual’s health, both physical and mental. They are captured using sensors and then processed and recognized into different activities. Activity recognition is the process of understanding a person’s movement and actions. In this work, we develop a language in a simple grammar that describes the activity and uses it to recognize the activity. We call this language as Activities of Daily Living Description Language, or A(DL)2 in short.Even after an activity has been recognized, the data it represents is still digital data and it would take some expertise and time to understand it.  To overcome this problem, a system that can visualize and animate individuals’ activity in real time without violating any privacy issues, can be built. This will not only help in understanding the current state of individual but will also help those who are in charge of monitoring them remotely like nurses, doctors, family members, thereby rendering better care and support especially to the elderly people who are aging.  We propose a real time activity recognition and animation system that recognizes and animates the individual’s activity. We experimented with one of the basic ADLs, walking, and found the result satisfactory. Individuals location is tracked using sensors and is sent to the recognition system which then decides the type of activity in real time by using the language to describe it, and then the data is sent to a visualization system which animates that activity. When fully developed, this system intends to serve the purpose of providing better health care and immediate support to the people in need.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/94535,,"Activity Recognition,ADL,Animation",Activity recognition and animation of activities of daily living,dissertation
"Zhang, Wensheng,Cai, Ying,Chang, Carl K","Ma, Xiaojuan",N/A,"Hiring a trusted third party (TTP) with a small probability to re-execute outsourced tasks is an efficient way for a client to ensure the correctness of the results returned by an untrusted cloud server. A deposit from the server is required before the execution and will be taken away as a penalty if the server is caught misbehaving; the emerging blockchain system has made this mechanism easier to implement. The allocation of the client's total budget and the server's deposit among a set of tasks highly affects the probability of hiring a TTP, the latency for confirming the results and the wage earned by the server. In this thesis, we develop algorithms that efficiently allocate the fund (budget and deposit) to meet different requirements (low latency, high wage, high profit-deposit ratio, etc.). To be specific, we propose: (1) an optimal fund allocation algorithm for the fixed budget and fixed deposit scenario; (2) an adaptive approximate algorithm (AA) with an error bound; (3) an algorithm based on binary search and AA to decide a proper deposit given a target profit-deposit ratio; and (4) accurate (dynamic programming based) and approximate (reinforcement learning based) algorithms for the more general scenario when the budget is not fixed and the client has a hybrid goal (involving both budget and latency).",https://dr.lib.iastate.edu/handle/20.500.12876/arY43aWv,Computer science,"computation outsourcing,fund allocation,game theory",Adaptive fund allocation for game-based verifiable computation outsourcing,thesis
Leslie Miller,"Kalyanasamy, Arun",2018-08-11T14:25:48.000,"<p>To write a code once and run the application anywhere has been the holy grail of application developers. With the diversities in the operating contexts introduced by various hardware, software, users and carriers, user interfaces must cater to all the present and future operating contexts. The expense of developing a software product to tailor to new market fragmentations is soaring. We propose a model that will aid user-interface designers working in the field of mobile computing to build applications across operating contexts, without the hassle of redesigning it to accommodate the unique constraints introduced by each operating context. This research will make the following contributions: 1) propose a novel user interface model for isolating the common features of an operating context for automatic rendering of the interface according to the constraints of each context, 2) prove feasibility of the model by handling one such operating context constraint namely screen size and present an efficient implementation of the proposed model for it using Microsoft Silverlight and 3) use the above implementation to measure the cost-benefit trade-off with a user study involving tasks of varying complexity.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26763,Computer Sciences,"adaptation,adaptive interface,application fragmentation,automatic rendering,operating context",Adaptive interfaces for application defragmentation in diverse operating contexts,thesis
"Jannesari, Ali,Ying, Cai,Li, Qi","Abebe, Waqwoya Mesfin",N/A,"Vehicular ad-hoc networks (VANETs) are highly dynamic traffic networks made up of vehicles and infrastructure nodes like Road Side Units (RSUs).  In VANET safety applications, data sharing increases the situational awareness of nodes minimizing traffic accidents and congestions. In practice however, it is unlikely to recruit nodes to join a safety application network without providing an incentive. In this paper, we propose a system that incentivizes data sharing in VANETs using crypto tokens running on a lightweight Blockchain. Our system rewards data sharing nodes while mitigating against malicious node collusion and node misbehavior. Nodes in VANETs periodically broadcast a beacon known as the Basic Safety Message (BSM). In the proposed system, nodes create a trajectory of handshakes by digitally signing each other’s BSMs. Each node locally queues its handshakes until a certain threshold queue size is reached at which point the queue gets uploaded to the cloud infrastructure to earn reward tokens. Unlike previous works that deal with arbitrary or global thresholds, we design a scheme of setting regional, dynamic and tunable thresholds. Furthermore, we have trained a neural network model on the VeReMi dataset to help detect location falsification during node-to-node communication.",https://dr.lib.iastate.edu/handle/20.500.12876/jrl8W0Jr,Computer science,"Blockchain,Misbehavior Detection,Neural Networks,VANET",Adaptive thresholding and neural network-based misbehavior detection for incentivizing VANET data sharing,thesis
"Bhattacharya, Sourabh,Jia, Yan-Bin,Govindarasu, Manimaran","Clanin, Joseph Scott",N/A,Security games model asset protection scenarios between attackers and defenders over a collection of targets. This work analyzes a class of not-necessarily-zero-sum security games in which a resource-constrained attacker and defender allocate their resources over a finite target set under an additive utility constraint. A characterization of the Nash equilibria in such games is given in terms of necessary structural properties and a quadratic time algorithm to compute an equilibrium is proposed. An application of the special case of a zero-sum additive game to optimal deployment of defensive resources against transmission line attacks in power grids is presented. The efficacy of this approach is verified through simulation on standard IEEE power system test cases.,https://dr.lib.iastate.edu/handle/20.500.12876/JwjbNgVw,Computer science,"Equilibrium,Game Theory",Additive security games,thesis
"Jannesari, Ali,Quinn, Christopher,Tian, Jin,Huai, Mengdi","Stanley, Justin",N/A,"Advancements in reinforcement learning (RL) via deep neural networks have enabled their application to a variety of real-world problems. However, these applications often suffer from long training times. While attempts to distribute training have been successful in controlled scenarios, they face challenges in heterogeneous-capacity, unstable, and privacy critical environments. This work applies concepts from federated learning (FL) to distributed RL, specifically addressing the stale gradient problem. A deterministic framework for asynchronous federated RL is utilized to explore dynamic methods for handling stale gradient updates in the Arcade Learning Environment. Experimental results from applying these methods to two Atari-2600 games demonstrate a relative speedup of up to 95\% compared to plain A3C in large and unstable federations.",https://dr.lib.iastate.edu/handle/20.500.12876/Qr9mVoVr,Computer science,"asynchronous systems,distributed learning,distributed systems,machine learning,parallel optimization,reinforcement learning",Addressing stale gradients in asynchronous federated deep reinforcement learning,thesis
Jin Tian,"Saadati, Mojdeh",2020-09-23T19:13:19.000,"<p>Confounding bias, missing data, and selection bias are three common obstacles to valid causal inference in the data sciences. Covariate adjustment is the most pervasive technique for recovering casual effects from confounding bias. In this thesis, we introduce a covariate adjustment formulation for controlling confounding bias in the presence of missing-not-at-random data and develop a  necessary and sufficient condition for recovering causal effects using the adjustment. We also introduce an adjustment formulation for controlling both confounding and selection biases in the presence of missing data and develop a  necessary and sufficient condition for valid adjustment. Furthermore, we present an algorithm that lists all valid adjustment sets and an algorithm that finds a  valid adjustment set containing the minimum number of variables, which are useful for researchers interested in selecting adjustment sets with desired properties.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/94365,,"adjustment,causal effect,missing data,missing not at random,observational study,selection bias",Adjustment criteria for recovering causal effects from missing data,thesis
Wallapak Tavanapong,"Kaul, Sheetal",2018-08-11T19:15:33.000,"<p>In nearly one decade of Twitter’s being it has witnessed an ever growing user base from various realms of the world, one of them being politics. In the political domain, Twitter is used as a vital tool for communication purposes, running effective e-campaigns, and mining and affecting public opinions to name a few. We study the problem of automatically detecting whether a tweet posted by a state’s Senate’s twitter handle in the US has a reference to policy agenda(s). Such a capability can help detect the policy agendas that a state focuses on and also capture the inception of ideas leading to framing of bill/law. Furthermore, analyzing the spatial and temporal dynamics of tweets carrying policy agendas can facilitate study of policy diffusion among states, and help in comprehending the changing aspects of states learning policy-making from each other.</p>
<p>Currently, no study has been carried out that analyzes Twitter data to detect whether or not a tweet refers to a policy agenda. We present our analysis on 122,965 tweets collected from verified Twitter handles of the US state’s upper house – Senate. We present our high-level analysis on (a) how much Twitter has penetrated into state politics and (b) how states use the medium differently in terms of the messages they broadcast. Our proposed approach aims to automate classification of a tweet based on having a reference to policy agenda (Has Agenda) or not (No Agenda). We accomplish this by leveraging existing text classification methodology and achieve a recall of 89.1% and precision of 77.2% for the “Has Agenda” class. We investigate several machine learning algorithms to determine the best performing one for our binary classification problem. We conclude that support vector machine using linear kernel was the most efficient algorithm to use for our dataset. Lastly, we propose a set of hand-crafted features that together with feature selection and stemming improved our classifier’s performance. Prior to including these features the classifier was developed using, basic preprocessing techniques, and term occurrence (for feature extraction). An overall improvement of 5.187 % at a significance level of α=0.05 was achieved.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28738,"Computer Sciences,Political Science","Computer Science,State Legislature,Support Vector Machine,Twitter",Agenda detector: labeling tweets with political policy agenda,thesis
Giora Slutzki,"Leuchner, John",2018-08-16T03:46:12.000,"<p>The problem of deciding whether a join dependency [R] and a set  F of functional dependencies logically imply an embedded join dependency [S] is known to be NP-complete. It is shown that if the set  F of functional dependencies is required to be embedded in  R, the problem can be decided in polynomial time. The problem is approached by introducing agreement graphs, a type of graph structure which helps expose the combinatorial structure of dependency implication problems. Agreement graphs provide an alternative formalism to tableaus and extend the application of graph and hypergraph theory in relational database research;Agreement graphs are also given a more abstract definition and are used to define agreement graph dependencies (AGDs). It is shown that AGDs are equivalent to Fagin's (unirelational) embedded implicational dependencies. A decision method is given for the AGD implication problem. Although the implication problem for AGDs is undecidable, the decision method works in many cases and lends insight into dependency implication. A number of properties of agreement graph dependencies are given and directions for future research are suggested.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/81813,Computer Sciences,Computer science,Agreement graphs and data dependencies,dissertation
David Fernandez-Baca,"Chaudhary, Ruchi",2018-08-11T16:15:46.000,"<p>Despite the unprecedented outpouring of molecular sequence data in phylogenetics, the current understanding of the tree of life is still incomplete. The widespread applications of phylogenies, ranging from drug design to biodiversity conservation, repeatedly remind us of the need for more accurate and inclusive phylogenies. My thesis addresses some of the underlying challenges, by presenting theoretical and empirical results, as well as algorithms for a range of phylogenetic optimization problems.</p>
<p>In the first part of this thesis, I develop a heuristic method for the NP-hard unrooted Robinson-Foulds (RF) supertree problem, and show that it yields more accurate supertrees than those obtained from Matrix Representation with Parsimony (MRP) and rooted RF heuristic. In the second, I present an RF distance measure based approach (MulRF) for inferring a species tree from the input multi-copy gene trees, through a generalization of RF distance to multi-labeled trees. Through simulation, I show that this approach, which is independent of gene tree discordance mechanisms, produces more accurate species trees than existing methods when incongruence is caused by gene tree error, duplications and losses, and/or lateral gene transfer. Next, I perform a simulation study to evaluate the performance of Gene Tree Parsimony (GTP) under duplication and duplication and loss cost models and compare it to MulRF method. The objective is to study the effects of various types of sampling (e.g., gene tree and sequence sampling), gene tree error, and duplication and loss rates on the accuracy of the phylogenetic estimates by GTP and MulRF. Next, I present efficient error correction algorithms for gene tree reconciliation based on duplication, duplication and loss, and deep coalescence. In the end, I present NP-completeness proofs for two problems whose complexity was previously unknown.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27187,Computer Sciences,,Algorithms for constructing more accurate and inclusive phylogenetic trees,dissertation
David Fernandez-Baca,"Bansal, Mukul",2018-08-25T04:15:07.000,"<p>This thesis deals with two different problems: (1) the minimum bipartite fill-in problem, and (2) the gene-duplication problem. We establish the fixed parameter tractability of the minimum bipartite fill-in problem and develop a faster local search algorithm which speeds up heuristics for the gene-duplication problem.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/81517,Computer Sciences,Computer science,Algorithms for minimum bipartite fill-in and the gene-duplication problem,thesis
"Giora Slutzki,Steven M. LaValle","Simov, Borislav",2018-08-24T19:14:30.000,"<p>The dissertation presents algorithms for robotics and security. The first chapter gives an overview of the area of visibility-based pursuit-evasion. The following two chapters introduce two specific algorithms in that area. The algorithms are based on research done together with Dr. Giora Slutzki and Dr. Steven LaValle. Chapter 2 presents a polynomial-time algorithm for clearing a polygon by a single 1-searcher. The result is extended to a polynomial-time algorithm for a pair of 1-searchers in Chapter 3.;Chapters 4 and 5 contain joint research with Dr. Srini Tridandapani, Dr. Jason Jue and Dr. Michael Borella in the area of computer networks. Chapter 4 presents a method of providing privacy over an insecure channel which does not require encryption. Chapter 5 gives approximate bounds for the link utilization in multicast traffic.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/78929,"Computer Sciences,Mathematics,Optics",Computer science,Algorithms for security in robotics and networks,dissertation
David Fernandez-Baca,"Agarwala, Richa",2018-08-23T15:12:57.000,"<p>This dissertation is a collection of papers from two independent areas: convex optimization problems in  R[superscript]d and the construction of evolutionary trees;The paper on convex optimization problems in  R[superscript]d gives improved algorithms for solving the Lagrangian duals of problems that have both of the following properties. First, in absence of the bad constraints, the problems can be solved in strongly polynomial time by combinatorial algorithms. Second, the number of bad constraints is fixed. As part of our solution to these problems, we extend Cole's circuit simulation approach and develop a weighted version of Megiddo's multidimensional search technique;The papers on evolutionary tree construction deal with the perfect phylogeny problem, where species are specified by a set of characters and each character can occur in a species in one of a fixed number of states. This problem is known to be NP-complete. The dissertation contains the following results on the perfect phylogeny problem: (1) A linear time algorithm when all the characters have two states. (2) A polynomial time algorithm when the number of character states is fixed. (3) A polynomial time algorithm when the number of characters is fixed.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/63840,Computer Sciences,Computer science,Algorithms for weighted multidimensional search and perfect phylogeny,dissertation
N/A,"Wallentine, Virgil",2018-08-24T22:06:48.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/77370,Computer Sciences,Machine theory,An abstract machine to control the execution of semi-independent concurrent computations,dissertation
"Chang, Carl K,Zhang, Wensheng,Wong, Johnny S,Cai, Ying,Mitra, Simanta","Yu, Chen-Yeou",N/A,"Vehicular edge computing (VEC) is a promising technology composed of distributed computation and 5G communications to deliver the Quality of Service (QoS) to intelligent vehicles. Battery electric vehicles (BEVs) and hybrid electric vehicles (HEVs) heavily rely on the electricity to perform regular functions or support different kinds of movements. As a result, battery management is a critical issue to BEVs and HEVs. Meanwhile, edge servers in the network need to do load balancing as well. We want the energy consumption in the whole vehicular networks to be minimized and the efficiency can be maximized by the assistance of edge computing.
To address this problem, an edge-based situation-aware framework is proposed to reduce energy consumption and mitigate traffic congestion. The framework is composed of three (3) components: 1) vehicular energy saver – turn off the unnecessary applications based on the LSTM model’s forecast in driver’s situation, 2) edge server load balancer – share the loading of requests to the neighbor servers and put the idle servers into sleep, and 3) traffic congestion mitigator – placing realistic rewards on suggested rerouting road sections.
The proposed framework is extensively experimented and simulated. The results show that, 1) our proposed framework can save electricity from the perspective of service requests, 2) offloading algorithm is effective in load-balancing, thus, saving more energy and, 3) more intelligent agents cooperate with the congestion rerouting policy.",https://dr.lib.iastate.edu/handle/20.500.12876/gwW753Dw,Computer science,"energy efficient,load balancing,LSTM,reinforcement learning,traffic congestion,vehicular edge computing",An edge-based situ-aware framework for energy-efficient 5G vehicular networks and traffic congestion mitigation,dissertation
N/A,"Qu, Sheng",2020-07-17T07:22:36.000,"<p>Federal agencies collect and analyze data to carry out their missions. A significant portion of these activities requires geospatial data collection in the field. Models for computer-assisted survey information collection are still largely based on the client-server paradigm with symbolic data representation. Little attention has been given to digital geospatial information resources, or emerging mobile computing environments. This paper discusses an infrastructure designs for delivering geospatial data users in a mobile field computing environment. Mobile field computing environments vary widely, and generally offer extremely limited computing resources, visual display, and bandwidth relative to the usual resources required for distributed geospatial data. Key to handling heterogeneity in the field is an infrastructure design that provides flexibility in the location of computing tasks and returns information in forms appropriate for the field computing environment. A view agent based infrastructure has been developed with several components. Wrappers are used for encapsulating not only the data sources, but the mobile field environment as well, localizing the details associated with heterogeneity in data sources and field environments. Within the boundaries of the wrappers, mediators and object-oriented views implemented as mobile agents work in a relatively homogeneous environment to generate query results. Mediators receive a request from the user application via the field wrapper, and generate a sequence of mobile view agents to search for, retrieve, and process data. The internal infrastructure environment is populated with computation servers to provide a location for processing, especially for combining data from multiple locations. Each computation server has a local object-oriented data warehouse equipped with a set of data warehouse tools for working with geospatial data. Since the prospect of query reuse is likely for a field worker, we store the final and intermediate results in the data warehouse, allowing the warehouse to act as an active cache. Even when field computing capacity is ample, the warehouse is used to process data so that network traffic can be minimized.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/96913,,Computer science,An infrastructure for delivering geospatial data to field users,thesis
"Leslie Miller,Shashi Gadia,James Roth","Yahya, Melissa",2018-08-23T07:23:46.000,"<p>Bioterrorism has become one of the greatest threats for nations. As bioterrorism threats are increasing these days, research in bioterrorism surveillance has been conducted to detect the threats at the earliest time. Following this research, some bioterrorism surveillance systems have been developed. Some of these systems use humans as indicators and the others use animals. Both of these systems detect the threats by collecting and analyzing the statistical data on health trends. Since most bio-weapons start as animal diseases, using animals as indicators allow identification of a bioterrorism threat at earlier time than using humans.;Current bioterrorism surveillance systems based on animals focus only on certain groups of animal species while there are other animal species that are also very susceptible to the highly-threatening diseases. The system introduced here provides a step towards the identification of highly-threatening diseases in animal species, such as cattle and pigs. In the event of such identification, a red flag is raised and related information is sent to agents responsible for looking into the threats in more detail. The design, implementation and result of this system are given and explained.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/68484,Computer Sciences,Computer science,An online system for bioterrorism surveillance,thesis
Shashi K. Gadia,"Wang, Xiaofeng",2018-08-11T20:21:08.000,"<p>The transition from paper-oriented record keeping to their on-line counterpart has been</p>
<p>anything but smooth. Often, the on-line systems are difficult to use and do not provide a</p>
<p>structure to maintain organizational memory and procedures. A user momentarily sees the</p>
<p>part he / she is currently working on and not aware of the big picture. Often, the user is has to</p>
<p>work in such convoluted ways that require considerable learning curve. The Course Catalog at</p>
<p>universities seem to be facing this these problems.</p>
<p>In this thesis we provide XCCat, an XML-based road-map for an enterprise solution for</p>
<p>design, implementation, usage, and maintenance of course catalogs for institutions of higher</p>
<p>learning. XCCat brings information about curriculum requirements, courses, changes to</p>
<p>courses, approval sequences, participants and deliberations surrounding catalog development,</p>
<p>and report generation under a single umbrella in a live XML-based database that not only</p>
<p>remembers everything but can also be queried for this memory. The concept of catalog</p>
<p>publication is carefully developed so when a catalog for the next year is published, the</p>
<p>development version of the catalog for the following year is also published; the latter</p>
<p>automatically carries forward the unfinished work in progress. This simplifies many things:</p>
<p>need for tracking is reduced considerably, the development version of the catalog is always</p>
<p>available, increases opportunities for collaboration, there is only one goto place foll all</p>
<p>participants (e.g. university, colleges, and programs) for all catalog related issues. There is</p>
<p>only one view of the catalog, all information is visible to everyone except the parts that need</p>
<p>authorization for making updates. A fair portion that is enough to assess the viability of the</p>
<p>ideas behind XCCat have already been implemented.</p>
<p>The approach comes close to what has been termed zero information-loss in the field of</p>
<p>temporal databases. In the zero information loss model, one could query data, the database</p>
<p>history of data, query updates, and query queries. Zero information-loss is a part of what is</p>
<p>known as provenance in databases and information systems these days.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28042,Computer Sciences,,An XML-based framework for management of a course catalog system with zero information-loss,thesis
"Li, Qi,Rajan, Hridesh,Tavanapong, Wallapak,Le, Wei,Gao, Hongyang","Chakraborty, Mohna",N/A,"Customers express their opinions and provide feedback through reviews, making the analysis of the reviews crucial for business development. However, the sheer volume of reviews necessitates cost-effective approaches for review analysis. Hiring expert annotators is expensive in terms of time and money. Alternative methods include using fine-tuned large language models, high-quality rules derived from human-annotated corpora, or a combination of both. Yet, these approaches require sufficient labeled samples, which are expensive and seldom available, especially in resource-scarce domains. In the absence of high-quality rules or sufficiently labeled samples, the performance of large language models degrades due to noise and bias in training.

To address the issue of noise and bias in training, we propose strategies based on the intuition that tasks with the same goal should agree on their interpretations of the same reviews. Based on this intuition, we propose frameworks with multiple modules having similar goals and then apply canonical correlation analysis as an early stopping indicator to avoid overfitting to noise. We also apply the self-training process to gradually enrich the training data, mitigating weak supervision bias.

In addition to typical model training, recent studies have shown that prompt-based approaches can effectively leverage the knowledge embedded in pre-trained language models for various tasks, requiring very few labeled samples. These approaches use prompts and verbalizers that map model predictions to the label space. Prompts and verbalizers can be manually provided by humans or generated automatically. Manual prompts and verbalizers rely heavily on human knowledge of the task and are often sub-optimal since language models may interpret them differently from humans. Automatic prompt and verbalizer generation use large generative models that need few labeled samples for fine-tuning; however, their performance is sensitive to the choice of few-shot samples. Additionally, deploying and tuning large generative models on local hardware is costly due to their size. To overcome the limitations of recent studies, we propose approaches to automatically generate high-quality prompts and verbalizers and identify few-shot samples in a zero-shot/cold-start fashion using a moderately sized masked language model, which can be easily deployed and tuned in-house for real-world applications.

Specifically, for automatic prompt generation, we propose a prompt augmentation technique to generate prompts and a novel ranking metric to rank the automatically generated prompts under the zero-shot setting based on the intuition that high-quality prompts should be sensitive to changes in certain keywords in the given sentence. Our intuition for selecting verbalizers and few-shot samples is that they both should model the distribution of the samples in the provided dataset. We propose a novel group-ranking approach based on the intra and inter similarity among groups following the cold-start setting. The verbalizer tokens and few-shot samples are then obtained using highly ranked groups. 

This dissertation discusses the proposed cost-effective approaches that facilitate the efficient analysis of reviews from various domains with minimal supervision.",https://dr.lib.iastate.edu/handle/20.500.12876/avVO3pZr,Computer science,"Data Annotation,Data Mining,Prompt-based Learning,Review Analysis",Analysis of textual-based reviews with minimal supervision,dissertation
"Le, Wei,Basu, Samik,Rajan, Hridesh,Ciardo, Gianfranco,Cohen, Myra B","Kallingal Joshy, Ashwin",N/A,"It appears to be a fact of life that no matter how much effort is spent on testing a program, software defects are continually introduced and removed during the development process. Numerous static and dynamic code analysis based techniques exist to help the developers quickly locate and fix these faults. However, in spite of existence of advanced techniques like path-sensitive static analysis tools, fuzzers, automated fault localization, and program repairs, a significant amount of developer's time is still spent in locating, understanding and fixing faults.

In this thesis, we developed code fragment analysis techniques to address these challenges. The code fragments consists of noncontinuous statements extracted from the original programs, with respect to some desired property. These code fragments can then be made executable to help gain further insight on the property. First, we identified a set of challenges for extracting and building such statements and devised solutions for them. Then, we used our findings to automatically validate path-sensitive static analysis warnings by converting the paths reported by the tools into code fragments. These code fragments are then dynamically executed to check for the existence of the static warnings reported by the tools.

We found that code fragments are able to validate static warnings by exposing their dynamic symptoms. Hence, we created as-small-as-possible executable code fragments that can reproduce the faults called fault signatures. We found that the smaller size and complexity of the fault signatures helped fuzzers to generate crashing inputs for faults that are harder to reproduce using the original programs. As the next step, we generated fault signatures for crashes reported by fuzzers. We then used them to identify ""unique"" faults and deduplicate the crashing inputs reported by the fuzzers. We found that using fault signatures correctly grouped the crashing inputs for unique faults and outperformed the state-of-the-art fuzzer based deduplication methods by as much as 75 times.

Finally, we analyzed features of code fragments and used diffs generated from vulnerability fixing patches to actively train a machine learning model that identifies statements that contribute towards the bug fix. This model was then used to produce a large dataset of line-level vulnerability labels that can be used by other deep-learning based approaches for tasks like identifying vulnerable lines, bug detection, and fault localization. We found that using our dataset improved the performance of LineVul, a state-of-the-art vulnerability detection model, in terms of both the number of detected vulnerable functions/statements and their accuracy.",https://dr.lib.iastate.edu/handle/20.500.12876/aw4NWl6r,Computer science,"Code Fragment,Deduplication,Fault Signature,Fuzzing,Static Warnings,Testing",Analyzing code fragments for faults,dissertation
"Cohen, Myra B.,Basu, Samik,Newman, Jennifer","Sinha, Urjoshi",N/A,"Highly configurable systems which offer users a large array of options to choose from, come with many challenges. Users of these software systems often want to optimize a particular objective such as improving a functional outcome or increasing system performance. However, many applications today are data-driven, meaning they depend on inputs or data which can be complex and varied. Examples of data-driven systems include tools such as a search engine and scientific software such as a DNA alignment tool. In any data-driven system, the output of the application is directly influenced by the information being fed to these systems which further depends on the nature of the end-user’s objectives. For instance, in a search engine the output depends on the input search query string or how the user is framing these queries. Hence, when trying to optimize these systems, a search needs to be run (and re-run) for all inputs, making optimization a heavy-weight and potentially impractical process. The second part of the problem is that we are also faced with the challenge of devising effective ways of testing these large configuration spaces, given their data-driven nature. In this work, we explore these issues on data-driven highly-configurable scientific applications in two domains- bio-informatics and cyber-physical systems.
In the first part of this thesis, we ask if it is possible to use an optimization algorithm to find configurations to improve functional objectives. We also try to find patterns of best configurations over all input data and try to examine if sampling can be used to approximate the results. For the data-driven application studied in the bio-informatics domain, we find that the default configuration is best only 34% of the time, while clear patterns emerge of other best configurations. We also find evidence that it is possible to use light-weight optimization approaches for this problem. Finally, we demonstrate that sampling of the input data helps find patterns at a lower cost.
The second part of this thesis focuses on studying configurable autopilot tools used in cyber-physical systems such as drones. We present an intelligent approach of exploring configurability in autopilot tools by using model-based testing which involves feature-modeling, classification trees as some of the key components. We discover that it is possible to detect patterns of poor, or sub-optimal configurations for a given user-scenario using our method.
This work thus aims to present efficient approaches to study data-driven configurable tools and optimize configuration spaces with regard to specific user objectives.",https://dr.lib.iastate.edu/handle/20.500.12876/3wxaeJov,Computer science,"bioinformatics,configuration optimization,data-driven,genetic algorithm,px4-autopilot,software engineering",Analyzing the impact of configurations in data-driven software applications,thesis
"Wurtele, Eve Syrkin,Prabhu, Gurpur M,Mitra, Simanta","Kaliki, Sumanth Kumar",N/A,"Next-generation RNA sequencing technology has revolutionized translational research. RNA-Seq enables researchers to accurately quantify and compare the gene expression levels within and across diverse biological samples. Today, petabytes of multidimensional, complex RNA-Seq data are available publicly. Researchers analyze this multidimensional data using dimensionality reduction algorithms and heat maps—however, many available tools are written in the R or Python language. These tools lack an easy-to-use interactive interface and are not suitable for large data sets. Interactivity is a key requirement for efficient data analysis of complex multidimensional data sets. Based on this, Singh et al. developed a Java tool, MetaOmGraph [Singh et al. (2020)], for fully interactive analysis of large omics data sets.
     This thesis extends the functionality of MetaOmGraph (MOG) software and adds dimensionality reduction features for easy and efficient analysis of bulk and single-cell RNA-Seq data. I have added Principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE) dimensionality reduction algorithms, and heat maps to MOG to facilitate multidimensional data analysis. Researchers can interactively visualize PCA, t-SNE, and heat maps by changing the final appearance of plots using various options provided in each of these. Sample filtering in MOG is now made easy and efficient for the user by creating sample subset lists. Overall, these contributions to MOG make it more user-friendly. MOG software is available at https://metnetweb.gdcb.iastate.edu/MetNet_MetaOmGraph.htm and the code is at https://github.com/urmi-21/MetaOmGraph.",https://dr.lib.iastate.edu/handle/20.500.12876/ywAbdBnv,Computer science,"Analysis of large omics data sets,MetaOmGraph,MOG,PCA,t-SNE",Application of Computer Science to enhancing and adding functionality to an innovative software - MetaOmGraph,thesis
"Rajan, Hridesh,Mitra, Simanta,Shihab, Emad","OBrien, David",N/A,"Code intelligence tools such as GitHub Copilot have begun to bridge the gap between natural language and programming language. A frequent software development task is the management of technical debts, which are suboptimal solutions or unaddressed issues which hinder future software development. Developers have been found to ``self-admit'' technical debts (SATD) in software artifacts such as source code comments. Thus, is it possible that the information present in these comments can enhance code generative prompts to repay the described SATD? Or, does the inclusion of such comments instead cause code generative tools to reproduce the harmful symptoms of described technical debt? Does the modification of SATD impact this reaction? Despite the heavy maintenance costs caused by technical debt and the recent improvements of code intelligence tools, no prior works have sought to incorporate SATD towards prompt engineering. Inspired by this, this thesis contributes and analyzes a dataset consisting of 36,381 TODO comments in the latest available revisions of their respective 102,424 repositories, from which we sample and manually generate 1,140 code bodies using GitHub Copilot. Our experiments show that GitHub Copilot can generate code with the symptoms of SATD, both prompted and unprompted. Moreover, we demonstrate the tool's ability to automatically repay SATD under different circumstances and qualitatively investigate the characteristics of successful and unsuccessful comments. Finally, we discuss gaps in which GitHub Copilot's successors and future researchers can improve upon code intelligence tasks to facilitate AI-assisted software maintenance.",https://dr.lib.iastate.edu/handle/20.500.12876/1wgeG9Ar,Computer science,"code generation,large language models,technical debt",Are prompt engineering and todo comments friends or foes? An evaluation on GitHub Copilot,thesis
"Zhang, Wensheng,Saifullah, Abusayeed,Cai, Ying","Permatasari, Elisabeth Kusuma Adi",N/A,"Reliable and real-time communication between sensors and actuators is a precursor in
industrial Internet of Things (IIoT) applications. Time synchronized channel hopping (TSCH) is
a key feature in existing IIoT wireless standards such as WirelessHART for reliable
communication. The current hopping strategy in WirelessHART utilizes a predefined channel
sequence which requires sender-receiver pairs to compute the channels using a modular function
prior to communication. As the sequence length is limited by the total number of channels, the
function produces a strong repetitive channel usage. Consequently, a jammer is able to derive the
channel usage through a small effort by listening to the active links without knowing the original
sequence. Additionally, adopting harmonic superframe lengths also makes channel cracking easier.
Existing channel hopping strategies in other wireless domains are not designed to meet high
reliability and real-time requirements of WirelessHART networks. A channel hopping for
WirelessHART network must guarantee fast switching (within 0.192 ms), channel synchronization
between senders and receivers, and interference-free concurrent transmissions. In this work, an
attack-resilient channel hopping mechanism that meets these requirements is proposed. Our
approach significantly reduces the strong repetitive pattern in channel sequence by adding
randomness at multiple levels. We have implemented our approach and evaluated through testbed
experiments and also through simulations in Cooja. The results show that our proposed approach
can significantly lower a jammer’s capability in making correct channel prediction without
increasing the channel hopping overhead from the current approach.",https://dr.lib.iastate.edu/handle/20.500.12876/Nveo0y5z,Computer science,"Attack resilience,Channel hopping,Industrial Internet of Things,WirelessHART",Attack-resilient channel hopping for WirelessHART,thesis
"Johnny S. Wong,Julie A. Dickerson","Velagaleti, Narasimha Rao",2018-08-11T15:18:39.000,"<p>Routing in wireless networks is not an easy task as they are highly vulnerable to attacks. The main goal of this work is to study the routing performance and security aspects of wireless ad hoc and mesh networks. Most of the routing protocols use hop-count as the routing metric. Hop count metric may not be appropriate for routing in wireless networks as this does not account for the link qualities, advantages of multi-radio paradigm etc.</p>
<p>There are several metrics designed for link quality based source routing protocols for multi-radio wireless ad hoc and mesh networks. For example Weighted Cumulative Expected Transmission Time (WCETT), Adjusted Expected Transfer Delay(AETD) etc. But these metrics do not consider the effect of individual link qualities on the total route quality and route selection. This lack of ability from WCETT or AETD would allow them to select suboptimal paths when actually an optimal path is available. In another point of view, this inability can create a routing disruption attack named as delay-variation attack (a variant of black hole attack). It can be launched by a couple of colluding attackers attracting packets at one point by showing very good link qualities and dropping packets at another point by decreasing the link quality. To select an optimal route and prevent the above mentioned attack, a new routing metric known as Variance Based Path Quality metric (VBPQ) is proposed. VBPQ metric provides a robust, reliable and secure edge to the routing mechanism.</p>
<p>Another major contribution of this study is to provide a detection mechanism for wormhole attacks in wireless ad hoc networks operating on link quality based source routing protocols. There have been several detection techniques designed for hop count based routing protocols but not for link quality based source routing protocols. In this work, a data mining approach called Cross feature analysis is used in an algorithm to detect wormhole attacks.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24792,Computer Sciences,"routing,security,attacks,VBPQ,wireless mesh networks,wireless networks,wormhole attack",Attacks and countermeasures on routing protocols in wireless networks,thesis
"Jannesari, Ali,Darr, Matthew,Li, Qi","Herrera, Jansel Emmanuel",N/A,"Learning compressed representations of higher-dimensional data for exploratory analysis and discovery of semantically meaningful information is a long-standing problem in computer vision. Self-supervised methods such as contrastive learning have been employed for model pre-training with good results, while clustering with the embeddings has been less impressive. Without a labeled dataset supporting supervised learning on the desired categories, other strong assumptions such as the number of clusters are often made prior to training, with no guarantees the clusters are biased towards semantically meaningful categories. In this paper, we present a learning paradigm that employs an auxiliary loss with contrastive learning that injects an inductive bias into the training process via a conveniently known and understood characteristic while avoiding strong assumptions such as the number of clusters in the data during training. This results in the ability to use the resulting embeddings for cluster discovery that isn't predicated on the strong assumption of the number of clusters prior to training. This is especially useful in most real-world datasets where the number of clusters is not known ahead of time such as in the domain of computational agriculture. In addition, a hard attention mask was explored on the embeddings, and was found to help further disperse and separate the clusters for easier identification. Outliers were also easily identified from the subsequent embeddings. We compare results between our method and a contrastive learning-only approach on a real-world dataset containing 227,060 in-field images consisting of 11 different crop classes and show the clear value in injecting an inductive bias via auxiliary loss, as well as the value of hard masking.",https://dr.lib.iastate.edu/handle/20.500.12876/dvmqeB3v,Computer science,"Contrastive Learning,Deep Learning,Machine Learning,Unsupervised Learning",Attention based contrastive learning with non-Siamese architecture,thesis
"Mitra, Simanta,Prabhu, Gurpur,Chang, Carl","Sreepathy, Gayathri",N/A,"Learning from online videos mainly helps the students and every individual understand a specific topic easily because of the realistic picturization. One of resources available to students is automated analysis and indexing of online lecture videos using image processing. Many online educational organizations and universities use video lectures to support teaching and learning. In past decades, video lecture portals have been widely used and are very popular. The text displayed in these video lectures are a valuable source for analyzing and indexing the lecture contents. Considering this scenario, we present an approach for automatic analysis and indexing of lecture videos using OCR (Optical Character Recognition) technology. For this, we segregated the unique key frames from a lecture video to extract the video contents. After the segregation of key frames by applying OCR and ASR (Automatic Speech Recognition) technology we can extract the textual data contents from the video lecture. From the obtained metadata, we segmented the video lecture based on the time-based text occurrence of the topics. The performance and the effectiveness of proposed analysis and indexing is proven by the evaluation.",https://dr.lib.iastate.edu/handle/20.500.12876/jrl84O6r,Computer science,"ASR,Image processing,Lecture videos,OCR",Automated analysis and indexing of lecture videos,thesis
"Wallapak Tavanapong,Johnny Wong","Liu, Xuemin",2018-08-11T17:10:39.000,"<p>With 655,000 deaths worldwide per year, colorectal cancer it is the third most common form of cancer and the third leading cause of cancer-related death in the Western world. Colonoscopy is currently the preferred screening modality for prevention of colorectal cancer, in which a tiny camera is inserted into the colon to look for early signs of colorectal cancer. A recent systematic review calculated a 22% miss rate for all colonoscopic neoplasia, being 2.1% for advanced lesions. This could be attributed to factors such as inadequate endoscope withdrawal time, poor range of motion of the endoscope, and general endoscopist experience. Therefore the demand for quality control for colonoscopic procedures is increasing, and many researchers have been taking efforts in this area. In this paper, we first presented a novel technique - Colon Center Axis Determination Technique for Non-dark Lumen Images, and the performance evaluation result demonstrates that this technique enables a more accurate view mode classification for all kind of images. Secondly, we proposed two novel approaches to help objectively measure the quality of colonoscopy. A set of objective metrics has been proposed, and preliminary analysis result shows the spiral number during whole procedure/withdrawal phase has a relatively strong positive association with the ground truth circumferential inspection score. The other approach is using association rule mining knowledge to determine patterns of colon inspection. The preliminary result demonstrates that endoscopists with good and relatively poor inspection skill have different inspection patterns, and thus using patterns to assess colonoscopy quality would be anther feasible and promising method.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25681,Computer Sciences,"image analysis,quality metrics,Quality of colonoscopy",Automated measurement of quality of mucosa inspection for colonoscopy,thesis
Samik Basu,"Ahuja, Lakshay",2019-03-26T17:39:38.000,"<p>Congestion has become a major threat to a country's economy. It not only causes loss in terms of man-hours and fuel costs, but also causes frustration among the public. It has become important for traffic operators to clear off congestion in a timely manner to resume the normal flow of traffic. This research focuses on evaluating congestion detection algorithms to implement them in an Intelligent Transportation System to detect congestion in real time. We have evaluated statistical based algorithms as well pattern recognition based algorithms to detect non-recurrent congestion on I-74, Davenport, Iowa, USA. Inter-Quartile Distance based algorithms and Supervised Learning based Decision Tree and Random Forest Classifiers are compared and evaluated in this study.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/30962,Computer Sciences,,Automatic incident detection,thesis
Samik Basu,"Bankar, Sneha",2018-08-11T16:45:42.000,"<p>Model Checking of distributed systems which communicate via message exchanges</p>
<p>is an open research problem. Such communication results in asynchronous interaction</p>
<p>between senders and receivers, one where the communicating entities, do not move in</p>
<p>lock-step. Model Checking is only possible for systems which can be represented as</p>
<p>finite state systems. Distributed Systems which communicate asynchronously cannot be</p>
<p>represented as finite state systems due to undefined bound on the message buffers meant</p>
<p>for message exchanges. Thus in general model checking such systems is undecidable.</p>
<p>In this thesis, we present a technique to automatically identify asynchronous systems</p>
<p>whose interactions can be represented by some finite state systems. This will allow us</p>
<p>to automatically model check the asynchronous systems. We also present a prototype</p>
<p>implementation and discuss the application of our technique on several case studies from</p>
<p>existing literature.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28239,Computer Sciences,,Automatic Verification of Interactions in Asynchronous Systems with Unbounded Buffers,thesis
N/A,"Patanroi, Daniel",2020-11-13T19:41:37.000,"<p>XML is a simple and very flexible text format, originally designed to meet the challenges of large-scale electronic publishing. Great as XML is for representing data, many XML-based query processors and storage managements have been proposed. With the classical memory problem of DOM parsers when an XML document is mapped onto an internal tree structure, many implementations handle a rather small document size. CanStoreX with textual page implementation approaches the problem by breaking an XML document into smaller pieces, stored into pages. It preserves the structure of the original XML document as well as does not require the whole document to be loaded into the main memory at once. Its binary page implementation removes major memory problems. This allows CanStoreX to parse XML documents of size 100 gigabytes or larger without any conspicuous problems. This shows that CanStoreX is scalable in terms of storage requirement, memory management, and query processing. The only two bottlenecks, encoding and decoding processes, can be diminished by embedding them into a computer chip, which will further bring CanStoreX to its primal state.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97130,,Computer science,Binary page implementation of a canonical native storage for XML,thesis
Hridesh  . Rajan,"Islam, Md Johirul",2020-02-12T22:56:10.000,"<p>Big data-driven transportation engineering has the potential to improve utilization of road infrastructure, decrease traffic fatalities, improve fuel consumption, and decrease construction worker injuries, among others. Despite these benefits, research on big data-driven transportation engineering is difficult today due to the computational expertise required to get started. This thesis proposes Boa_T, a transportation-specific programming language, and it's big data infrastructure that is aimed at decreasing this barrier to entry. Our</p>
<p>evaluation, that uses over two dozen research questions from six categories, shows that research questions</p>
<p>are easier to realize as Boa_T computer programs, an order of magnitude faster when these programs are run,</p>
<p>and exhibit 12-14x decrease in storage requirements.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31886,Computer Sciences,"big data,Boa,domain specific programming language",BoaT: A domain specific language and shared data science infrastructure for large scale transportation data analysis,thesis
Hridesh Rajan,"Dyer, Robert",2018-08-11T09:12:49.000,"<p>Mining software repositories provides developers and researchers a</p>
<p>chance to learn from previous development activities and apply that</p>
<p>knowledge to the future. Ultra-large-scale open source repositories</p>
<p>(e.g., SourceForge with 350,000+ projects, GitHub with 250,000+</p>
<p>projects, and Google Code with 250,000+ projects) provide an extremely</p>
<p>large corpus to perform such mining tasks on. This large corpus allows</p>
<p>researchers the opportunity to test new mining techniques and</p>
<p>empirically validate new approaches on real-world data. However, the</p>
<p>barrier to entry is often extremely high. Researchers interested in</p>
<p>mining must know a large number of techniques, languages, tools, etc,</p>
<p>each of which is often complex. Additionally, performing mining at</p>
<p>the scale proposed above adds additional complexity and often is</p>
<p>difficult to achieve.</p>
<p>The Boa language and infrastructure was developed to solve these</p>
<p>problems. We provide users a domain-specific language tailored for</p>
<p>software repository mining and allow them to submit queries via our</p>
<p>web-based interface. These queries are then automatically</p>
<p>parallelized and executed on a cluster, analyzing a dataset containing</p>
<p>almost 700,000 projects, history information from millions of</p>
<p>revisions, millions of Java source files, and billions of AST nodes.</p>
<p>The language also provides an easy to comprehend visitor syntax to</p>
<p>ease writing source code mining queries. The underlying</p>
<p>infrastructure contains several optimizations, including query</p>
<p>optimizations to make single queries faster as well as a fusion</p>
<p>optimization to group queries from multiple users into a single query.</p>
<p>The latter optimization is important as Boa is intended to be a</p>
<p>shared, community resource. Finally, we show the potential benefit of</p>
<p>Boa to the community by reproducing a previously published case</p>
<p>study and performing a new case study on the adoption of Java language</p>
<p>features.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27740,Computer Sciences,,Bringing ultra-large-scale software repository mining to the masses with Boa,dissertation
"Liu, Jia J,Rajan, Hridesh H,Zhang, Wensheng W,Jia, Yan-Bin YB","Yuan, Zhengxiong",N/A,"The rapid growth of mobile devices has spurred the development of crowd-learning applications, which rely on users to collect, report and share real-time information. A critical factor of crowd-learning is information freshness, which can be measured by a metric called age-of-information (AoI). Moreover, recent advances in machine learning and abundance of historical data have enabled crowd-learning service providers to make precise predictions on user arrivals, data trends and other predictable information. These developments lead to a fundamental question: Can we improve information freshness with predictions in mobile crowd-learning? In this paper, we show that the answer is affirmative. Specifically, motivated by the age-optimal Round-Robin policy, we propose the so-called “periodic equal spreading” (PES) policy. Under the PES policy, we first reveal a counter-intuitive insight that the frequency of prediction should not be too often in terms of AoI improvement. Further, we analyze the AoI performances of the proposed PES policy and derive upper bounds for the average age under i.i.d. and Markovian arrivals, respectively. In order to evaluate the AoI performance gain of the PES policy, we also derive two closedform expressions for the average age under uncontrolled i.i.d. and Markovian arrivals, which could be of independent interest. Our results in this paper serve as a first building block towards understanding the role of predictions in mobile crowd-learning.",https://dr.lib.iastate.edu/handle/20.500.12876/Dw88pAKw,Computer science,"Age-of-information,Information freshness,Mobile crowd-learning,Predictions",Can we improve information freshness with predictions in mobile crowd-learning?,thesis
Carl K. Chang,"Jaygarl, Hojun",2018-08-11T12:20:31.000,"<p>Testing object-oriented software is critical because object-oriented languages have been commonly used in developing modern software systems. Many efficient test input generation techniques for object-oriented software have been proposed; however, state-of-the-art algorithms yield very low code coverage (e.g., less than 50%) on large-scale software. Therefore, one important and yet challenging problem is to generate desirable input objects for receivers and arguments that can achieve high code coverage (such as branch coverage) or help reveal bugs. Desirable objects help tests exercise the new parts of the code. However, generating desirable objects has been a significant challenge for automated test input generation tools, partly because the search space for such desirable objects is huge.</p>
<p>To address this significant challenge, we propose a novel approach called Capture-based Automated Test Input Generation for Objected-Oriented Unit Testing (CAPTIG). The contributions of this proposed research are the following.</p>
<p>First, CAPTIG enhances method-sequence generation techniques. Our approach intro-duces a set of new algorithms for guided input and method selection that increase code coverage. In addition, CAPTIG efficently reduces the amount of generated input.</p>
<p>Second, CAPTIG captures objects dynamically from program execution during either system testing or real use. These captured inputs can support existing automated test input generation tools, such as a random testing tool called Randoop, to achieve higher code coverage.</p>
<p>Third, CAPTIG statically analyzes the observed branches that had not been covered and attempts to exercise them by mutating existing inputs, based on the weakest precon-dition analysis. This technique also contributes to achieve higher code coverage.</p>
<p>Fourth, CAPTIG can be used to reproduce software crashes, based on crash stack trace. This feature can considerably reduce cost for analyzing and removing causes of the crashes.</p>
<p>In addition, each CAPTIG technique can be independently applied to leverage existing testing techniques. We anticipate our approach can achieve higher code coverage with a reduced duration of time with smaller amount of test input. To evaluate this new approach, we performed experiments with well-known large-scale open-source software and discovered our approach can help achieve higher code coverage with fewer amounts of time and test inputs.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26100,Computer Sciences,"Automated Testing,Object-oriented Testing,Software Engineering,Software Testing,Test Generation",Capture-based Automated Test Input Generation,dissertation
"Rajan, Hridesh,Gao, Hongyang,Huai, Mengdi","Imtiaz, Sayem Mohammad",N/A,"Modern software is increasingly incorporating a new kind of component, the deep learning
(DL) model, to implement functionalities that have defied traditional programming. Like
traditional components, these DL models also evolve. However, unlike traditional software, there
is a gap in understanding and characterizing changes throughout the DL software evolution. To
fill the gap, we studied 27K revisions from 969 top-rated DL models from GitHub, which have
been developed using the three most popular libraries (i.e., TensorFlow, PyTorch, and Keras). We
developed a taxonomy of changes made during the evolution of DL models. Also, we investigated
the common changes and their intents quantitatively and qualitatively to understand the change
dynamics of DL model evolution. Specifically, what are the common changes made to the model?
How are these changes associated with different stages of the DL pipeline? How are change
intents distributed in the context of DL applications? This thesis paves the way to characterize
the changes in the evolution of DL models by answering those questions. It guides practitioners in
effectively developing and maintaining DL software. Our findings reveal how library design and
default parameter choices can affect the evolution of deep learning models and highlight the
importance of identifying better change operators. We also identify several DL-specific quality
issues addressed by the changes studied, highlighting the need for renewed attention from the
refactoring community and tool developers.",https://dr.lib.iastate.edu/handle/20.500.12876/jrl8Qb6r,Computer science,"Deep Learning,SE4AI,Software Evolution",Characterizing the changes in the evolution of deep learning models,thesis
"Lutz, Jack H,Aduri, Pavan,Chaudhuri, Soma,Fernadnez-Baca, David,Lathrop, James,McNicholl, Timothy","Huang, Xiang",N/A,"In this dissertation we study the computational power of chemical reaction networks (CRNs), under both the deterministic and stochastic semantics of the model. We explore the class of real numbers that are computed in real time by deterministic CRNs. We develop the elements of the theory of algorithmic randomness in continuous-time Markov chains with the aim of applying to stochastic CRNs, which are essentially special cases of CTMCs.

We first introduce the notion of computing a real number in real time. We show that every algebraic number is computable by chemical reaction networks in real time. We also show the real-time equivalent of CRNs and general purpose analog computers (GPACs), which are seemingly more powerful that CRNs. As a by-product of this fact, we give simple and natural constructions for some famous transcendental numbers.

Next we extend the above work to population protocols. We generalize the notion of numbers computed by large population protocols (LLPs) (Bournez, Fraigniaud, and Koegler, 2012). They proved that large population protocols can only compute exactly the algebraic numbers. However, their definition comes with  an extra restriction: the systems must have finitely many fixed points. We relax the finitary restriction and show that we can now compute transcendental numbers.

Our last work is on algorithmic randomness in continuous-time Markov chains. We first define the randomness of trajectories in terms of a new kind of martingale (algorithmic betting strategy). After that we prove equivalent characterizations in terms of constructive measure theory and Kolmogorov complexity. As a preliminary application we prove that, in any stochastic chemical reaction network, every random trajectory with bounded molecular counts has the non-Zeno property that infinitely many reactions do not occur in any finite interval of time.",https://dr.lib.iastate.edu/handle/20.500.12876/5w5p14Xz,Computer science,"analog computing,chemical reaction network,Kolmogorov complexity","Chemical reaction networks: Computability, complexity, and randomness",dissertation
Johnny S. Wong,"Li, Rihui",2018-08-11T06:14:49.000,"<p>Twitter is a novel online microblogging service launched in July 2006. This service has been rapidly gaining worldwide popularity. It has more than 500 million users, out of which there are more than 332 million active users in May 2015. Twitter website is one of the ten most visited websites and has been described as “the SMS of the Internet.” It is not only widely used in a person’s daily life, but also in politics, such as running election campaigns, mining or influencing public opinions.</p>
<p>We study the problem of automated classification of tweets posted on official accounts by a state’s senate and a state’s House of Representatives as well as accounts by individual senators and house representatives, to one of the 21 policy agenda topics specified by Policy Agenda Project [2]. This problem is a multi-class classification problem for short text since each tweet has a limit of 140 characters. Compared with traditional text classification, short text classification has a special characteristic that the content is short and sparse. Therefore, it is very challenging to extract a useful feature for classification. To achieve a reasonable performance, we investigate three methods including Support Vector Machine (SVM) with Linear Kernel, SVM with Topics Grouping and Tweets Merging, and Convolutional Neural Networks (CNN). Based on the experimental results, the CNN method achieved the best performance of 77.34% accuracy on an independent testing set of 1,388 tweets in the Iowa dataset. Furthermore, the CNN method is robust and stable without the need to manually tune the hyper-parameters, which are the attributes of the neural network such as the number of hidden layers, the number of units per layer, and the connections per unit.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29939,"Computer Sciences,Library and Information Science,Political Science","Convolutional neural network,Machine Learning,Policy Agenda Topic,Tweets Classification",Classification of tweets into policy agenda topics,thesis
Carl K. Chang,"Cheng, Shuxing",2018-08-11T09:20:32.000,"<p>In order to capture the business dynamics underlying SOA-based service systems, we propose and formalize the concept of a competitive service market (CSM). A CSM is composed of a set of composite service providers, each managing a collection of atomic service providers. With the help of service composition protocol, composite service providers are able to invoke atomic services and aggregate them into value-added composite services for servicing various types of customers' requests. Centering around the setting of a competitive service market, our research is separated into three parts:</p>
<p>1. Aiming to support the quantitative-based decision processes of different market players, we construct stochastic models to conduct performance analysis at various levels spanning vertically on the structural hierarchy of the service market.</p>
<p>2. In the context of requirements analysis, we classify the concept of service and service instance in terms of their respective functional and non-functional features. Hereafter, we identify the related storage issues and propose a counting Bloom filter-based hybrid storage architecture for the service registry design underlying the service market. A feature-based service discovery protocol is developed to demonstrate the usefulness of this design.</p>
<p>3. The business relationship between different market players are typically framed through the service level agreements (SLAs), which specify the attributes of QoS-based metrics and service costs for the realized service provisioning. SLAs constitute the backbone structure for managing the CSM. We identify several SLA design patterns in terms of different business scenarios that can occur in the life cycle of a service market. Against each pattern we study the corresponding SLA design scheme that can meet its unique requirements. In addition, we systematically investigate the application of Bayes estimator in these schemes, since the knowledge of their negotiation counterpart or market competitors is essential for reaching the goal of utility optimization. At the end, we cast the hybrid SLA design framework into a stochastic model that allows decision makers to obtain evaluations of performance of interest.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26097,Computer Sciences,"bilateral negotiation,performance modeling,service level agreement,services computing","Competitive service market: modeling, storage and management",dissertation
"Aduri, Pavan,Basu, Samik,Dorius, Shawn,Quinn, Christopher,Li, Qi","Rajagopal Padmanabhan, Madhavan",N/A,"Optimization problems involving submodular functions naturally arise in various application domains such as Information Diffusion, Machine Learning, and Sensor Placement. In this thesis, we study the constrained optimization of submodular functions with an emphasis on applications in Information Diffusion.

First, we introduce the SCSK-C problem: Maximize a submodular function with a submodular cost and cardinality constraint. We provide efficient approximation algorithms with data-dependent additive errors. We focus on an application in Information Diffusion by formulating the Constrained Influence Maximization Problem: the objective is to maximize the information spread among the ""Target"" users while the number of ""Non-Target"" users to whom the information reaches is limited by a hard constraint.

Next, we study the problem of identifying and leveraging the ""degree of influence"" of users in social networks i.e., if a user is strongly influenced or weakly influenced. To this end, we develop a mathematical model for measuring the Attitude, the ""degree of influence"". We prove that the Attitude function is monotone and submodular. We provide an approximation algorithm for computing attitude with provable guarantees. We then formulate the Attitude Maximization problem: find a seed set of a given size that will result in the maximum total attitude of the network. We first prove that the Attitude Maximization problem is NP-hard. Based on the monotonicity and submodularity of Attitude, we propose a greedy algorithm that achieves (1-1/e) approximation guarantee.

Finally, we introduce the Diff-C problem, which is to maximize the difference between two submodular functions with an added cardinality constraint. We establish a close relation between SCSK-C and Diff-C. Specifically, we show that solving SCSK-C will allow us to solve Diff-C efficiently. We adapt our techniques that solve SCSK-C and produce efficient approximation algorithms that solve Diff-C. We experimentally evaluate our algorithms for Diff-C with both modular and submodular cost functions.",https://dr.lib.iastate.edu/handle/20.500.12876/9z0Kmb7r,Computer science,"Information Diffusion,Machine Learning,Social Networks,Submodular Optimization",Constrained submodular optimization and applications in information diffusion,dissertation
"Rajan, Hridesh,Chang, Carl,Leavens, Gary,Le, Wei,Lutz, Robyn","Khairunnesa, Samantha Syeda",N/A,"Application programming interfaces (APIs) are indispensable parts of modern software development.
The APIs allow us to avoid reinventing the wheel; however, they also need to be correctly used to enable
the developers to write reliable codes. Contracts specify the proper use of APIs (and methods in general).
Thus, a contract tells the client of a method the obligations that are expected to be satisfied. In return, the
callee method will accommodate the part of the contract to the client (e.g., to the caller method) that it
promises. The formal contract, even informal documentation, can assist the developers in understanding
the functionality of APIs and the right way to use it. Thus, it is crucial to understand and collect the
contracts for APIs that developers use to construct software. In addition, utilizing machine learning (ML)
APIs is becoming more frequent to solve conventional algorithmic problems in contemporary software.
The key reason is the robustness of software based on ML techniques. However, machine learning APIs are
not free of error either. Although a rich body of literature discusses the contracts in non-ML APIs, ML
APIs have not been studied to understand their contracts. This is the first problem the thesis addresses. In this work, we present a new understanding of the type of contracts required for ML APIs to investigate whether the contracts are different from traditional counterpart software APIs. We give an empirical study to document contracts for ML APIs. One of the key insights is that the software engineering community can use some current contract mining approaches to obtain contracts for ML APIs due to the similarity in the type of contracts that ML APIs share with traditional APIs. Still, there might be a necessity to combine behavioral and temporal contract mining approaches that have been independently developed so far. In addition, the contracts for ML APIs also require capturing context information, e.g., output data labels, level of the layer constructing the neural network, etc. We see that along with system failures; such contract breaches result in incorrect functionality, performance issues, etc. As a result, there is an immediate need to formalize the contracts for ML APIs that can capture this type of context information to support reasoning. Interestingly, this problem persists in several application domains, including ML. As a solution to this problem, the second part of the thesis focuses on formalizing the contracts for APIs (or methods in general) that show context dependency. Although the idea of context is similar to what is observed in context-oriented programming, this work focuses on writing contracts for (API) methods that may behave differently depending on the state of the context. Furthermore, the context-dependent methods in question also require maintaining shared context. At this time, there is no explicit way to describe this type of context information directly using state-of-the-art contract languages. We propose a context-aware contract language in this thesis to allow reasoning on methods that involve context information. We present the applicability of such a language, discuss the design in terms of syntax, operational semantics, and type
rules. Finally, we describe an automatic contract mining technique to address the sparse usage problem in usage-based precondition mining. Our insight is to leverage the knowledge that can be understood through the language’s constructs and semantics. And to capture this knowledge, our approach includes a technique to analyze the data and control flow in the program that leads to API calls and infer conditions implicitly present in the code.",https://dr.lib.iastate.edu/handle/20.500.12876/2vaZ4n3r,Computer science,"API,context-aware,contract,machine learning",Context-aware contracts,dissertation
David Fernández-Baca,"Shutters, Brad",2018-07-24T08:09:02.000,"<p>This dissertation addresses some of the algorithmic and combinatorial problems at the interface between biology and computation.</p>
<p>In particular, it focuses on problems in both computational phylogenetics, an area of study in which computation is used to better understand evolutionary relationships, and algorithmic self-assembly, an area of study in which biological processes are used to perform computation.</p>
<p>The first set of results investigate inferring phylogenetic trees from multi-state character data. We give a novel characterization of when a set of three-state characters has a perfect phylogeny and make progress on a long-standing conjecture regarding the compatibility of multi-state characters.</p>
<p>The next set of results investigate inferring phylogenetic supertrees from collections of smaller input trees when the input trees do not fully agree on the relative positions of the taxa. Two approaches to dealing with such conflicting input trees are considered. The first is to contract a set of edges in the input trees so that the resulting trees have an agreement supertree. The second is to remove a set of taxa from the input trees so that the resulting trees have an agreement supertree. We give fixed-parameter tractable algorithms for both approaches.</p>
<p>We then turn to the algorithmic self-assembly of fractal structures from DNA tiles and investigate approximating the Sierpinski triangle and the Sierpinski carpet with strict self-assembly. We prove tight bounds on approximating the Sierpinski triangle and exhibit a class of fractals that are generalizations of the Sierpinski carpet that can approximately self-assemble.</p>
<p>We conclude by discussing some ideas for further research.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27379,Computer Sciences,"agreement supertrees,algorithmic self-assembly,computational phylogenetics,fractal self-assembly,perfect phylogeny,quartet compatibility",Contributions to computational phylogenetics and algorithmic self-assembly,dissertation
N/A,"Richards, Hamilton",2018-08-17T14:09:15.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/78347,Computer Sciences,Computer science,Controlled information sharing in the SYMBOL-2R computer system,dissertation
N/A,"Sweet, Alan",2018-08-16T06:39:47.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/78549,Computer Sciences,Computer science,Correctness in multi-user hierarchically structured information systems,dissertation
"Jannesari, Ali,Huai, Mengdi,Basu, Samik","Chen, Hanze",N/A,"Matching binary to source code and vice versa has various applications in different fields, such as computer security, software engineering, and reverse engineering. Even though there are existing methods that try to match source code with binary code to accelerate the reverse engineering process, most are designed to focus on one programming language. However, in real life, programs are developed using different programming languages depending on their requirements. Thus, cross-language binary-to-source code matching has recently gained more attention. Nonetheless, the existing approaches still struggle to have precise predictions due to the inherent difficulties when the problem of matching binary code and source code needs to be addressed across programming languages.

This paper presents GraphBinMatch, an innovative machine-learning application developed to detect code similarity and clones across different programming languages. The graph neural network-based method presented here addresses challenges in binary-to-source code matching across different programming languages and significantly improves existing methodologies. GraphBinMatch can effectively learn similarities between binary and source codes and accommodate a broad range of programming languages. We evaluated GraphBinMatch across multiple tasks and found that it significantly outperforms current methodologies, with up to a 21\% increase in F1 scores and a 39\% increase in recall compared to state-of-the-art research. Moreover, we demonstrate GraphBinMatch's versatility by introducing an OpenMP bug detection application. While GraphBinMatch is promising, we also candidly discuss its limitations and potential for future work. This comprehensive view offers promising directions for future research.",https://dr.lib.iastate.edu/handle/20.500.12876/kv7kW3yv,Computer science,code clone detection,Cross-language binary-to-source code matching approach using graph neural networks,thesis
"Chowdhury, Ratul,Sarkar, Soumik,Lathrop, James,Shao, Zengyi","Teoh, Yee Chuen",N/A,"SARS-CoV-2 emerged in China In late 2019 and has since had a profound global impact, infecting over 670 million individuals and resulting in more than 6.8 million fatalities by 2023. Although vaccines have demonstrated effectiveness against the native SARS-CoV-2 strain and its variants, the Omicron variant has emerged as the predominant variant due to critical mutations in its spike protein. These mutations have led to an increase in binding affinity to human cell receptor ACE2 (angiotensin-converting enzyme 2), allowing for viral entry and antigen replication within the host. In response, the human immune system generates B-cells and T-cells to protect against the virus. Opsonization is a crucial immune mechanism for antigen elimination, involving binding antibodies produced by B-cells. The survival of the viral antigen relies on its ability to escape this binding process while maintaining its binding affinity with entry receptors like ACE2, often achieved through mutations in the spike protein region. A priori knowledge of a virus's future infective and transmissive strains will enable humans to be therapeutically prepared with escape-proof antibody formulations. To this end, we introduce a simulation platform, CTRL-V (Computational Tracking of Likely Viral Escape variants), that iteratively model the viral escape process of a viral antigenic protein when confronted with human antibodies. We show a test case demonstration in correctly recovering the infective strains of SARS-CoV-2 starting with the wildtype spike receptor-binding domain against known commercial neutralizing antibodies. CTRL-V unveils a putative viral mutational landscape by leveraging only the information about the antibody-antigen complex structure and amino acid interaction preferences in proteins. This information is critical to surveillance and antibody design strategies for preventing viral diseases in humans and livestock.",https://dr.lib.iastate.edu/handle/20.500.12876/RwyqJnpw,Computer science,"Integer Linear Programming,Integer optimization model,Mutation prediction model,SARS CoV-2,Viral protein mutation prediction",CTRL-V: Computational tracking of likely viral escape variants by iterative optimization,thesis
Wensheng Zhang,"Zhang, Jinsheng",2018-08-11T07:36:24.000,"<p>Cloud-based storage service has been popular nowadays. Due to the convenience and unprecedent cost-effectiveness, more and more individuals and organizations have utilized cloud storage servers to host their data. However, because of security and privacy concerns, not all data can be outsourced without reservation. The concerns are rooted from the users' loss of data control from their hands to the cloud servers' premise and the infeasibility for them to fully trust the cloud servers. The cloud servers can be compromised by hackers, and they themselves may not be fully trustable.</p>
<p>As found by Islam et. al.~\cite{Islam12}, data encryption alone is not sufficient. The server is still able to infer private information from the user's {\em access pattern}. Furthermore, it is possible for an attacker to use the access pattern information to construct the data query and infer the plaintext of the data.</p>
<p>Therefore, Oblivious RAMs (ORAM) have been proposed to allow a user to access the exported data while preserving user's data access pattern. In recent years, interests in ORAM research have increased, and many ORAM constructions have been proposed to improve the performance in terms of the communication cost between the user and the server, the storage costs at the server and the user, and the computational costs at the server and the user.</p>
<p>However, the practicality of the existing ORAM constructions is still questionable:</p>
<p>Firstly, in spite of the improvement in performance, the existing ORAM constructions still require either large bandwidth consumption or storage capacity. %in practice.</p>
<p>Secondly, these ORAM constructions all assume a single user mode, which has limited the application to more general, multiple user scenarios.</p>
<p>In this dissertation, we aim to address the above limitations by proposing four new ORAM constructions:</p>
<p>S-ORAM, which adopts piece-wise shuffling and segment-based query techniques to improve the performance of data shuffling and query through factoring block size into design;</p>
<p>KT-ORAM, which organizes the server storage as a $k$-ary tree with each node acting as a fully-functional PIR storage, and adopts a novel delayed eviction technique to optimize the eviction process;</p>
<p>GP-ORAM, a general partition-based ORAM that can adapt the number of partitions to the available user-side storage and can outsource the index table to the server to reduce local storage consumption; and</p>
<p>MU-ORAM, which can deal with stealthy privacy attack in the application scenarios where multiple users share a data set outsourced to a remote storage server and meanwhile want to protect each individual's data access pattern from being revealed to one another.</p>
<p>We have rigorously quantified and proved the security strengths of these constructions and demonstrated their performance efficiency through detailed analysis.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29366,Computer Sciences,"Computer Science,Access Pattern,Data Outsourcing,Privacy",Data access pattern protection in cloud storage,dissertation
N/A,"Parsian, Mahmoud",2018-08-16T20:03:58.000,"<p>Recently, implementation of data structures and correctness proofs of data structure implementations have become important problems in the construction of data abstraction languages, data base systems, and software engineering. The research reported here is primarily concerned with the definition and implementation of data structures, and how to prove an implementation correct;This thesis develops a general technique to implement the source data structure (d1) in terms of the target data structures (d2). That is, given data structures d1 and d2, an implementation of d1 by d2 is defined separately on the syntactical and semantical levels of data structure elements. This work makes a sharp distinction between the specification of a data structure and its implementation. Specification of data structures is considered to be abstract, that is, implementation independent of any specific programming language;This thesis deals primarily with developing criteria for providing provably correct implementation of data structures. The correctness of data structure implementation is developed on two levels: syntactical and semantical. Syntactically, correct implementations deal with algebraic equations (conditional and unconditional) that specify a data structure, while the semantically correct implementations define correctness on the basis of the semantic algebra in the data structure specifications. All the implementations are specified by tree transducers, reducing the problem of implementation to a problem of translation;The issue of tree transducers is addressed on the syntactic and semantic levels. Two key syntactical properties of tree transducers have been investigated. They are, the ""consistency"" and ""semiconsistency"" of tree transducers with respect to the algebraic equations defining the source and target data structures. These properties have been used for the syntactical correctness proof of the implementation of data structures. One of the key results of this thesis is the development of ""syntactically honest"" tree transducers, which is based on a lattice. It has been proved that syntactically honest tree transducers form a base for syntactically correct implementation of data structures.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/81997,Computer Sciences,Computer science,Data structure implementation and correctness,dissertation
"Morris J. Chang,Carl K. Chang","Piyasinghe, Priyangika",2020-02-12T22:59:29.000,"<p>Peer-to-Peer (P2P) botnet is one of the major threats in network security for serving as the infrastructure that is responsible for various cybercrimes. Enterprises routinely collect terabytes of security-relevant data. This proposed work exploits such data to propose a novel Internet-scale P2P botnet detection that fuses big data behavioral analytics in conjunction with graph theoretical concepts. In addition to detecting botnets in large data sets, our method capable of meeting the challenges that incur botnet having encrypted command-and-control (C&C) channels, the stealthy botnet that hard to observe any malicious activities in the network traffic, and botnet with randomized communication patterns.</p>
<p>In a popular botnet-assisted attack scenario, the attacker(s) commands a swarm of bot-infected computers to send out flooding packets to a target server, intending to reduce the services provided by the server, to a state where they cannot be accessed by legitimate users. It is essential to detect these attacks commonly known as Distributed Denial of Service (DDoS) attacks accurately in a timely fashion so that mitigation can be done before a server down.</p>
<p>Apart from detecting the threat, it is important to the organization that they have significant insights about the targeted attack to understand future short and long term trends of an ongoing P2P botnet attack. This helps to quantify attack impacts like intensity and estimated number of compromised machines. The second part of our work focused on using time series analysis to identify those features and provide the capability to recognize the current as well as future similar situations and hence appropriately respond to the threat.</p>
<p>Experimental evaluation for detection and forecasting has demonstrated both high accuracy and great scalability of the proposed system.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31950,Computer Sciences,"big data,DDoS attacks,P2P botnet,quantify attacks,time series analysis",Data-driven approaches for peer-to-peer botnet detection and forecasting,dissertation
N/A,"Padmanabhan, Prasanna",2020-08-05T05:06:42.000,"<p>Today, many software organizations are utilizing product lines as a way of improving productivity, improving quality and reducing development time. When a product family evolves (a new member is added to it), there must be a way to verify whether the new member's specific requirements are met within the reuse constraints of its product family. The contribution of this paper is to demonstrate such a verification process by describing a requirements engineering tool called DECIMAL. DECIMAL is an interactive, automated, GUI driven verification tool that automatically checks for completeness (checking to see if all commonalities are satisfied) and consistency (checking to see if dependencies between variabilities are satisfied) of the new member's requirements with the product family's requirements. DECIMAL also checks that variabilities are within the range and data type specified for the product family. The approach is to perform the verification using a database as the underlying analysis engine. Finally, a pilot study of a virtual reality device driver product family is described which investigates the feasibility of this approach by evaluating the tool.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97558,,Computer science,DECIMAL: a requirements engineering tool for product families,thesis
Andrew S. Miner,"Babar, Junaid",2019-11-04T21:43:17.000,"<p>Symbolic data structures and algorithms are increasingly popular tools for the analysis of complex systems. Given a high-level model of a system, such as a Petri Net, we can automatically verify certain properties about it. In this thesis, we develop data structures and techniques that can be used to improve such analyses.</p>
<p>First, we show how decision diagrams can be used efficiently in traditional explicit generation algorithms. Next, we show how symbolic reachability analysis can be used to detect deadlocks in Petri Nets. We also present a symbolic approach that can detect deadlocks in unbounded Petri Nets.</p>
<p>Finally, we introduce a new type of decision diagram, ESRBDD, that combines multiple reduction rules, is canonical, and produces a more compact representation than previous efforts. We show that operations on ESRBDDs are at least as efficient as those on the underlying decision diagrams and introduce extensions to ESRBDDs that improve on their compactness and operational efficiency.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31576,Computer Sciences,"BDDs,ESRBDDs,Explicit exploration with MDDs,Petri nets,Symbolic deadlock detection,ZDDs",Decision diagrams: Extensions and applications to reachability analysis,dissertation
"Rajan, Hridesh,Ciardo, Gianfranco,Cohen, Myra,Le, Wei,Tian, Jin","Pan, Rangeet",N/A,"Deep learning-based software is prevalently being utilized in various applications, i.e., image classification, autonomous driving, and medical analysis. To build these models, developers collect data, craft the architecture of the model, and finally, train the model with the available data. To make any changes or build a new model, the most common way is to train a model from scratch. This scenario is very similar to traditional software in the pre-modular days with limited reusability. However, with the notion of decomposition, monolithic software is decomposed into modules that eventually make the software development and maintenance process more manageable, flexible, and comprehensible. With that motivation, we ask, can we decompose deep learning models into modules? Could this decomposition lead to fine-grained reusability and replaceability? To answer these questions, in this work, we have shown that it is feasible to decompose fully connected neural networks (FCNN) into modules, one for each output class in the training dataset. Each module takes the same input that the model does but acts as a binary classifier. These decomposed modules can be reused to build a new problem without needing retraining. Also, one module can be replaced by another without affecting other modules. As the next step, we have shown that it is also possible to decompose more complex models, i.e., convolutional neural networks (CNN), into modules. Since edges in an FCNN have a one-to-one relationship between nodes in consecutive layers, whereas, in CNN, edges are shared among the input and output nodes, the previous approach cannot directly be applied to CNN. Also, the previous approach does not work for other CNN-related layers, i.e., merge. To that end, we apply a decomposition strategy that leverages unsharing the shared weight and bias by using a mapping-based technique that stores the position of the nodes that are not part of a module. We also show how these reusable and replaceable modules perform compared to models trained from scratch to solve similar problems. While these two contributions show the possibilities of enabling the two benefits of decomposition, i.e., fine-grained reusability and replaceability, we believe that other benefits can be had, e.g., hiding changes and understanding the logic. We evaluated these decomposition strategies and showed that they do not lead to a significant loss of accuracy when compared to the original models. Also, we found that these decomposed modules can be reused and replaced to build new problems without the need to retrain a model from scratch. To understand how nodes at each hidden layer interact with others, we apply an approach for decomposition that splits a DL model into modules that are connected using three conditional clauses, i.e., AND, OR, and NOT. We call this approach structured decomposition of deep learning models. Finally, we also show how decomposition-based approaches can hide the changes to fewer modules. To do that, first, we identify the changes that a trained DL model undergoes by studying GitHub repositories. As a result, we found 8 types of changes. Then, we evaluated decomposition approaches and found out of 8, 5 changes can be hidden by applying decomposition-based approaches.",https://dr.lib.iastate.edu/handle/20.500.12876/jrl8W4Yr,Computer science,"decomposition,deep learning,deep neural network,modularity",Decomposing deep learning models into modules,dissertation
"Zhang, Zhu,Tian, Jin,Tavanapong, Wallapak,Bao, Sheng,Li, Qi","Yuan, Jie",N/A,"In recent years, natural language processing (NLP) techniques have been advancing rapidly due to the availability of massive amounts of textual data, the advancement of computational resources, and the evolution of deep learning models. They have been employed in a broad application spectrum, such as healthcare, education, and business. In this work, we explore the utility of NLP techniques in a specific business scenario, forecasting market volatility, and examine three different tasks, including representation learning, forecasting volatility with long-sequential data, and model interpretation. In particular, these tasks are addressed by a tensor-product-representation-based topic model (TPR-TM), a long- and short-term memory retrieval architecture (LASER), and a GPT-2 model with an improved beam search algorithm (BEAM).

In the first task, we observed a gap where most topic models have solely leveraged one level (grain) of the latent topic concept to represent and reconstruct documents. To bridge the gap, we propose a new neural topic model, the tensor-product-representation-based topic model (TPR-TM), which can produce more meaningful word and document representations utilizing TPRs of selected high- and low-level latent topic concepts. Then, the learned TPRs are successfully employed in predicting stock market volatility.

In the second task, we noticed that, computationally, deep learning architectures such as recurrent neural networks (RNNs) on extremely long input sequences remained infeasible because of time complexity and memory limitations. We address this challenge by proposing a Long- and Short-term Memory Retrieval (LASER) architecture with flexible memory and horizon configurations to forecast market volatility. 

In the third task, we realized that understanding the inner workings of deep neural networks is challenging due to the largely black-box nature of large neural networks. We tackle the interpretability issue by devising a BEAM algorithm that leverages a large pre-trained language model (GPT-2). It generates human-readable narratives verbalizing the evidence leading to the model prediction. 


In conclusion, empirical studies demonstrate the superior performance of our proposed models in learning multi-level latent topics concepts and meaningful TPRs, predicting high-volatility market scenarios, and generating high-quality narratives compared to existing methods in the literature.",https://dr.lib.iastate.edu/handle/20.500.12876/4vGXMxNr,Computer science,"Deep learning,Market volatility forecasting,Model interpretation",Deep learning-driven market volatility forecasting and model interpretation,dissertation
"Michael C. Dorneich,David Fernandez-Baca","Barnawal, Prashant",2018-08-11T13:35:55.000,"<p>The research study evaluated the effect of different manufacturability feedback modalities on design engineers’ ability to improve the manufacturability of their designs. A manufacturability feedback tool called the Three Dimensional Integrated Feedback (3DIF) has been developed and evaluated, the purpose of which is to provide manufacturability feedback information to design engineers early at the conceptual design stage. Conceptual design is an important factor which determines most of the overall manufacturing cost, resources and time, but design engineers are not manufacturing specialists. Providing early manufacturability feedback to design engineers assist them to improve the manufacturability of their designs. Studies have shown that mode of data representation affects its interpretability. An evaluation study was conducted with design engineers to evaluate how different feedback modalities affected their design performance, usability and workload. Results show that providing feedback in three-dimensional modality significantly improved the design manufacturability with less mental workload compared to textual and no feedback. Providing textual feedback was no better than providing no feedback. This study will benefit manufacturing industries by demonstrating that easy-to-use, three-dimensional manufacturing feedback can significantly improve the design, increase usability, reduce workload, and potentially lower the cost of the design process.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28974,"Cognitive Psychology,Computer Sciences,Engineering","Human Computer Interaction,Computer Science,Design engineers,Design for Manufacturability,DFM,Three Dimensional Integrated Feedback,3DIF,User study",Design and evaluation of feedback system in design for manufacturability,thesis
N/A,"Dorn, Brian",2020-11-13T21:02:55.000,"<p>Static type checking allows programmers to locate potential bugs prior to code execution. However, developing a static type checker is a complicated endeavor. Implementers must address a number of concerns including recursion over syntax elements, unification of type variables within environments, and generation of meaningful error messages for users. The inherent complexity of type checkers can lead to code that is difficult to both understand and maintain. This thesis presents the design and implementation of an abstract type inference engine and its use in the revision of a student-oriented type checker for the Scheme programming language. Our inference engine provides a complete set of unification facilities to programmers for the specification of a type checking system. It allows for a clean separation of unification algorithms, inference rules, and error generation. We also demonstrate the applicability of the engine by using it to construct a type checker for Scheme targeted at novice programmers. This checker borrows a student-friendly type notation from a previous version and extends its system, providing for language native module support, a more complete treatment of advanced data types, and better error messages.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97889,,Computer science,Design and implementation of a reusable type inference engine and its application to Scheme,thesis
N/A,"Tran, Minh",2020-08-05T05:04:51.000,"<p>Periodic broadcast is an effective paradigm for large-scale dissemination of popular videos. In the periodic broadcast paradigm, a video file is logically partitioned into a number of segments. These segments are periodically broadcast (using mulitcast) on the server channels. A client tunes into one or more channels at proper times to download the video segments into the client disk buffer. The client typically switches channels to download subsequent segments while playing out one of the buffered segments. Periodic broadcast guarantees a bounded service delay, which is equal to the length of time to broadcast the first segment, regardless of the number of concurrent requests making it suitable for popular videos. Considerable research efforts have gone into designing many excellent periodic broadcast protocols in terms of minimizing the server network bandwidth and the client resources. However, there are only a few implementations of periodic broadcast protocols available. This is probably because little has been documented on how the memory and disk bandwidth resources of a periodic broadcast server should be allocated. In this thesis, we present a Generalized Periodic Broadcast Server (GPBS) model that supports any periodic broadcast protocol. Based on this model, we formulate and solve a new optimization problem whose solution provides insights into the server's memory and disk resources allocation. We use our analysis to estimate (i) the effect of keeping some video segments in the server memory during the entire broadcast of the video, and (ii) the effect of data placement on disk in periodic broadcast servers. We also discuss our prototype implementation of GPBS. Our work facilitates future implementation and deployment of many existing periodic broadcast protocols.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97434,,Computer science,Design and implementation of periodic broadcast video servers,thesis
"Shashi K. Gadia,Johnny S. Wong","Zhang, Qian",2018-08-24T23:27:53.000,"<p>This thesis presents the design and implementation of XML_based Linux File System Runner (XML_LFS), a file system simulator that integrates the representation ability of Extensible Markup Language (XML) with the beauty of Linux file system architecture. XML_LFS uses a layered approach to design a generic file system runner from scratch utilizing Java programming language and JDOM. The hierarchical directory structure of the file system is kept in an XML file for easy manipulation as well as on disk for crash recovery. UNIX-like file systems such as the Second Extended File System (Ext2), a native mini file system (mini3fs) and Linux kernel codes for file system operations are explored for the real implementation work.;Traditional file system consists of a hierarchical tree, composed of directories and files. Each directory can contain both files and subdirectories. This is an equivalent concept to ""semi-structured"" elements in XML. Embedding an XML log file layer into the Linux file system architecture can speed up the directory look up by combining the power of XML and XQuery as well as eliminating the limitations of the existing fixed-attribute file system model by treating files as elements to a customizable XML document. Thus, the whole development environment is more useful for future file system research. The future of XML file system is discussed in detail. Complete system architecture and functionalities are built and the process is described in the thesis. Initial Bonnie-like and Andrew-like benchmarks of the prototype implementation show that XML_LFS achieves the expected performance results.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/67224,Computer Sciences,Computer science,Design and implementation of XML-based Linux file system runner,thesis
"Rajan, Hridesh,Tavanapong, Wallapak,Aduri, Pavan,Gao, Hongyang,Li, Qi","Ahmed, Shibbir",N/A,"Deep Learning (DL) is increasingly used in critical software, but these systems are often black boxes. When a DL model predicts an output for a given input, it is unclear whether that output can be trusted, especially if the input comes from an unknown region, one that the model may not have been trained on. Therefore, trained DL models pose challenges in ensuring trustworthy predictions during deployment. Furthermore, DL systems are prone to bugs, which can be prevented by specifying contracts. Due to the complexity of DL model architecture and expected output, existing methods for specifying traditional software are insufficient for specifying DL software. This dissertation presents some innovative techniques for ensuring the reliability of DL model's output. In the first approach, DeepInfer, data precondition has been introduced. Data preconditions, derived from a trained DL model's weights and biases, determine the trustworthiness of the model's predictions during deployment by checking precondition violation of unknown input. We extensively evaluated DeepInfer on 29 real-world DNN models using four different datasets, demonstrating its utility, effectiveness, and performance improvements, significantly outperforming SelfChecker and being 3.27 times faster. Second, a feature debugging technique has been proposed that determines feature importance by inferring the trained model's assumption about the input features. Therefore, our proposed technique enhances the explainability of the DL model without retraining or modifying the model, allowing developers to debug models by removing less important features. In the evaluation, using real-world data from prior work, our proposed technique successfully identifies important features in both classification and regression models, demonstrating high recall and significantly faster performance than comparable state-of-the-art techniques. Third, to ensure the reliability of DL software, we have introduced contract layer to the deep learning library to intercept API calls, which enables checking the contracts to detect and fix bugs in deep learning programs that cause unreliable output and are not detected by DL libraries. Our approach identified 259 performance bugs out of 272 real-world buggy programs efficiently with less overhead compared to existing tools, and user surveys demonstrated its effectiveness for DL application debugging. Lastly, this dissertation provides a comprehensive investigation and evaluation of our proposed techniques, emphasizing their effectiveness, efficiency, and limitations, and also suggests some future directions in the exciting field of Software Engineering for trustworthy AI.",https://dr.lib.iastate.edu/handle/20.500.12876/OrD8MVVr,Computer science,"API contracts,Deep learning,Deep neural networks,Weakest precondition",Design by contract for deep learning APIs and models,dissertation
"James I. Lathrop,Robyn R. Lutz","Ellis, Samuel",2018-08-12T01:18:04.000,"<p>The programming of matter (molecular programming) is often realized through DNA strand displacement (DSD). Computations and algorithms using DNA strand displacement are often modeled using chemical reaction networks (CRN) that abstract away DNA specific reactions and terminology. These CRNs can yield complex behavior similar to computer programs. If these molecular programs are further used in vivo to effect cell change or fight disease, the molecular program becomes a safety critical system.</p>
<p>In this paper we propose and analyze a molecular watchdog timer, based on a software component often used to monitor the health of a critical system. Using goal-oriented requirements engineering and a stochastic CRN model (SCRN) we design a watchdog timer using two components, a delay clock and a threshold detector. The models are directed, informed, and verified by use of a probabilistic model checker.</p>
<p>Analyzing requirements and the design uncovered several defects that were addressed in subsequent iterations. During each phase, the system was modeled using formal verification tools and simulations to verify correctness of the model. It was found that this iterative methodology with verification was potent at illuminating requirement and design flaws. In addition, the final verified model helped determine specific parameters and molecules for initial biological experiments.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28035,Computer Sciences,"Chemical Reaction Networks,Goal-Oriented Requirements Engineering,Molecular Programming,Software Safety,Watchdog Timer",Designing a molecular watchdog timer for safety critical systems,thesis
"Jannesari, Ali,Cohen, Myra,Le, Wei,Chang, Carl,Quinn, Christopher","Phan, Hung Dang",N/A,"In recent times, the applications of Natural Language Processing (NLP) models have inspired numerous
researchers to propose various automated Software Engineering (SE) models, including software estimation,
natural language to code translation, and natural language to code search. A major challenge in achieving high
accuracy in SE tasks with NLP models stems from the distinct characteristics of Software Engineering artifacts
compared to typical NLP artifacts. This thesis analyzes two types of SE artifacts: software documentation
and code snippets. In this work, I aim to optimize the performance of Large Language Models (LLMs) in SE
through two research directions. In the first, I propose several representation techniques for SE artifacts to
accurately reflect their characteristics as inputs for SE models. In the second direction, I enhance machine
learning model pipelines by adapting them to our new artifact representations and incorporating classical
machine translation models to leverage their flexibility in code generation for automated SE tasks.

The practical outcomes of my research are three engines addressing two SE tasks. The first is HeteroSP,
a Heterogeneous Graph Neural Networks model for estimating story points from software issues, which
surpasses the traditional software estimation approach using the Recurrent Neural Network model, DeepSE.
The second is ASTTrans-CS, a code search model that leverages query-to-ASTTrans Representation through
Neural Machine Translation to enhance well-known Information Retrieval models like GraphCodeBERT and
UniXcoder, thereby improving code search accuracy in the newly curated CAT benchmark of code queries
and code snippets. Addressing the limitations of ASTTrans, which performs better with smaller datasets of
fewer than 5000 code snippets, I introduce Oracle4CS. This approach integrates the more advanced code
representation technique, ASTSum, and another pipeline utilizing Statistical Machine Translation for code
search on the standard CodeSearchNet dataset. Through evaluation, I demonstrate that classical Statistical
Machine Translation can significantly enhance the code search process when used as an oracle to enrich
input queries. My proposed artifact representations and pipelines show promising potential to enhance the
performance of NLP models in SE tasks.",https://dr.lib.iastate.edu/handle/20.500.12876/ywAbZ2kv,Computer science,"Code Search,Heterogeneous Graph Neural Networks,Large Language Model,Neural Machine Translation,Statistical Machine Translation,Story Point Estimation",Designing artifact representation and automated pipeline for machine learning based Software Engineering,dissertation
Robyn R Lutz,"Xiong, Wandi",2020-09-23T19:13:23.000,"<p>Software product lines evolve over time, both as new products are added to the product line and as existing products are updated. This evolution creates unintended as well as planned changes to Systems. A persistent problem is that unintended changes are hard to detect. Often they are not discovered until testing or operations. Late discovery is a problem especially in safety-critical, cyberphysical product lines such as avionics, pacemakers, and smart-braking systems, where unintended</p>
<p>changes may lead to accidents.</p>
<p>This thesis proposes an approach and a prototype tool to detect unintended changes earlier in development of a new product in the product line. The capability to detect potentially risky, unintended changes at the design stage is beneficial because repair is easier, less costly, and safer in design than when detection is delayed to testing or operations.</p>
<p>The Product Line Change Detector (PLCD) introduced here analyzes products’ SysML block and parametric diagrams, which are typical project artifacts for cyber-physical systems, in order to detect problematic, unintended changes. The PLCD software automatically detects potential change-related issues, ranks them in terms of severity using the products’ safety-analysis artifacts, and reports them to developers in a graphical format. Developers select and fix the reported issues with the assistance of the tool’s displays, with the tool recording the fixes and updating the SysML diagrams accordingly.</p>
<p>The evaluation of PLCD’s performance and capabilities uses three product lines, extended from cyber-physical systems in the literature: NASA astronaut jetpack, vehicle dynamics, and low-earth satellite. The evaluation focuses on unintended changes that cause physical unit inconsistencies, such as between meters and feet, since those may lead to accidents in cyber-physical product lines. The evaluation results show that PLCD successfully detects such unintended changes both in a single product and between products in a software product line.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/94399,,"Cyber-Physical System,Software Product Line,SysML,Unit Inconsistency",Design-time detection of physical-unit changes in product lines,thesis
"Rajan, Hridesh,Dyer, Robert,Li, Yang","Batole, Fraol Ahmed",N/A,"Deep Learning (DL) is widely used in various applications, ranging from healthcare to chatbots. However, violations of usage protocols in these applications can lead to severe consequences, such as inaccurate predictions and system failures. To address this issue, researchers have proposed several techniques for detecting and preventing bugs in deep learning programs. However, existing static analysis tools only focus on undefined variables and shape-related bugs or do not consider a crucial property of DL programs, such as data dependency between layers. To address this challenge, we propose NeuralState, an approach to detect performance and program crash bugs in a DL program. NeuralState follows a four-step process: (i) gather specifications for Deep Learning operations from different sources; (ii) introduce abstract states to represent these Deep Learning operations; (iii) design formal rules for transitioning between states based on the specifications; (iv) utilize a combination of standard analysis techniques (i.e., typestate and value propagation) to identify bugs in a DL program. Our evaluation using a real-world benchmark that contains 45 real-world buggy programs shows that NeuralState can precisely detect more bugs than the state-of-the-art tool, NeuraLint. Specifically, it shows a 25% relative improvement in precision and 63% relative improvement in recall when compared with an existing technique.",https://dr.lib.iastate.edu/handle/20.500.12876/9z0KE97r,Computer science,"Deep learning,Static Analysis",Detecting protocol violations in deep learning programs using typestate and value propagation analysis,thesis
"Mitra, Simanta,Prabhu, Gurpur,Chang, Carl","Cai, Tenson",N/A,"Nowadays, software applications operate on a massive scale in terms of features and the number of customers they serve. As software applications become increasingly complex, it becomes increasingly difficult to modify the source code without the application becoming bloated and disorganized. In response, software engineers are continually designing software architectural patterns and concepts to enhance code organization and productivity. One very popular concept is the plugin system architecture, which allows developers to add features and functionalities to an application without modifying the core application itself. This research delves into different design patterns and components of plugin systems and an implementation of a plugin system for Spring Boot, a popular tool used by Java developers to develop enterprise web applications.",https://dr.lib.iastate.edu/handle/20.500.12876/WwPgDVEz,Computer science,"plugin,spring boot",Developing a plugin framework for spring boot,thesis
"Les Miller,Masha Sosonkina","Gulabani, Teena",2018-08-11T06:05:18.000,"<p>Three major high performance quantum chemistry computational packages, NWChem, GAMESS and MPQC have been developed by different research efforts following different design patterns. The goal is to achieve interoperability among these packages by overcoming the challenges caused by the different communication patterns and software design of each of these packages. A chemistry algorithm is hard to develop as well as being a time consuming process; integration of large quantum chemistry packages will allow resource sharing and thus avoid reinvention of the wheel. Creating connections between these incompatible packages is the major motivation of the proposed work. This interoperability is achieved by bringing the benefits of Component Based Software Engineering through a plug-and-play component framework called Common Component Architecture (CCA). In this thesis, I present a strategy and process used for interfacing two widely used and important computational chemistry methodologies: Quantum Mechanics and Molecular Mechanics. To show the feasibility of the proposed approach the Tuning and Analysis Utility (TAU) has been coupled with NWChem code and its CCA components Results show that the overhead is negligible when compared to the ease and potential of organizing and coping with large-scale software applications.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25544,Computer Sciences,"Common Component Architecture,Component Based Software Engineering,High Performance components,NWChem,QM/MM,Quantum Chemistry",Development of high performance scientific components for interoperability of computing packages,thesis
"Jia, Yan-Bin,Fernandez-Baca, David,Tian, Jin,Oliver, James,Stoytchev, Alexander","Mu, Xiaoqian",N/A,"Dexterous robotic manipulation aims at achieving human-like agility and dexterity.  In this thesis, we focus on two manipulation tasks:  batting and cutting.  Both tasks require the robot to react in an accurate, fast, and smooth way as a human does.  

Batting a flying object to a target is a skillful maneuver.  In robotic manipulation, precision batting remains one of the most challenging tasks as it leverages computer vision, modeling, motion planning, and robotic control, etc.  We investigate targeted batting by a two-degree-of-freedom robotic arm, assuming all its movements to take place in one vertical plane.   With the object’s pre-batting configuration predicted, the robot-object impact can be solved under Coulomb friction and energy-based restitution. Motion planning for the robot is conducted under constraints from impact dynamics, projectile flight mechanics, robot kinematics, etc.  Planning is executed repeatedly upon every new estimate of the object’s motion.  Meanwhile, the robot keeps adjusting its motion toward the final batting configuration.  Experiments with different objects have claimed better performance than a human without training.

Skills of cutting natural foods are important for the robot looking to play a bigger role in kitchen assistance. In addition to achieving material fracture, there is an objective of executing fast and smooth motions of the kitchen knife, which in the process performs work to overcome material toughness, act against friction, and generate deformation on the object. To imitate human-like cutting, we decompose the cutting action into three phases: pressing, touching, and slicing.  Position, impedance, and force controls are applied, either separately or jointly, in the three phases to cope with evolving contacts between the knife and the object or between the knife and the cutting board. In pressing, data acquired by a force/torque sensor can be used to estimate the object's properties such as Poisson's ratio, fracture toughness, and the product of the coefficient of blade-object friction and pressure distribution. These properties are all specific to the object and vary with its freshness and the portion undergoing fracture.  The estimated property values can be applied in knife trajectory optimization in the same phase of pressing. Moreover, they can also be used for control purposes in the phase of slicing. Experiments conducted on several types of fruits and vegetables have exhibited natural cutting movements like those performed by a human hand. Cutting along the planned “optimal” trajectories has claimed less work than along those with constant knife moving directions.",https://dr.lib.iastate.edu/handle/20.500.12876/YvkAodjz,"Robotics,Computer science","Dexterous Robotic Manipulation,Robotic Batting,Robotic Cutting",Dexterous robotic manipulation: Batting and cutting,dissertation
"Stephen Gilbert,Les Miller","Guo, Enruo",2018-08-11T17:18:01.000,"<p>This work first presents two implementations of Intelligent Tutoring Systems (ITSs) on engineering undergraduate-level diagram education: StaticsTutor for free-body diagram and Thermo Cycle Tutor for refrigeration T-v diagram. Initial investigations on several groups of students have shown their educational effectiveness.</p>
<p>Unlike text-based input, diagram has some intrinsic challenges that lead it hard to teach. One example is conceptual knowledge is highly interconnected with procedural knowledge. Learned from the two ITSs, we provided some general pedagogical guidelines for the future Diagram-based ITSs.</p>
<p>Also, we learned classes can be used as a way of representing geometric shapes in diagrams. Thus, we extended our work to the generality of how the current approach can be applied to other domains. We chose a popular type of diagram, called Block Diagram, which contains geometric objects and lines/arrows in connecting them. We developed a methodology to represent a diagram’s information and an ontology of diagram evaluation processes to diagnose students' diagrams. Our work contributes to the development of Diagram-based ITSs authoring tools.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28767,"Artificial Intelligence and Robotics,Computer Sciences","Computer Science,Human Computer Interaction,Diagrams,Intelligent Tutoring Systems",Diagram-based intelligent tutoring systems,dissertation
Iddo Friedberg,"Banerjee, Priyanka",2021-06-11T00:46:18.000,"<p>Genomic islands(GIs) are clusters of genes that are acquired during the Horizontal Gene Transfer process(HGT) by bacterial genomes. These islands play a crucial role in the evolution of bacteria by helping them adapt to changing environments. The detection of GIs is therefore an important problem in medical and environmental research. There have been many previous studies on computationally identifying GIs, but most of the studies rely on either closely related genomes or annotated nucleotide sequences with predictions based on a fixed set of known features. Previous research on unannotated sequences has not been able to reach a good accuracy due to the lack of information taken into account while prediction and lack of GI boundary detection method. In this thesis, I present a machine learning-based framework called TreasureIsland, that uses an unsupervised representation of DNA sequences to predict GI.  I propose to improve the boundary detection problem of GI by using a boundary fine-tuning method to attain better precision. I evaluate the efficiency of my framework by using a reference dataset obtained by the comparative genomics method and from the literature. The evaluations show that this framework was able to achieve a high recall and accuracy when compared to other GI predictors.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/nrQBQW4z,,"DNA sequence,fine-tune,Genomic Islands,machine learning,recall,unsupervised representation",Discovering genomic islands using DNA sequence embedding,thesis
"Tian, Jin,Andorf, Carson M,Aduri, Pavankumar R,Prabhu, Gurpur M,Song, Guang,Tavanapong, Wallapak","Cho, Kyoung Tak",N/A,"The history of science is the history of finding true belief from observations. Humans build knowledge from observations, experiences, and/or other knowledge. Sometimes, it is difficult to understand a phenomenon with superficial observation. Observed data is the key to solve problems around us. As the size and complexity of data grows, so does the need to accurately process the data. There is now an opportunity to apply artificial intelligence and machine learning approaches to large-scale multi-omic data sets in new areas such as agriculture and crop improvement. In this study, we focus on maize (Zea mays L.) genomic data, and applied machine learning approaches to discover meaningful features from the genomic data. First we explored the relationship between phenotype and genotype in the maize genome by building a database of images and genomes called MaizeDIG. Second, we applied the k-mer concept to construct machine learning frameworks for predicting gene expression using gene sequences. We describe two k-mer Naive Bayes classifiers for genomic sequence classification: k-mer Naive Bayes (NB(k)) and two-phase k-mer Naive Bayes (tNB(k)). Finally, we extend NB(k) and tNB(t) methods and propose new methods to represent sequence: k-mer distance model and reduced k-mer alphabet model.

NB(k) only considers relative frequencies of each respective alphabet under Naive Bayes classifier. To better represent the complexity of protein structure, we propose a new k-mer distance model. We constructed a distance matrix with the distance between all pairwise k-mers in a sequence, and it can be used to measure or compare two gene sequences in terms of similarity. Since the size of the amino acid alphabet affects the complexity and therefore limits k-mer size we propose the reduced k-mer alphabet model. Instead of the NB(k) approach based on the traditional 20 amino acids, we applied the k-mer method with smaller-sized groupings based on physico-chemical properties of amino acids or randomly generated reduced alphabet groupings. Our machine learning approaches allow researchers to predict when and where genes are expressed in the absence of experimental data and make links between genomic and phenotypic data. These predictions and linkages can be used to better understand the relationship between the genes in a plant and the traits observed in farmers' fields.",https://dr.lib.iastate.edu/handle/20.500.12876/ywAbO0gv,"Computer science,Bioinformatics","k-mer distance,k-mer Naive Bayes,MaizeDIG,MaizeGDB,predicting gene expression,sequence based prediction",Discovering relationships between genotype and phenotype: Machine learning approaches,dissertation
Yan-Bin Jia,"Guo, Feng",2018-08-11T03:26:41.000,"<p>Robotic grasping of deformable objects is inherently different from that of rigid objects, and is an under-researched area. Difficulties arise not only from expensive deformable modeling, but also from the changing object geometry under grasping force.</p>
<p>This dissertation studies strategies of grasping deformable objects using two robotic fingers. Discovering the inapplicability of the traditional force-centered grasping strategies for rigid objects, I have designed an approach for grasping deformable objects that specifies finger displacements. This not only ensures equilibrium under the elasticity theory, but also enhances stability and simplifies finger control in the implementation.</p>
<p>Deformable modeling is carried out using the Finite Element Method (FEM), for which our analysis establishes uniqueness of the shape of a grasped object given the finger displacements. Meanwhile, preprocessing based on the Singular Value Decomposition greatly reduces the complexity of computation. Grasping strategies have been investigated for both 2D and 3D objects. With a hollow 2D object, the grasping fingers make point contacts. The condition of a successful grasp is that the friction cones at the two contacts must contain the line segment through them before and after the deformation. With a solid planar object, the fingers make area contacts. Grasp computation is carried out by an event-driven algorithm, which has been validated by our robot experiments. For 3D objects, a simple squeeze-and-test strategy has been designed to lift them off the supporting table against gravity with a method that predicts the squeeze amounts.</p>
<p>In reality, objects shapes are affected to various degrees by gravity, but such a effect has been ignored in the FEM-based modeling. For accuracy, the gravity-free shape of an object is sometimes needed. I have introduced an iterative algorithm that will converge to such shape as a ""fixed point"".</p>
<p>In the last part of my thesis, I study planning of the finger squeeze paths, not to limited by straight movements. The objective is to not only enlarge the range of finger placements for successful grasps, but also improve stability and energy efficiency. I have designed a path planning algorithm based on the Rapidly-exploring Random Trees (RRT) that is able to achieve certain optimalities.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28322,Robotics,"Computer Science,Deformable,Displacement-based,Finite Element Method,Grasping,Planning",Displacement-based grasping of deformable objects,dissertation
Gurpur M. Prabhu,"Mukkavilli, Lakshmankumar",2018-08-15T08:00:06.000,"<p>During the past decade there has been a tremendous surge in understanding the nature of parallel computation. A number of parallel computers are commercially available. However, there are some problems in developing application programs on these computers;This dissertation considers various issues involved in implementing parallel algorithms on Multiple Instruction Multiple Data (MIMD) machines with a bounded number of processors. Strategies for implementing divide-and-conquer algorithms on MIMD machines are proposed. Results linking time complexity, communication complexity and the complexity of divide-and-combine functions of divide-and-conquer algorithms are analyzed. An efficient criterion for partitioning a parallel program is proposed and a method for obtaining a closed form expression for time complexity of a parallel program in terms of problem size and number of processors is developed.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/82670,Computer Sciences,Computer science,Divide-and-conquer algorithms for multiprocessors,dissertation
Samik Basu,"Chatterjee, Swapnanjan",2018-08-11T13:09:00.000,"<p>Attack graphs provide formalism for modelling the vulnerabilities using a compact representation scheme. Two of the most popular attack graph representations are scenario attack graphs, and logical attack graphs. In logical attack graphs, the host machines present in the network are represented as exploit nodes, while the configurations (IDS rules, firewall policies etc.) running on them are represented as fact nodes. The actual user privileges that are possible on each of these hosts are represented as privilege nodes.</p>
<p>Existing work provides methods to analyze logical attack graphs and compute attack paths of varying costs. In this thesis we develop a framework for analyzing the attack graph from a defender perspective. Given an acyclic logical dependency attack graph we compute defense policies that cover all known exploits that can be used by the attacker and also are preferred with respect to minimizing the impacts. In contrast to previous work on analysis of logical attack graphs where quantitative costs are assigned to the vulnerabilities (exploits), our framework allows attack graph analysis using descriptions of vulnerabilities on a qualitative scale. We develop two algorithms for computing preferred defense policies that are optimal with respect to defender preferences. Our research to the best of our knowledge is the first fully qualitative approach to analyzing these logical attack graphs and formulating defense policies based on the preferences and priorities of the defender.</p>
<p>We provide a prototype implementation of our framework that allows logical attack graphs to be input using a simple text file (custom language), or using a GUI tool in graphical markup language (GML) format. Our implementation uses the NVD (National Vulnerability Database) as the source of CVSS impact metrics for vulnerabilities in the attack graph. Our framework generates a preferred order of defense policies using an existing preference reasoner. Preliminary experiments on various attack graphs show the correctness and efficiency of our approach.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28141,Computer Sciences,"attack graph,defense policy,logical attack graphs,network,preference,security",DrAGON: A Framework for Computing Preferred Defense Policies from Logical Attack Graphs,thesis
Ting Zhang,"Wang, Wanwu",2018-08-11T18:53:54.000,"<p>Model checking has attracted considerable attention since this technique is an automatic technique for verifying finite state concurrent systems. It is a formal method to verify if a software system or hardware system meets its properties. Nowadays, model checkers have become indispensable tools in hardware and software design and implementation, since they can reduce human efforts. Time and feasibility of the model checking process depends up on the size and complexity of the formal system model. However, the state space explosion problem still remains a major hurdle, as the number of global states can be enormous. There are many methods to improve the speed of model checkers and abstraction technology is one of them. Abstraction amounts to removing or simplifying details, as well as removing entire components of the concrete model irrelevant to the specifications. In practice, abstraction-based methods have been essential to verify designs of different fields of industrial complexity. Manual abstraction is ad hoc and error-prone; hence, automatic abstraction strategies are desirable for verifying actual hardware and software design.</p>
<p>This thesis presents a new approach to check the model using two abstractions---Universal Abstraction and Existential Abstraction. These new techniques can check both the Existential fragment of Computational Tree Logic (ECTL) and the Universal fragment of Computational Tree Logic (ACTL) specifications. I developed a Model Checker, called LOTUS, building upon these new techniques with a traditional fixed point algorithm on Linux. Experimental results confirmed the feasibility and validity of this new dynamic model checking technique. The input grammar is designed and implemented using Bison and YACC on Linux. The process for this new technique follows. First, automatically construct two abstraction models according to the specifications defined in the input file by the user. Second, LOTUS verifies whether the abstraction model satisfies the specifications and outputs the conclusion. Lotus can produce the final output, if the conclusion is credible; otherwise, refine the two abstraction models according to the counterexamples or witnesses produced by the abstraction model. This process is repeated until the abstraction model is equivalent to the concrete model or the authentic conclusion can be obtained. In this thesis, I aim to provide a complete picture of this dynamic model-checking algorithm, ranging from design details to implementation-related issues and experiments of the Philosopher Dinning Problem.</p>
<p>The main contributions of this approach include three aspects. First, Dynamic Abstraction Algorithm can check both ECTL and ACTL within abstraction methods. Second, a transition abstraction is introduced in this thesis with the purpose is to make the model checker easier to implement. Third, refinement of both abstraction models according to either witness or counterexample is actually modifying the transitions. This method may reduce time and space consumption.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27484,Computer Sciences,,Dynamic abstraction model checking,thesis
"Rozier, Kristin Y,Jones, Phillip H,Wongpiromsarn, Tichakorn","Johannsen, Christopher",N/A,"Specifying and monitoring temporal properties over sets on real-time embedded systems requires a logic that offers sufficient expressiveness and acceptable worst-case performance. If a system designer chooses to use a non-first-order logic, they sacrifice expressiveness; if they choose a first-order logic, they sacrifice performance. To mitigate this tradeoff, we present a first-order variant of Mission-time Linear Temporal Logic (MLTL) that can specify a wide range of behaviors and offers efficient monitoring of those behaviors. We also present a set of MLTL rewrite rules and use equality saturation to optimize MLTL monitor encodings automatically. After applying equality saturation to a set of human-authored MLTL formulas, our experimental evaluation found a ~35% average monitor size reduction.",https://dr.lib.iastate.edu/handle/20.500.12876/avVOm01r,Computer science,"Formal methods,Formula rewriting,MLTL,Runtime verification,Specification language,Temporal logic",Dynamic set reasoning: Specifying and optimizing monitor encodings,thesis
Lu Ruan,"Zheng, Yanwei",2018-08-11T22:14:17.000,"<p>Compared with traditional WDM network, OFDM-based flexible optical networks are able to provide better spectral efficiency due to its flexible allocation of requests on fine granularity subcarrirers. Survivability is a crucial issue in OFDM-based flexible optical networks. In [19], Ruan and Xiao propose a new survivable multipath provisioning scheme (MPP) that provides flexible protection levels in OFDM-based flexible optical networks. They also studies the static Survivable Multipath Routing and Spectrum Allocation (SM-RSA) problem which aims to accommodate a given set of demands with minimum utilized spectrum. It is shown that the MPP scheme achieves higher spectral efficiency than the traditional single-path provisioning (SPP) scheme. In this thesis, we study the dynamic SM-RSA problem, which allocates multiple routes and spectrum for a given demand as it arrives at the network. We develop an ILP model for the problem as well as a heuristic algorithm. We conduct simulations to study the advantage of MPP over SPP for dynamic traffic scenario in terms of blocking performance and fairness. We also compare the performance of the MPP heuristic algorithm and the ILP model.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27685,Computer Sciences,,Dynamic survivable multipath provisioning in OFDM-based flexible optical networks,thesis
Jack Lutz,"Nandakumar, Satyadev",2018-08-11T18:58:48.000,"<p>This thesis establishes some results in algorithmic information</p>
<p>theory. Martin-Lof in 1966 formalized the notion of what it means</p>
<p>for a given sequence to be random, using the theory of</p>
<p>computation. Equivalent, alternate definitions justify that this</p>
<p>definition is robust. A finite string is called random if there is no</p>
<p>short program which outputs the string - that is, the Kolmogorov</p>
<p>complexity of the string is within an additive constant of the length</p>
<p>of the string itself. With the introduction of a suitable notion</p>
<p>termed self-delimiting Kolmogorov complexity, an infinite sequence is</p>
<p>called random if almost all its finite prefixes are incompressible in</p>
<p>the above sense. Algorithmic information theory studies the properties</p>
<p>of such random sequences.</p>
<p>It is natural to enquire whether a sequence random according to this</p>
<p>definition satisfies the well-known probabilistic laws. Early results</p>
<p>by van Lambalgen and Vovk established that all random sequences obey</p>
<p>Borel-Cantelli Lemmas, the Strong Law of Large Numbers and the Law of the Iterated Logarithm. van Lambalgen conjectured that Birkhoff's</p>
<p>ergodic theorem, however, will resist such effectivization. Nevertheless, V'yugin in a tour-de-force established an effective version of the Ergodic Theorem. V'yugin's version has a technical limitation, however, that restricts its applicability. Random variables are assumed to be computable, and hence continuous. In the first part of the thesis, we extend V'yugin's theorem to handle a restricted class of discontinuous functions, and prove that we can apply it to effectivize some results on continued fractions.</p>
<p>The concept of dimension gives a quantitative measurement of how</p>
<p>non-random sequences or sets are. In subsequent chapters we deal with the concept of dimension, in various resource-bounded settings. We</p>
<p>give an alternate characterization of constructive Hausdorff and</p>
<p>constructive packing dimension using generalizations of tools used in</p>
<p>establishing the effective ergodic theorem.</p>
<p>The final two chapters deal with finite-state dimension, which can be</p>
<p>seen as density of information as measured by finite-state</p>
<p>machines. This ties to a classical notion of normality of numbers. We</p>
<p>prove that multiplication by a rational number does not alter the</p>
<p>finite-state dimension of the base-$k$ expansion of a real number,</p>
<p>generalizing and giving a new proof of Wall's classical theorem that</p>
<p>multiplication by a non-zero rational does not alter the normality of</p>
<p>a number.</p>
<p>In the last chapter, we give constructions of normal and</p>
<p>non-normal Liouville numbers, a well-known class of transcendental</p>
<p>numbers.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25199,Computer Sciences,"Algorithmic Information Theory,Ergodic Theorem,Liouville numbers,Normal Numbers,Resource Bounded Dimension,Resource Bounded Measure","Dynamics, measure and dimension in the theory of computing",dissertation
Jin Tian,"Guha Thakurta, Kristima",2018-08-11T17:13:39.000,"<p>Causal structure discovery is a much-studied topic and a fundamental problem in Machine Learning. Causal inference is the process of recovering cause-effect relationships between the variables in a dataset. In general, causal inference problem is to decide whether X causes Y, Y causes X, or there exists an indirect relationship between X and Y via a confounder. Even under very stringent assumptions, causal structure discovery problems are challenging. Much work has been done on causal discovery methods with two variables in recent years. This thesis extends the bivariate case to the possibility of having at least one confounder between X and Y. Attempts have been made to extend the causal inference process to recover the structure of Bayesian networks from data. The contributions of this thesis include (a) extending causal discovery methods to the networks with exactly one confounder (third variable) ; (b) an algorithm to recover the causal graph between every pair of variables with the presence of a confounder in a large dataset; (c) employing a search algorithm to find the best Bayesian network structure that fits the data .</p>
<p>Improved results have been achieved after the introduction of confounders in the bivariate causal graphs.</p>
<p>Further attempts have been made to improve the Bayesian network scores for the network structures of some medium to large-sized networks using the standard ordering based search algorithms such as OBS and WINASOBS. Performance of the methods proposed have been tested on the benchmark datasets for cause-effect pairs and from the BLIP library.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/30546,Computer Sciences,,Effectiveness of classification approach in recovering pairwise causal relations from data.,thesis
"Cai, Ying,Tavanapong, Wallapak,Qi, Li","Ji, Kun",N/A,"Rank-aware queries let users specify functions to score the records in a relation. An input to the functions produces a score for each record, which can be used for purposes such as ranking. While this feature makes rank-aware queries essential to a whole range of applications such as data analysis, the involvement of scoring functions makes them difficult to process. For example, to find the record at a particular rank under a function input, the current common approach is to compute the score for every record and sort them out. 

In a prior research, it is shown that given a set of records and their scoring functions, the domain of these functions can be partitioned into a number of disjointed subdomains such that the functions can be sorted according to their scores in each subdomain. A novel data structure called {\em Intersection-tree} (I-tree) was subsequently developed to compute the subdomains and sorted functions for each subdomain. The tree supports efficient search of the subdomains and the sorted function lists, where users can retrieve various types of rank-aware information, such as top k and the rank of a record of interest. The extensive evaluation confirms that I-tree is more efficient than other techniques when compared head to head.  

The I-tree, unfortunately, is computation-intensive to construct for a large number of records. A set of $n$ $d$-variable linear functions, each used to score one record, can create a total of $O(n^2)$ intersections, which together can partition their domain into $O(n^{2d})$ subdomains. These subdomains are computed and indexed while the intersections are inserted one by one to the tree. A major computation in this insertion process is feasibility checking, i.e., given a set of inequalities and an equation $f(X)=0$, determine whether there exist two distinct inputs $X^{'}$ and $X^{""}$ that satisfy all the inequalities, and $f(X^{'}) < 0$ and $f(X^{""}) > 0$. The original implementation applies the classic simplex algorithm to perform these checkings with a significant amount of unnecessary computation and thus is not efficient. In this thesis, we study the characteristics of I-tree and propose two approaches that minimize of the cost of feasibility checking in I-tree construction: 1) reduce the complexity of feasibility checking, and 2) reuse intermediate results for the next feasibility checking along the insertion path. We implement the proposed methods, evaluate their performance under various settings, and report the results in this thesis.",https://dr.lib.iastate.edu/handle/20.500.12876/dv6l3R0z,Computer science,"database,function Query,I-tree,rank consistency,rank-aware Query,simplex algorithm",Efficient feasibility checking algorithms for intersection-tree construction,thesis
"Ting Zhang,Wensheng Zhang","Jiang, Chuan",2018-08-11T11:01:42.000,"<p>The past few decades saw great improvements in the performance of satisfiability (SAT) solvers. In this thesis, we discuss the state-of-the-art techniques used in building an efficient SAT solver. Modern SAT solvers are mainly constituted by the following components: decision heuristics, Boolean constraint propagation, conflict analysis, restart, clause deletion and preprocessing. Various algorithms and implementations in each component will be discussed and analyzed. Then we propose a new backtracking strategy, partial backtracking, which can be easily implemented in SAT solvers. It is essentially an extension of the backtracking strategy used in most SAT solvers. With partial backtracking, the solver consecutively amends the variable assignments instead of discarding them completely so that it does not backtrack as many levels as the classic strategy does after analyzing a conflict. We implemented this strategy in our solver Nigma and the experiments show that the solver benefits from this adjustment.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28056,Computer Sciences,"backtracking,conflict-driven clause learning,satisfiability",Efficient satisfiability solver,thesis
"Ciardo, Gianfranco,Bonakdarpour, Borzoo,Miner, Andrew,Basu, Samik,Lutz, Jack,Sabzikar, Farzad","Zaman, Eshita",N/A,"Model checking is an automated verification technique that has proved useful in verifying various systems such as sequential circuit designs, communication protocols, software systems, cyber-physical systems, and biological systems. A model checker comprises three vital components: a finite state model that requires to be verified, a system specification expressed using temporal logic, and a verification algorithm that exhaustively explores the entire state space of the model to determine whether the property holds or not. However, due to the exhaustive search, the model checker faces a problem known as state space explosion. Using efficient algorithms, and data structures makes it possible to mitigate this issue.   

This dissertation includes three works that propose efficient algorithms for model-checking properties in stochastic and partially specified systems. In the first work, we extended the probabilistic reward computation tree logic (PRCTL) to express more general performance measures in a discrete-time Markov reward model. We proposed new operators to compute expected rewards conditioned on satisfying a PRCTL path formula, and operators to compute expected rewards at a given time $t$, also conditioned on satisfying the PRCTL formula. The second work explores the difficulty of various methods to verify HyperPCTL formulas. The logic HyperPCTL was proposed to express probabilistic hyperproperties in a system and the syntax is defined based on the self-composition of the DTMCs. Based on the observation that these DTMCs evolve independently, we proposed algorithms to numerically solve each participating DTMC separately and later combine the results. We also found the class of HyperPCTL formulas where we need to store the history of a path. We conclude that model-checking this class of HyperPCTL formulas can be as complex as when model-checking HyperPCTL with self-composition. Finally, the third work explores model-checking CTL formulas (with 3-valued semantics) in partially specified systems. Due to being in the early development stage, and having incomplete knowledge about the system or its properties, evaluation of some properties in states of the system might be \emph{unknown}. We provide a formalism to concretize the labeling of a model that explores the design space based on minimizing cost given it satisfies the formula $\varphi$.",https://dr.lib.iastate.edu/handle/20.500.12876/9z0KEMxr,"Computer science,Statistics","expected rewards,HyperPCTL model checking,partially specified systems,probabilistic model checking",Efficient stochastic model checking,dissertation
"Jannesari, Ali,Wongpiromsarn, Tichakorn,Rajan, Hridesh","KUMAR, CHANDAN",N/A,"Recent developments in the edge devices have increased the utility of edge devices in volume
estimation of uneven terrains. Existing techniques utilize several geo-tagged images of the
landscape, captured in-flight by an edge device mounted over a UAV, to generate 3D models and
perform volume estimation through manual boundary marking. These methods, although
accurate, require significant time, human effort and are heavily dependent on GPS. We present an
efficient deep learning framework that detects the object of interest and automatically determines
the volume (independent of GPS) of the detected object on-the-fly. Our method employs a stereo
camera for depth sensing of the object and overlays a unit mesh grid over the object’s boundary to
perform volume estimation. We explore the accuracy vs computational complexity trade-off on
variations of our technique. Experiments indicate that our method reduces the time for volume
estimation by several orders of magnitude in contrast to existing methods and is independent of
GPS as well. With this work, we try to complete the volume estimation work for dynamic
environments as well.",https://dr.lib.iastate.edu/handle/20.500.12876/0zEyJ8Oz,Artificial intelligence,"Depth Detection,Edge AI,UAVs",Efficient volume analysis for dynamic environments using Deep Learning,thesis
"Judy M. Vance,Leslie Miller","Pavlik, Ryan",2018-08-11T17:26:09.000,"<p>This research focuses on the exploration of software and methods to support natural interaction within a virtual environment. Natural interaction refers to the ability of the technology to support human interactions with computer generated simulations that most accurately reflect interactions with real objects. Over the years since the invention of computer-aided design tools, computers have become ubiquitous in the product design process. Increasingly, engineers and designers are using immersive virtual reality to evaluate virtual products throughout the entire design process.</p>
<p>The goal of this research is to develop tools that support verisimilitude, or likeness to reality, particularly with respect to human interaction with virtual objects. Increasing the verisimilitude of the interactions and experiences in a virtual environment has the potential to increase the external validity of such data, resulting in more reliable decisions and better products.</p>
<p>First, interface software is presented that extends the potential reach of virtual reality to include low-cost, consumer-grade motion sensing devices, thus enabling virtual reality on a broader scale. Second, a software platform, VR JuggLua, is developed to enable rapid and iterative creation of natural interactions in virtual environments, including by end-user programmers. Based on this software platform, the focus of the rest of the research is on supporting virtual assembly and decision making. The SPARTA software incorporates a powerful physically-based modeling simulation engine tuned for haptic interaction. The workspace of a haptic device is both virtually expanded, though an extension to the bubble technique, and physically expanded, through integration of a haptic device with a multi-directional mobile platform. Finally, a class of hybrid methods for haptic collision detection and response is characterized in terms of five independent tasks. One such novel hybrid method, which selectively restores degrees of freedom in haptic assembly, is developed and assessed with respect to low-clearance CAD assembly. It successfully maintains the high 1000 Hz update rate required for stable haptics unlike previous related approaches.</p>
<p>Overall, this work forms a pattern of contributions towards enabling natural interaction for virtual reality and advances the ability to use an immersive environment in decision making during product design.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28420,Computer Sciences,"Human Computer Interaction,Computer Science,Haptics,Human-Computer Interaction,Virtual Reality",Enabling natural interaction for virtual reality,dissertation
"Bao, Forrest,Cai, Ying,Li, Qi,Zhang, Wensheng,Zhang, Zhu","Luo, Ge",N/A,"Automatic summarization plays a pivotal role in condensing vast amounts of information into concise representations, thereby facilitating efficient information retrieval and comprehension. Nonetheless, evaluating the effectiveness of automatic summarization systems remains a significant challenge. This work presents advancements in automatic summarization evaluation across three main areas.

In the first part, we introduce a comprehensive framework for summarization metric design, taking into account various linguistic and structural aspects of summaries. We propose evaluating the overall quality of a summary by training a metric from synthesized summaries. By learning a preference rank of gold summaries and inferior summaries synthesized by corrupting the gold summary, our method eliminates the need for a reference summary during inference time.

In the second part, we delve into a specific aspect of summarization evaluation: the assessment of factual consistency in machine-generated summaries. Initially, we systematically analyze the shortcomings of current methods in synthesizing inconsistent summaries. Subsequently, employing the parameter-efficient finetuning (PEFT) technique, we discover that a competitive factual consistency detector can be achieved using thousands of real model-generated summaries with human annotations.

In the third part, we extend our investigation to incorporate explanations into the consistency metric, aiming to provide insights into the underlying processes driving the evaluation outcomes. We collect human-written natural language explanations on the inconsistent summary and article pairs. Leveraging the collected data, we train a text-generation model to output the consistency judgment and explanation together. We demonstrate the efficacy of the trained metric in enhancing the interpretability of the consistency assessment, empowering users to understand and interpret the evaluation results effectively.",https://dr.lib.iastate.edu/handle/20.500.12876/7wbODGyv,Computer science,"consistency,hallucination,LLM,NLG evaluation,summarization","Enhancing automatic summarization evaluation: Metric design, consistency, and explanation",dissertation
"Syrkin Wurtele, Eve,Eulenstein, Oliver,Aduri, Pavan","Vajjhala, Kumara Sri Harsha",N/A,"Exploratory analysis is an important preliminary step in bioinformatics, and in scientific research in general. It facilitates discovering interesting patterns, plotting visualizations, finding anomalies, and developing testable hypotheses. MetaOmGraph, an open-source software, provides several features for interactive exploratory data analysis of high dimensional datasets, especially biological omics data. The first part of this thesis elucidates the implementation of multiple features to improve MetaOmGraph. In particular, it elaborates on large project handling, enhanced reproducibility, new visualizations, and improved user interface in the new version of MetaOmGraph. In the new version of MetaOmGraph (v2), I have used an efficient JSON-based project structure, which is loaded into memory using a stream-based parser. This substantially improved project load time and performance on larger projects. I have also built a logging and playback framework in the software and applied it to all the major statistical analyses and visualizations. This feature allows for reproducible experiments and easy sharing of results across different platforms in the MetaOmGraph v1 version. Moreover, I developed an effective window management system by providing a taskbar, added new visualizations: Heatmap and Violin Plot, re-designed the metadata upload component, and improved other user interface components.

In the second part of my thesis, I consider the research question of classifying breast tissues in the human cancer RNA-seq dataset into tumor and non-tumor classes and identifying the biologically important annotated and novel genes. I utilized a random forest machine learning model to classify the tissues and compare the feature importance obtained from the model to the up-regulated and down-regulated genes obtained through differential expression analysis. My results suggest a batch effect due to technical variation in the data generation, which results in an almost perfect classification. I also identify the genes that might be interesting biologically due to their strong differential expression level, greater than the confounding effect, and high feature importance.",https://dr.lib.iastate.edu/handle/20.500.12876/9z0KmJ4r,"Computer science,Artificial intelligence,Bioinformatics","Computational Methods,Exploratory Analysis,Machine Learning,RNA-seq,Software Engineering","Enhancing MetaOmGraph software, and using interpretable machine learning to study novel genes in human breast cancer",thesis
"Le, Wei,Li, Yang,Stevens, Clay","Alam, Mirza Sanjida",N/A,"The increasing complexity of software systems necessitates robust vulnerability detection
mechanisms. This thesis explores Chain of Thought (CoT) reasoning, detecting line-level
vulnerability using the LineVul model, and the generation of augmented data using the NatGen
tool to evaluate the robustness of large language models (LLMs) for detecting vulnerabilities in
code. CoT prompts are designed to elicit step-by-step reasoning in LLMs, aiding in vulnerability
detection tasks that require complex code understanding. We explore the potential of LLMs in
understanding, reasoning, and predicting code vulnerabilities through the usage of the CoT
prompting technique and augmented dataset. Despite the demonstrated effectiveness of these
models, our analysis reveals significant challenges in their robustness, particularly in
distinguishing buggy from fixed code and accurately identifying vulnerability types. We introduce
the LineVul model, a state-of-the-art transformer-based approach that improves line-level
vulnerability detection using our cleaned datasets generated by the ActiveClean method.
ActiveClean employs active learning to create high-quality line-level vulnerability data,
significantly enhancing the detection capabilities of LineVul model. Our evaluations show that the
integration of these models and techniques results in improved performance in identifying critical
security vulnerabilities in software systems and advancing the field of software security.
Our replication package is located at https://figshare.com/s/24819ff5706133596bee.",https://dr.lib.iastate.edu/handle/20.500.12876/8zn7D9Ww,Computer science,AI,Evaluating AI for vulnerability detection,thesis
"Jannesari, Ali,Prabhu, Gurpur,Cohen, Myra","Bhattacharjee , Arijit",N/A,"OpenMP target offload has been in the inception phase for some time but has been gaining traction in the recent years with more compilers supporting the constructs and optimising it at the general level. Its ease of programming compared to other models makes it quite desirable in the industry. This work investigates  how different compilers are interacting with the different constructs at runtime and how the callbacks affect the performance of each compiler. We also dive into the programs in the Polybench benchmark test suite with the main focus on linear algebra to generate an OpenMP GPU target offload implementation with parallelization techniques obtained from DiscoPoP for the C Language. The main focus is on the DOALL and reduction loops which come under loop level parallelism. The problem encountered are issues with the device data mapping. We also analyze the compiler used and see its behaviour in code generation for OpenMP target offload. While converting these benchmarks, we faced a myriad of issues related to the data mapping of multi-dimensional data structures onto the target from the host while using the GCC compiler. The main work done to counter this issue was to suggest a code transformation algorithmic approach which efficiently resolves the issues without loosing the correctness of the said programs.",https://dr.lib.iastate.edu/handle/20.500.12876/6wBl2par,Computer science,"array,compiler,multi-dimensional,offload,OpenMP,target",Evaluation of GPU-specific device directives and multi-dimensional data structures in OpenMP,thesis
"Jannesari, Ali,Aduri, Pavan,Lutz, Robyn R","Dutta, Akash",N/A,"Stagnation of Moore's law has led to the increased adoption of parallel programming for enhancing performance of scientific applications. Frequently occurring code and design patterns in scientific applications are often used for transforming serial code to parallel. But, identifying these patterns is not easy. To this end, we propose using Graph Neural Networks for modeling code flow graphs to identify patterns in such parallel code. Additionally, identifying the runtime parameters for best performing parallel code is also challenging. We propose a pattern-guided deep learning based tuning approach, to help identify the best runtime parameters for OpenMP loops. Overall, we aim to identify commonly occurring patterns in parallel loops and use these patterns to guide auto-tuning efforts. We validate our hypothesis on 20 different applications from Polybench, and STREAM benchmark suites. This deep learning-based approach can identify the considered patterns with an overall accuracy of 91%. We validate the usefulness of using patterns for auto-tuning on tuning the number of threads, scheduling policies and chunk size on a single socket system, and the thread count and affinity on a multi-socket machine. Our approach achieves geometric mean speedups of 1.1x and 4.7x respectively over default OpenMP configurations, compared to brute-force speedups of 1.27x and 4.93x respectively.",https://dr.lib.iastate.edu/handle/20.500.12876/1wge035r,Computer science,"Autotuning,Code Patterns,Graph Neural Networks,OpenMP,Performance Counters",Evaluation of Graph Neural Networks for code pattern detection and autotuning of OpenMP loops,thesis
"Wongpiromsarn, Tichakorn,Liu, Jia,Rajan, Haridesh,Brabanter, Kris","Ullah, Hamad",N/A,"Semantic understanding of the perceived world is the key element in autonomous systems' decision-making. The perceived world relies on object classification of each class probability in a given scenario. The widely used approach is the maximum of all class probabilities output of the probability distribution is associated with the object or image in a classification task. A more sophisticated system may also utilize the probability distribution over the classes instead of basing its decision only on the most likely class. The current best approach relies intensively on the accuracy of the model. However, when presented with new data, the model fails to obtain 100\% accurate results. Inaccurate classification leads to unsafe behavior in an autonomous system. Let us take an example of an autonomous system. Suppose there is a pedestrian on the crosswalk. The system will only stop if it correctly classifies the object on the crosswalk. Autonomous vehicles need multiple of sensors and actuators. Running wires to connect all sensors and actuators increases the level of complexity. 

This thesis presents a new task-based neural network for object classification and compares its performance with a typical probabilistic classification model to improve threshold-based probabilistic decision-making. Furthermore, it introduces a configurable cross layer network emulator known as EMANE with wireless communication optimization for the distributed system based on HeavyBall optimization.",https://dr.lib.iastate.edu/handle/20.500.12876/Yr3Kjdar,Computer science,"Autonomous vehicle,Machine Learning,Networking,Object classification",Evaluation of object classifier and cross layer network emulator: Application to autonomous vehicle,thesis
Yong Guan,"Myneedu, Sai Giri Teja",2018-08-11T10:19:43.000,"<p>Abstract</p>
<p>Peer to Peer(P2P) file sharing networks are amongst the best free sources of information on the internet. Voluntary participation and lack of control makes them a very attractive option to share data anonymously. However a small group of people take advantage of the freedom provided by these networks and share content that is prohibited by law. Apart from copyrighted content, there are cases where people share les related to Child Pornography which is a criminal offense. Law enforcement attempts to track down these offenders by obtaining a court order for search and seizure of computers at a suspect location. These seized computers are forensically examined using storage and memory-forensics tools. However before the search warrant is issued strong evidence must be presented to provide a reason for suspiscion. Deficient investigation in the intial stages might lead to mis-identification of the source and steer the investigation in a wrong direction.</p>
<p>Initial evidence collection on peer to peer le sharing networks is a challenge due to the lack of a central point of control and highly dynamic nature of the networks. The goal of this work is to create a working prototype of an initial evidence collection tool for forensics in P2P networks. The prototype is based on the idea that P2P networks could be monitored by introducing modified peer nodes onto the network for a certain time period and recording relevant information about nodes that possess criminally offensive content. Logging information sent by a suspicious node along with timestamps and unique identication information would provide a strong, verfiiable initial evidence. This work presents one such working prototype in alignment with the goals stated above.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24530,Computer Sciences,"Digitial Forensics,Evidence Collection,P2P Networks",Evidence Collection for Forensic Investigation in Peer to Peer Systems,thesis
Robyn R. Lutz,"Krishnan, Sandeep",2018-07-21T18:40:54.000,"<p>The systematic reuse provided by software product lines provides</p>
<p>opportunities to achieve increased quality and reliability as a product line</p>
<p>matures. This has led to a widely accepted assumption that as a product line evolves, its reliability improves. However, evidence in terms of empirical investigation of the relationship among change, reuse and reliability in evolving software product lines is lacking.</p>
<p>To address the problem this work investigates: 1) whether reliability as measured by post-deployment failures improves as the products and components in a software product line change over time, and 2)</p>
<p>whether the stabilizing effect of shared artifacts enables accurate prediction of failure-prone files in the product line.</p>
<p>The first part of this work performs defect assessment and investigates defect trends in Eclipse, an open-source software product line. It analyzes the evolution of the product line over time in terms of the total number of defects, the percentage of severe defects and the relationship between defects and changes.</p>
<p>The second part of this work explores prediction of failure-prone files in the Eclipse product line to determine whether prediction improves as the</p>
<p>product line evolves over time. In addition, this part investigates the effect</p>
<p>of defect and data collection periods on the prediction performance.</p>
<p>The main contributions of this work include findings that the majority of files with severe defects are reused files rather than new files, but that</p>
<p>common components experience less change than variation components. The work also found that there is a consistent set of metrics which serve as prominent predictors across multiple products and reuse categories over time. Classification of post-release, failure-prone files using change data for the Eclipse product line gives better recall and false positive rates as compared to classification using static code metrics. The work also found that on-going change in product lines hinders the ability to predict failure-prone files, and that predicting post-release defects using pre-release change data for the Eclipse case study is difficult. For example, using more data from the past to predict future failure-prone files does not necessarily give better results than</p>
<p>using data only from the recent past. The empirical investigation of product line change and defect data leads to an improved understanding of the interplay among change, reuse and reliability as a product line evolves.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27182,Computer Sciences,,Evidence-based defect assessment and prediction for software product lines,dissertation
"Eulenstein, Oliver,Friedberg, Iddo,Fernandez-Baca, David,Lavrov, Dennis,Huang, Xiaoqiu","Nguyen , Huy",N/A,"A complex system is a group or organization in which many components interact to perform functions that a single component cannot. The study of the evolution of complex systems can reveal the pattern of interactions, the dependencies between entities, the impacts on the system’s environment, etc. In bacterial genomes, a gene block can be thought of as a complex system. Gene blocks are genes that are co-located on chromosomes, whose evolutionary conservation is essential and useful in phylogenetic and functional studies. Conservation of gene blocks across several species indicates that some of the gene blocks are operons, which are gene blocks that are transcribed together into an mRNA under the control of a single promoter. Previously, an event- based model was introduced to study the evolution of gene blocks in bacteria. It was proven to be practical and efficient in examining the formation of gene block. In this dissertation research, I present three studies on the evolution of gene blocks in bacteria using the event-based model.
In the first study, we introduced the formalization of the event-based model and used it to study the problem of finding orthologous gene blocks in bacteria. Unfortunately, the problem is NP-Hard, which means it is unlikely to have an efficient algorithm to solve the problem exactly. Additionally, it is hard to approximate the problem within a ratio of log2(n)/2 ≃ .72 ln n where n is the number of genes in the reference gene blocks. Fortunately, it is possible to approximate it within a ratio ln n using a variant of the greedy algorithm of the Set Cover problem. To evaluate the optimality of this method, we formulated the problem as an Integer Linear Program. Using a data set from my previous paper, we compared my method with the state-of-the-art method and showed that mine was more efficient and optimal.
In the second study, we explored the evolution of the homologous gene blocks under the event- based model. Using the maximum parsimony approach, we developed two heuristic algorithms to reconstruct the ancestral state of gene blocks of a group of taxa given a reference gene block. We applied these two methods to study the evolution of 55 operons in Escherichia coli and 83 operons in Bacillus subtilis. From the results, we discovered several plausible intermediate states of gene blocks which might be fully or partially functional. From the reconstruction of the ancestral states of gene block, we formed hypotheses on how the gene blocks evolved, and what external forces could have impacted on the course of the evolution.
In the third study, we extended the heuristic algorithm from the second study to trace the evolution of bacterial gibberellin biosynthetic operon. Gibberellin hormones are ubiquitous regulators of growth and developmental processes in vascular plants. Although homologous operons encode GA biosynthetic enzymes in both rhizobia and phytopathogens, there is a clear functional distinction between them. Using the reconstruction algorithm, we characterized losses, gains, and duplications of the gibberellin operon genes within alpha-proteobacteria rhizobia and revealed a complex history of horizontal gene transfer of individual genes and clusters of genes.
In conclusion, the event-based model provides a powerful tool to study the evolution of gene blocks in bacteria. Although the problems are NP-hard, heuristic approaches can provide meaningful results within a reasonable time. In the future work section, I present directions to extend our studies, as well as problems they might solve.",https://dr.lib.iastate.edu/handle/20.500.12876/YvkAV7az,Computer science,"Bacteria,Bioninformatics,Heuristics,Homology,NP Hardness,Operon",Evolution of gene blocks in bacteria using an event-based model,dissertation
Ying Cai,"Xu, Ge",2018-08-23T00:58:34.000,"<p>We present a new approach for K-anonymity protection in Location-Based Services (LBSs). Specifically, we depersonalize location information by ensuring that each location reported for LBSs is a cloaking area that contains K different footprints--- historical locations of different mobile nodes. Therefore, the exact identity and location of the service requestor remain anonymous from LBS service providers. Existing techniques, on the other hand, compute the cloaking area using current locations of K neighboring hosts of the service requestor. Because of this difference, our approach significantly reduces the cloaking area, which in turn decreases query processing and communication overhead for returning query results to the requesting host. In addition, existing techniques also require frequent location updates from all nodes, regardless of whether or not these nodes are requesting LBSs. Most importantly, our approach is the first practical solution that provides K-anonymity trajectory protection needed to ensure anonymity when the mobile host requests LBSs continuously as it moves. Our solution depersonalizes a user's trajectory (a time-series of the user's locations) based on the historical trajectories of other users.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/68452,Computer Sciences,Computer science,Exploring historical location data for anonymity preservation in location-based services,thesis
"Rajan, Hridesh,Zhang, Wensheng,Li, Yang","Singh, Astha",N/A,"Trained (deployed) deep learning models, like traditional deployed software, have four properties: they are large, they change in response to changes in design decisions, their changes can be expensive, and multiple architectural styles exist to organize them. Developers follow a sequence of steps to create deep learning models, which includes data collection, model
architecture design, and training using available data. Once these trained models meet the desired criteria, such as satisfactory testing accuracy, they are deployed into the production environment. Just like software, this deployment marks the beginning of an ongoing maintenance and evolution process. To align with evolving production conditions and changing user needs, continuous adjustments and updates to these models are necessary. This work makes three contributions to aid the understanding of evolution and maintenance for deployed models. First, through a study of 696 top-starred GitHub repositories, we present a taxonomy of changes in deployed models. Second, we validate these changes through a user study. Our findings reveal that over 60\% of users opt for retraining models when encountering changes, which is detrimental to sustainability! Finally, we analyze the capability of existing model decomposition styles, from machine learning (ML)
and software engineering (SE) literature, w.r.t. their ability to cope with these changes. For SE and ML researchers, our results will motivate research on evolution and maintenance of deployed DL models. For practitioners, we anticipate that our results can help select appropriate decomposition styles for deployed models based on anticipated changes in their project.",https://dr.lib.iastate.edu/handle/20.500.12876/4vGX63Nr,Computer science,"Artificial Intelligence,Deep Learning,Software Engineering",Exploring post-deployment deep learning model changes and the impact of modularity,thesis
N/A,"Chen, Tongjie",2020-11-06T02:21:43.000,"<p>MultiJava is an extension of the Java Programming Language with open classes and symmetric multiple dispatch. This thesis extends MultiJava by adding support for parametric polymorphism, which allows the definition and implementation of generic abstractions. Multiple dispatch has limited interaction with generics due to the semantics of generics chosen for Java. The interesting problem is how to mix MultiJava's open classes feature with the new generic feature. This thesis explains extensions to MultiJava's Semantics and describes the implementation of generics as an extension to the MJC compiler.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97746,,Computer science,Extending MultiJava with generics,thesis
"Vasant Honavar,Stephen Gilbert","Kodavali, Sateesh Kumar",2018-08-11T07:21:19.000,"<p>An Intelligent Tutoring System (ITS) is an artificially intelligent educational software application that teaches a user skills by giving personalized feedback as the user completes tasks within a problem domain. Despite their popularity, authoring these systems is a labor-intensive process, requiring many different skill sets. A major component of an ITS is the cognitive model. Historically its implementation has required not only cognitive science knowledge, but also programming knowledge as well. To address this challenge, the Extensible Problem Specific Tutor (xPST) was developed for easy authoring of ITSs for existing software and websites. This work develops an xPST authoring tool to simplify the process of xPST authoring by the end user and to help conduct research experiments. It also evaluates the xPST system in terms of the time taken by the users to author successful models. This work also extends xPST framework to enable the creation of generalized tutors in addition to problem specific tutors. To help non-technical military trainers create xPST tutors in game scenarios, this work develops a Torque xPST Driver plugin to enable xPST authoring in Torque 3D game and evaluates authoring in spatial environment scenarios like 3D games using the authoring tool. Finally, this work compares xPST and Cognitive Tutor SDK (another authoring framework) using a fraction addition study and shows that the ratio of training development time to training experience time using xPST is approximately 50% less that that of using Cognitive Tutor SDK. This thesis also shows that there is no significant difference between the “beginner programmer” and “experienced programmer” groups in terms of the time taken to author the tasks using xPST.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26055,Computer Sciences,"easy authoring,intelligent tutoring systems,ITS,xPST",Extensible Problem Specific Tutor (xPST) : Easy authoring of intelligent tutoring systems,thesis
Oliver Eulenstein,"Pappas, Nick",2018-08-11T19:00:28.000,"<p>One important computational problem is that of mining quasi bicliques from bipartite graphs. It is important because it has an almost endless number of applications and, in most real world problems, is more appropriate than the mining of bicliques. In my thesis I examine the following: the motivation for quasi bicliques, the existing literature for quasi bicliques, my implementation of a web application that allows the user to compute exact quasi biclique solutions using the biclique formulation and the exact solution algorithm provided by Chang et al.[1], and finally a polynomial heuristic algorithm for finding large quasi bicliques in the special case where we have all the biclique subgraphs of a bipartite graph available.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26935,Computer Sciences,"data mining,protein-protein interaction,quasi-biclique",Extracting large quasi-bicliques using a skeleton-based heuristic,thesis
N/A,"Maurer, Peter",2018-08-17T04:15:51.000,"<p>The programming language FCL (Functional Computing Language) is presented in some detail. FCL is a functional language which combines several features of existing applicative languages. Some of these features are generalized versions of their counterparts in other languages. A new feature which FCL presents is the modeling of data-structures as functions;A short quantitative comparison between FCL and five other languages is presented. The data for this comparison also serve as a set of examples which demonstrate the features of FCL;An algebraic algorithm for translating a restricted form of FCL to combinatoric form is presented. The pure combinatory code generated by this algorithm is suitable for generation of efficient data-flow graphs. This algorithm allows some of the benefits of demand-driven architectures to be realized in data-driven code;Finally, a theoretical foundation for the study of shared data is presented. Formal definitions for the concepts of timing, processes, and process communication are presented. These concepts are used to define a self-contained solution to the shared-data problem.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/79892,Computer Sciences,Computer science,FCL: a purely functional language for data-flow programming,dissertation
"Jennifer Davidson,Hridesh Rajan","Jalan, Jaikishan",2018-08-11T17:56:23.000,"<p>Steganalysis deals with identifying the instances of medium(s) which carry a message for communication by concealing their exisitence. This research focuses on steganalysis of JPEG images, because of its ubiquitous nature and low bandwidth requirement for storage and transmission.</p>
<p>JPEG image steganalysis is generally addressed by representing an image with lower-dimensional features such as statistical properties, and then training a classifier on the feature set to differentiate between an innocent and stego image. Our approach is two fold: first, we propose a new feature reduction technique by applying Mahalanobis distance to rank the features for steganalysis. Many successful steganalysis algorithms use a large number of features relative to the size of the training set and suffer from a ""curse of dimensionality"": large number of feature values relative to training data size. We apply this technique to state-of-the-art steganalyzer proposed by Tomas Pevny (SPIE 2007) to understand the feature space complexity and effectiveness of features for steganalysis. We show that using our approach, reduced-feature steganalyzers can be obtained that perform as well as the original steganalyzer. Based on our experimental observation, we then propose a new modeling technique for steganalysis by developing a Partially Ordered Markov Model (POMM) (IEEE ICIP '93) to JPEG images and use its properties to train a Support Vector Machine. POMM generalizes the concept of local neighborhood directionality by using a partial order underlying the pixel locations. We show that the proposed steganalyzer outperforms a state-of-the-art steganalyzer by testing our approach with many different image databases, having a total of 20,000 images. Finally, we provide a software package with a Graphical User Interface that has been developed to make this research accessible to local state forensic departments.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25218,Computer Sciences,"Feature Selection,JPEG Image,Markov,Steganalysis,Steganography","Feature selection, statistical modeling and its applications to universal JPEG steganalyzer",thesis
Soumik Soumik Sarkar,"Saha, Homagni",2021-06-11T00:49:17.000,"<p>Reliable detection of human occupancy in indoor environments is critical for various energy efficiency, security and safety applications. We consider this challenge of occupancy detection using extremely low quality, privacy-preserving images from low power image sensors. We propose a combined few shot learning and clustering algorithm to address this challenge that has very low commissioning and maintenance cost. While the few shot learning concept enables us to commission our system with a few labeled examples, the clustering step serves the purpose of online adaptation to changing imaging environment over time.  Apart from validating and comparing our algorithm on benchmark datasets, we also demonstrate performance of our algorithm on streaming images collected from real homes using our novel battery free camera hardware that also leads to a new dataset for the vision community</p>",https://dr.lib.iastate.edu/handle/20.500.12876/Qr9m4aVr,,"Clustering,Few shot Learning,Occupancy detection",Few Shot Clustering For Indoor Occupancy Detection With Extremely Low Quality Images From Battery Free Cameras,thesis
"Chang, Carl  K.,Zhang, Wensheng,Wu, Huaiqing","Liu, Mingdian",N/A,"This thesis presents a novel approach to enhancing the performance of activity recognition systems in daily living scenarios by fine-tuning pretrained foundation models. Recognizing the limitations of traditional small Convolutional Neural Network (CNN) models, which typically process only video inputs and struggle with accuracy and robustness, we explore the integration of large Transformer-based models that utilize both video and audio data. Our methodology involves pretraining on extensive datasets and subsequently fine-tuning on smaller, more specialized datasets. This process allows the model to maintain its general representation capability while improving its adaptability to specific tasks like activity classification, people counting, and user recognition.

A key component of our work is the employment of a novel dataset, the UIUC YouHome Activities-of-Daily-Living (ADL) Dataset, featuring diverse activities in varied environmental contexts to ensure a bias-free analysis. This dataset challenges conventional object detection methodologies with its unique light conditions, occlusions, and human-object interactions.

We demonstrate that our approach not only surpasses conventional methods in classification accuracy but also exhibits robustness to out-of-distribution data, a crucial aspect for real-world applications. Additionally, our method significantly reduces the computational and time cost of the training process, leveraging the pretrained foundation model's capabilities. However, we acknowledge certain limitations, such as the potential dataset bias in some activities and the sizeable nature of the pretrained models, which might restrict their deployment in smaller devices. Future work will focus on addressing these limitations and further validating the model's robustness with additional datasets, including those with audio input.

In conclusion, this thesis contributes to the field of activity recognition in daily living environments by offering a scalable and efficient solution that leverages the strengths of foundation models, setting a new standard for future research and potential applications in this domain.",https://dr.lib.iastate.edu/handle/20.500.12876/dvmqQBXv,"Computer science,Statistics","Activity Recognition,Fine-tuning,Foundation Models,Model Robustness",Fine-tuning foundation models for activity of daily living analysis,thesis
N/A,"Bachan, Mallika",2020-11-13T21:06:07.000,"<p>Finite-state dimension is a computational version of classical Hausdorff (fractal) dimension that was recently developed by Dai, Lathrop, Lutz and Mayordomo. The finite-state dimension of an infinite binary sequence S is a real number dim[Subscriot FS](S) in [0,1] that characterizes the ""information density"" of S with respect to finite-state machines. It has been shown that the finite-state dimension of a sequence is the infimum of all compression ratios achievable by using finite-state lossless compressors on that sequence. In this thesis, we calculate the finite-state dimension of two specific binary sequences that have been extensively investigated, namely, the Thue-Morse sequence and the Kolakoski sequence. Along the way, we give an expository review of finite-state dimension and these two sequences.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97821,,Computer science,Finite-state dimension of the Kolakoski sequence,thesis
N/A,"Keller, Curtis",2020-11-13T15:28:37.000,"<p>Model checking is emerging as a promising technique for verification of large software systems. The major focus of research in this context is to address the problem of reasoning about infinite-domain variables and to provide helpful, easy-to-understand information for debugging in the event of an error. In this thesis we present the model checker, FocusCheck, for verification and debugging of sequential C programs. FocusCheck (a) constructs and applies on-the-fiy data-abstraction on the push-down representation of programs, (b) identifies Focus-Statement Sequences from the counterexamples which define the context of errors, (c) is able to zoom in on specific program segments that most likely harbor the statements that caused the error, and (d) includes an intuitive graphical user interface which provides the user with various views of the counterexample increasing the readability of the results. We demonstrate the effectiveness of the techniques using a number of case studies. We also propose a heuristic that looks ahead at branch points with the aim to quickly discover a counterexample in fewer steps and less memory.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/98030,,Computer science,FocusCheck: a tool for verification and counterexample analysis of sequential C programs,thesis
"Wongpiromsarn, Tichakorn,Rozier, Kristin Y.,Jones, Phillip H.,Ciardo, Gianfranco,Basu, Samik","Hariharan, Gokul",N/A,"Formal methods are used extensively in software and hardware verification, quantum system verification, ensuring autonomous vehicle safety, and protecting security-critical software from threats. However, the validity of formal methods critically depends on the correctness of specifications. This thesis contributes to (a) developing a new specification language, Mission-time Linear Temporal Logic Multi-type (MLTLM), to ease writing and verifying specifications that involve multiple types, e.g., different time scales, (b) solving for the satisfiability and maximum satisfiability of two popular specification languages, MLTL and LTL to aid specification debugging and optimally design systems that are safe by construction.

Chapter 2 introduces MLTLM as a specification language and provides many examples of system specifications that involve different types. We prove the succinctness of MLTLM and develop translations to MLTL that are minimally succinct and equivalent to MLTL. Lastly, we integrate an MLTLM runtime verification (RV) monitor into the popular R2U2 MLTL RV monitor. Comparisons between the optimal MLTL translations and MLTLM specifications on the native MLTL and MLTLM RV monitors suggest that the latter results in substantially lower hardware resource usage and verification times.

Chapter 3 explores solving for the maximum satisfiability (MaxSAT) of MLTL specifications. Given a set of specifications, MaxSAT solves for the subset of the largest cardinality that is simultaneously satisfiable. MaxSAT is important for feature prioritization and cost analysis while designing systems that are safe by construction. We develop a new algorithm that does fast translations to Boolean logic, and use off-the-shelf Boolean MaxSAT solvers to solve the MLTL MaxSAT problem. Results show that MLTL satisfiability checking is faster using the Boolean translations of this work compared to other recent approaches.

Chapter 4 provides a solution to the MaxSAT of LTL specifications. Whereas MLTL specifications have equivalent Boolean logic formulas, such translations are unavailable for LTL. Hence, we approach the problem using core-based techniques. We modify an existing algorithm to extract the unsatisfiable core from a set of LTL formulas and then we use the unsatisfiable core with a Boolean MaxSAT algorithm to solve for LTL MaxSAT. Compared to the original algorithm, the core-extraction feature has minimal overhead ($\mathcal O(1)$). The LTL MaxSAT solver is applied to two real design instances: robot navigation and power load distribution, and on randomly generated instances.",https://dr.lib.iastate.edu/handle/20.500.12876/WwPgm8kz,Computer science,"Cyber-physical systems,Formal verification,Linear Temporal Logic,MaxSAT,SAT,Unsatisfiable core",Formal specifications for cyber-physical systems,dissertation
Clifford Bergman,"Cha, Sahnghyun",2018-08-11T07:17:10.000,"<p>A chosen ciphertext attack against the RSA encryption standard PKCS#1 v1.5 was introduced by Daniel Bleichenbacher at Crypto '98. This attack was the first example where an adaptive chosen ciphertext attack is not just a theoretical concept but a practical method to crack a semantically secure encryption scheme.</p>
<p>This paper reviews the notion of the semantic security which was believed to be secure enough in reality and the reason for which this belief was denied. The paper also presents a demonstration of the Bleichenbacher's attack by using a simplified version of PKCS#1 v1.5 format.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25913,Computer Sciences,"Bleichenbacher's attack,Chosen ciphertext security,Semantic security",From semantic security to chosen ciphertext security,thesis
N/A,"Chang, Wen-Chieh",2020-11-13T21:07:34.000,"<p>The gene duplication model has been used to explain the incongruence between a gene phylogeny and a species phylogeny. This well studied model is extended in the thesis work by considering multifurcated gene phylogenies and relaxing the embedding (mapping) condition. The resulting extended model provides a better reconciliation when input data is not able to clarify the ambiguity (soft multifurcations). With a formal framework and theoretic analyses, optimal solutions are provided under parsimony assumption. The result can be further utilized in a heuristic search to find an optimal species phylogeny from a set of given gene phylogenies.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97855,,Computer science,Gene tree reconciliation with soft multifurcations,thesis
Wallapak Tavanapong,"Kumar, Prashant",2018-08-11T17:30:24.000,"<p>Twitter has become a political reality where political parties, presidential candidates, legislatures and journalists post tweets about the latest events sharing texts, pictures, hashtags, URLs, and mentioning other users. Gaining insight from the vast amount of political data on Twitter is only possible with proper computational tools.</p>
<p>We propose to store and manage Twitter data in an optimized Neo4j graph database for serving queries about political communication among state legislators of 50 U.S. states, state reporters, and presidential candidates for the 2016 presidential election. Our rationale for selecting this relatively new database technology is threefold: (1) ease of use in explicitly modeling and visualizing communication relationships among entities of interest; (2) flexibility to evolve the database overtime to quickly adapt to changes in user requirements; and (3) user-friendly intuitive query interface. We developed a Python-based Google App Engine application using Twitter API to collect tweets from the Twitter’s handlers of the aforementioned political actors. We employed best practice guidelines in graph database design to develop five different database models in order to distinguish the impact of each query optimization technique. We evaluated each of the models on the same set of tweets posted during January 1, 2016 to November 11, 2016 using the same set of queries of interest to political communication scholars in terms of the average query response times. Our experimental results confirmed the benefits of the best practice design guidelines. In addition, they show that the optimized database model is able to provide significant improvement in query response times. Reducing the number of hops used in the graph queries and using database indexes on most commonly used attributes reduced the average query response time in our dataset by as much as 74.52% and by 85.27%, respectively, compared to the reference model. Nevertheless, the reduction in the average query response time comes with the cost of the increase in graph database relationship store size by 5.49% compared to the reference model.</p>
<p>Our contributions are as follows. (1) The optimized Neo4j graph database that will be updated weekly with new tweets; the access to this database can be made available to political communication scholars. (2) The above findings added to currently limited guidelines in graph database designs. (3) The findings about political communication prior to the Iowa caucus of the 2016 primary presidential election.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/30132,Computer Sciences,,Graph Data Modeling for Political Communication on Twitter,thesis
"Gao, Hongyang,Sukul, Adisak,Mitra, Simanta","Zhou, Tiancheng",N/A,"In the past decades, numerous biological research evidences have shown that in many cases, long non-coding RNAs (lncRNAs) can be the key factors in causing serious human diseases, such as cancer, metabolism disorder, and cardiovascular disease, with related gene regulation and variation. Thus, predicting the potential associations between lncRNAs and diseases is beneficial for pathogenesis studies, including getting a deeper understanding of the functionality of lncRNAs and improving therapy research. Existing computational methods for lncRNAs-disease association (LDA) prediction most likely work by integrating heterogeneous information from given data. However, many of them do not fully utilize the topological information from the heterogeneous graph consisting of different biological variables such as lncRNA, disease, miRNA, and protein, which could eventually affect the efficiency and accuracy of the LDA prediction. Furthermore, we found that the overall performance of existing methods can be further improved based on the study we conducted.
We proposed a novel method based on a link prediction technique via subgraphs extraction and performed training as well as testing on a relational graph convolutional neural network to predict the potential lncRNAs-disease associations. In this method, we constructed a heterogeneous graph consisting of three types of nodes: lncRNAs, miRNAs, and disease, with their known associations between each node pair as edges. Then, we extracted a list of local subgraphs around each target link from the heterogeneous graph, where the subgraphs preserve rich heuristic information related to link existence with the neighbor nodes directly connected to the target links. Afterward, the relational graph convolutional network correctly identified most potential lncRNAs-diseases associations based on the extracted local subgraphs. The experimental results have shown that we had superior performance compared to several state-of-the-art methods with almost identical data. One of our matrices achieved unprecedented, much higher results than others and demonstrated a decent improvement on our lncRNA-diseases association prediction method.",https://dr.lib.iastate.edu/handle/20.500.12876/JwjbMB6w,Computer science,"Association Prediction,Deep Learning,LncRNA,Machine Learning",Heterogeneous sub-graph learning for lncRNA-disease associations prediction,thesis
"Tian, Jin,Sarkar, Soumik,Singh, Arti","Fotouhi, Fateme",N/A,"Object detection faces a formidable challenge in detecting small objects due to their diminutive size and sparse pixel representation. This difficulty is exacerbated by the common lack of resolution in images, making the task of distinguishing small objects complex. Annotating data for training datasets also demands substantial manual effort and time, particularly when dealing with large numbers of small insects found in the field. Moreover, occlusion further complicates accurate detection, as small objects are prone to being obscured by other elements in the scene. To address these challenges, sophisticated techniques are needed, specifically tailored for small object detection within the broader field of object detection.
In the context of insect detection, the task carries significant importance as certain insects pose a threat to agriculture and food security. Detecting these small pests in a timely manner is crucial to safeguarding food production and maintaining a stable agricultural economy. However, the insect detection task comes with various challenges, including the time-consuming annotation process for large datasets and difficulties in identifying small insects in low-resolution images or from a distance. Variations in insect life stages and similarities in shape and colors among different insect classes further add to the complexity.
To overcome these challenges, this thesis explores and proposes two effective solutions: hierarchical transfer learning and slicing-aided hyper inference. Hierarchical transfer learning extends the concept of transfer learning by incorporating multiple steps of knowledge transfer from a large dataset to a target dataset. It leverages intermediate in-domain datasets to adapt the model progressively. On the other hand, slicing-aided hyper inference divides the original image into overlapping patches, enabling independent object detection on each patch and subsequent merging for accurate and comprehensive object detection outputs.
The effectiveness of these proposed solutions was thoroughly evaluated through a comprehensive analysis, considering metrics such as mean Average Precision, Recall, and Precision. The evaluation focused on the accuracy of insect detection, especially in challenging scenarios. The thesis aims to contribute to more effective pest control measures, thereby safeguarding crops and supporting sustainable agricultural practices.",https://dr.lib.iastate.edu/handle/20.500.12876/nrQBl5Az,"Computer science,Agriculture","deep learning,insect monitoring,machine learning,transfer learning",Hierarchical transfer learning for small object detection,thesis
Gurpur M. Prabhu,"Negoita, Gianina Alina",2019-03-26T18:11:58.000,"<p>Various aspects of high performance computing (HPC) are addressed in this thesis. The main focus is on analyzing and suggesting novel ideas to improve an application's performance and scalability on HPC systems and to make the most out of the available computational resources.</p>
<p>The choice of inter-process communication is one of the main factors that can influence an application's performance. This study investigates other computational paradigms, such as one-sided communication, that was known to improve the efficiency of current implementation methods. We compare the performance and scalability of the SHMEM and corresponding MPI-3 routines for five different benchmark tests using a Cray XC30. The performance of the MPI-3 get and put operations was evaluated using fence synchronization and also using lock-unlock synchronization. The five tests used communication patterns ranging from light to heavy data traffic: accessing distant messages, circular right shift, gather, broadcast and all-to-all. Each implementation was run using message sizes of 8 bytes, 10 Kbytes and 1 Mbyte and up to 768 processes. For nearly all tests, the SHMEM get and put implementations outperformed the MPI-3 get and put implementations. We noticed significant performance increase using MPI-3 instead of MPI-2 when compared with performance results from previous studies. One can use this performance and scalability analysis to choose the implementation method best suited for a particular application to run on a specific HPC machine.</p>
<p>Today's HPC machines are complex and constantly evolving, making it important to be able to easily evaluate the performance and scalability of HPC applications on both existing and new HPC computers. The evaluation of the performance of applications can be time consuming and tedious. HPC-Bench is a general purpose tool used to optimize benchmarking workflow for HPC to aid in the efficient evaluation of performance using multiple applications on an HPC machine with only a ""click of a button"". HPC-Bench allows multiple applications written in different languages, with multiple parallel versions, using multiple numbers of processes/threads to be evaluated. Performance results are put into a database, which is then queried for the desired performance data, and then the R statistical software package is used to generate the desired graphs and tables. The use of HPC-Bench is illustrated with complex applications that were run on the National Energy Research Scientific Computing Center's (NERSC) Edison Cray XC30 HPC computer.</p>
<p>With the advancement of HPC machines, one needs efficient algorithms and new tools to make the most out of available computational resources. This work also discusses a novel application of deep learning to a nuclear physics application. In recent years, several successful applications of the artificial neural networks (ANNs) have emerged in nuclear physics and high-energy physics, as well as in biology, chemistry, meteorology, and other fields of science. A major goal of nuclear theory is to predict nuclear structure and nuclear reactions from the underlying theory of the strong interactions, Quantum Chromodynamics (QCD). The nuclear quantum many-body problem is a computationally hard problem to solve. With access to powerful HPC systems, several ab initio approaches, such as the No-Core Shell Model (NCSM), have been developed for approximately solving finite nuclei with realistic strong interactions. However, to accurately solve for the properties of atomic nuclei, one faces immense theoretical and computational challenges. To obtain the nuclear physics observables as close as possible to the exact results, one seeks NCSM solutions in the largest feasible basis spaces. These results obtained in a finite basis, are then used to extrapolate to the infinite basis space limit and thus, obtain results corresponding to the complete basis within evaluated uncertainties. Each observable requires a separate extrapolation and most observables have no proven extrapolation method. We propose a feed-forward ANN method as an extrapolation tool to obtain the ground state energy and the ground state point-proton root-mean-square (rms) radius along with their extrapolation uncertainties. The designed ANNs are sufficient to produce results for these two very different observables in ^6Li from the ab initio NCSM results in small basis spaces that satisfy the following theoretical physics condition: independence of basis space parameters in the limit of extremely large matrices. Comparisons of the ANN results with other extrapolation methods are also provided.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31041,Computer Sciences,"artificial neural networks,computational nuclear physics,high performance computing,MPI,one-sided routines,workflow optimization","High performance computing applications: Inter-process communication, workflow optimization, and deep learning for computational nuclear physics",dissertation
"Tian, Jin,Sarkar, Soumik,Krishnamurth, Adarsh","Feng, Jiale",N/A,"The thesis examines traditional computer vision and deep learning approaches and presents four architectures for tracking circuit boards and electrical components. The general research addresses assembly tasks on a factory floor, particularly instructing workers. Nowadays, instructions are either printed or available on a monitor. A projector-based system that projects instructions onto the target objects or the workbench is investigated. For that purpose, the target object's category, position, and pose need to be known with high precision. 
With that goal in mind, approaches for detection and pose estimation for circuit boards and electrical components are implemented and compared, including feature descriptor-based matching, artificial neural networks (ANNs), and convolutional neural networks (CNNs). The results indicate that ANNs yield the highest precision given the limited training costs. CNNs increase the detection accuracy; however, they require a more extensive training dataset and more time for training. 
Each of the architectures proposed in this thesis is designed based on its predecessor, focusing on solving new challenges in the tracking process. Various input data are used and tested, including grayscale images, RGB images, depth maps, and point clouds.",https://dr.lib.iastate.edu/handle/20.500.12876/kv7kjxYv,Computer science,"ANN/CNN,Computer Vision,Deep Learning,Feature Detection,Object Detection,Pose Estimation",High-fidelity circuit boards and electrical components tracking with traditional and deep learning methods,thesis
"Quinn, Christopher J,Tavanapong, Wallapak,Stoytchev, Alexander","Pratt, Jacob Robert",N/A,"The performance of complex recurrent networks heavily relies on the selection of an appropriate activation function. Some established activation functions, known to enhance complex-valued recurrent networks, are chosen based on specific network properties. Exploring alternative approaches to complex activation holds promise for improving the performance of complex recurrent networks. This study introduces a novel activation function that leverages the structural properties of the Hopf bifurcation. The proposed activation function is evaluated using the Mackey Glass and Copy Memory datasets, commonly employed for analyzing recurrent networks. Two experiments are conducted: one to determine an appropriate configuration for the activation function and another to compare it against five established activations. The results indicate that the Hopf bifurcation shows promise as a complex activation, delivering comparable or better performance compared to more established functions.",https://dr.lib.iastate.edu/handle/20.500.12876/qzoDAdnw,"Artificial intelligence,Applied mathematics","Activation Function,Complex Valued,Dynamical Systems,Hopf Bifurcation,Machine Learning,Recurrent Neural Networks",Hopf bifurcation as an activation function for complex recurrent neural networks: A feasibility study,thesis
Giora Slutzki,"Jha, Pranava",2018-08-15T05:27:20.000,"<p>We consider hypercubes, median graphs, Cartesian product (sk10-product) of graphs, Kronecker product (x-product) of graphs and Strong product (sk10-product) of graphs from certain algorithmic and combinatorial points of view. Our basic results are the following: (1) We construct certain schemes for partitioning the set of vertices of the n-dimensional hypercube Qn into equal-size maximal independent sets, which lead to an upper bound on the minimum cardinality of a maximal independent set of Qn. Our lower and upper bounds are within a factor of two, and for n = 2[superscript]k - 1, k ≥ 1, the two bounds coincide and hence yield the optimal value. These bounds are correct also for the domination number of Qn. (2) We present two algorithms, each of which recognizes median graphs in time O(n[superscript]2log n). These are based on two different characterizations of median graphs given respectively by Bandelt and Mulder. This improves the previously best-known bound of O(n[superscript]4) of an algorithm of Chung, Graham and Saks. (3) We present two O(n[superscript]2log n) algorithms, each of which obtains an isometric embedding of a given median graph in a hypercube of least possible dimension. These have structures similar respectively to those of the recognition algorithms mentioned above. (4) We state and prove characterizations for the following: planarity of the sk10-product of graphs, and outerplanarity of the sk10-product of graphs and outerplanarity of the x-product of graphs. (5) For the three graph products, we discuss (i) bounds on the following topological invariants: chromatic numbers, independence numbers, domination numbers and clique numbers in terms of the invariants of the factor graphs, and (ii) conditions for the existence of Hamiltonian paths/cycles. This includes an improved lower bound on the independence number of the sk10-product and sufficient conditions for the existence of a Hamiltonian cycle in the x-product.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/82545,Computer Sciences,Computer science,"Hypercubes, median graphs and products of graphs: some algorithmic and combinatorial results",dissertation
N/A,"Baldwin, William",2018-08-15T11:31:58.000,"<p>A hierarchy of data structures, called the hypertree hierarchy, is presented which has strings and trees as its smallest two elements. A generalized frontiering operation is presented. Grammars, automata and regular expressions are extended to this hierarchy. These are shown to be equivalent and the resulting languages are called regular. This leads to a hierarchy of regular languages on the hypertree hierarchy. These are projected onto the set of strings by the frontier operation resulting in a true hierarchy of string languages. This hierarchy is called the (IO) algebraic language hierarchy. It has regular languages, context free languages and macro languages as its first three levels. It is contained in the set of context sensitive languages but is not equal to it. Other characterizations are presented.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/81438,Computer Sciences,Computer science,Hypertrees: a study in language specification,dissertation
"Dickerson, Julie,Jannesari, Ali,Walley, Justin,Zhang, Wensheng","Fanijo, Samuel Olawale",N/A,"Cell counting in Immunocytochemistry is crucial for biomedical research, aiding the diagnosis and treatment of several diseases such as neurological disorders, autoimmune disorders, and cancer. However, traditional counting methods are manual, time-consuming, and prone to human error. While deep learning methods have improved this problem, they rely on labeled datasets, which are also expensive to obtain, posing challenges for scalability and efficiency.

In this work, we present the Immunocytochemistry Dataset Cell Counting using the Segment Anything Model (IDCC-SAM), a pipeline that leverages a zero-shot approach to segment and count cells in immunocytochemistry cellular images using SAM under unprompted settings with no required manual labels. IDCC-SAM utilizes the Meta AI’s Segment Anything Model (SAM), pre-trained on 11 million images and 1.1billion masks, to segment and count cells in Immunocytochemistry cellular images. By employing a zero-shot approach, IDCC-SAM eliminates the need for manual annotations, enhancing scalability and reducing menial human labeling effort. 

Our experimental results demonstrate that IDCC-SAM achieves a new benchmark performance on the IDCIA immunocytochemistry dataset, outperforming the traditional supervised state-of-the-art Mask RCNN model by 9% and UNet by 1%, despite the latter being fine-tuned on labeled examples. Our work shows evidence that SAM has the potential to improve performance on cell counting tasks while reducing the reliance on specialized models and manual annotation efforts, via a zero-shot approach.",https://dr.lib.iastate.edu/handle/20.500.12876/JvNVm5Xv,Artificial intelligence,"Applied Computing,Artificial Intelligence,Cellular Biology,Deep Learning,Digital Health,Fluorescence Microscopy",IDCC-SAM: Automated cell counting in immunocytochemistry using the segment anything model,thesis
"Vasant Honavar,Drena Dobbs","Yan, Changhui",2018-08-25T00:37:13.000,"<p>Identification of interface residues involved in protein-protein and protein-DNA interactions is critical for understanding the functions of biological systems. Because identifying interface residues using experimental methods cannot catch up with the pace at which protein sequences are determined, computational methods that can identify interface residues are urgently needed. In this study, we apply machine-learning methods to identify interface residues with the focus on the methods using amino acid sequence information alone. We have developed classifiers for identification of the residues involved in protein-protein and protein-DNA interactions using a window of primary sequence as input. The classifiers were evaluated using both representative datasets and specific cases of interest based on multiple measurements. The results have shown the feasibility of identifying interface residues from sequence. We have also explored information besides primary sequence to improve the performance of sequence-based classifiers. The results show that the performance of sequence-based classifiers can be improved by using solvent accessibility and sequence entropy of the target residue as additional inputs. We have developed a database of protein-protein interfaces that consists of all the protein-protein interfaces derived from the Protein Data Bank. This database, for the first time, makes possible the quick and flexible retrieval of interface sets and various interface features. We have systematically analyzed the characteristics of interfaces using the largest dataset available. In particular, we compared interfaces with the samples that had the same solvent accessibility as the interfaces. This strategy excludes the effect of solvent accessibility on the distributions of residues, secondary structure, and sequence entropy.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/71686,"Artificial Intelligence and Robotics,Biology,Computer Sciences,Molecular Biology","Computer science,Bioinformatics and computational biology",Identification of interface residues involved in protein-protein and protein-DNA interactions from sequence using machine learning approaches,dissertation
Wei  . Le,"Leslie, Stroh",2020-02-12T22:57:28.000,"<p>Choosing an algorithm to use can depend on a variety of factors such as runtime, space, and</p>
<p>problem requirements. Many algorithms already have tested implementations in open source code.</p>
<p>Reusing or interchanging algorithms can help save development time and improve the performance</p>
<p>of applications.</p>
<p>Existing code search techniques often rely heavily on natural language components of the code.</p>
<p>Simple techniques, such as Grep, are sensitive to the naming choices and conventions in code. Grep</p>
<p>in particular do not precisely find implementations, outputting single lines. Grep does not rank</p>
<p>the result, and is subject to lots of noise.</p>
<p>We develop a technique to search for algorithms in code using existing pseudo code as a query.</p>
<p>We leverage the structural, mathematical and natural language components of pseudo code to find</p>
<p>its corresponding implementation in code. This approach defines a simple language to represent</p>
<p>pseudo code with atoms that include different features of the algorithm. We then use these features</p>
<p>to search code using a bounding box and extract the code snippet that contains the functionality</p>
<p>of the pseudo code.</p>
<p>We collected 19 different repositories in both C and Java and searched for 27 different algorithms.</p>
<p>Using our technique we found over 60 algorithm implementations in roughly 1.8 million lines of</p>
<p>code. We also conduct a comparison of our tool against a search implementation using a popular</p>
<p>enterprise search platform Apache Solr and show our approach can find more algorithms with high</p>
<p>rank.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31911,Computer Sciences,"Algorithms,Code Search",Identify algorithms from code,thesis
Vasant Honavar,"Sanghvi, Bhavesh",2018-08-11T13:51:26.000,"<p>Recent years have seen a rapid proliferation of information sources e.g., on the World-wide Web, in virtually every area of human endeavor. Such autonomous information sources are based on different ontologies, i.e., conceptualizations of the entities, properties, and relationships in the respective domains of discourse. However, practical applications (e.g., building predictive models from disparate data sources, assembling composite web services using components from multiple repositories) call for mechanisms that bridge the semantic gaps between disparate ontologies using mappings that express terms (concepts, properties, and relationships) in a target ontology in terms of those in one or more source ontologies. Such mappings may be established by domain experts or automatically using tools designed to discover such mappings from data. In either case, it is necessary to check whether the resulting mappings are consistent, and if necessary, make them consistent by eliminating a subset of the mappings. We consider the problem of identifying the largest (maximum) subset of mappings in the restricted, yet practically important setting of hierarchical ontologies. Specifically, we consider mappings that assert that a concept in one ontology is a subconcept, superconcept, or equivalent concept of a concept in another ontology. We model the problem of identifying the largest consistent subset of such mappings between hierarchical ontologies as the problem of identifying the minimum feedback arc set in a directed acyclic graph (DAG). Because identifying minimum feedback arc set is known to be NP-hard, it follows that identifying the maximum subset of consistent mappings between hierarchical ontologies is NP-hard. We then explore several polynomial time algorithms for finding suboptimal solutions including a heuristic algorithm for (weighted) minimum feedback arc set problem in DAGs. We compare the performance of the various algorithms on several synthetic as well as real-world ontologies and mappings.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25587,Computer Sciences,"Feedback Arc Set,Hierarchical Ontology,Mapping,NP-Hard,Ontology Merging,Semantic Web",Identifying and eliminating inconsistencies in mappings across hierarchical ontologies,thesis
Shashi K. Gadia,"Krithivasan, Srikanth",2018-08-23T18:35:06.000,"<p>XML is a markup language used for storing documents which contains structured information. Its flexibility helps in storing, processing and querying diverse and complex documents with any structure. While theoretically, XML could be used to handle any documents, the currently available parsers require large amounts of main-memory resulting into severe restriction on the size of XML documents. As a result, some technologies have been developed to break the XML documents in to smaller chunks and allow the parsers to load only a specific portion of the document when needed.;Two major but diagonally opposite approaches for storing an xml document on the disk have emerged. The first breaks an xml document into parent child pairs and stores them into relational storage. The second approach builds a native storage for xml that attempts to directly capture xml hierarchy. Canonical Storage for XML (CanStoreX) is a native storage technology being developed by our group at Iowa State University that has been tested for pagination of xml documents up to 100 Gigabytes in size. CanStoreX requires that every page is a self-contained xml document on its own right. Thus the pages themselves form an xml-like hierarchy.;XML can be used to encode a variety of data. Examples are system configuration, metadata, documents such as books, relational data, and object-oriented data. An array of technologies has developed to process xml documents. Our major interest in xml lies in the view that an xml document can be considered a database which can then be queried. There exists several query engines for xml. Kweelt is an excellent early platform that supports the Quilt query language. Quilt is a preliminary query language which has subsequently been extended to XQuery, a query language that has been standardized by the W3 Consortium. Quilt, the query language that Kweelt supports, is superseded by XQuery. The original Kweelt uses DOM parser; therefore it can only handle small documents. The main focus of this thesis is to deploy CanStoreX to query documents of the size of gigabytes. The resulting platform has been extensively tested.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/68185,Computer Sciences,Computer science,Implementation of a XQuery engine for large documents in CanstoreX,thesis
Wensheng Zhang,"Susan Varghese, Jinu",2018-08-11T12:30:27.000,"<p>Over the years WiFi has gained immense popularity in networking devices to transfer data over short distances. WiFi communication can consume a lot of energy on battery powered devices like mobile phones. The Standard Power Saving Management(SPSM) which is part of the standard specification for wireless LAN technology has been applied widely. However, it may not deliver satisfactory energy effiiciency in many cases as the wakeup strategy adopted by it cannot adapt dynamically to traffic pattern changes. Motivated by the fact that it has been more and more popular for a mobile device to have both WiFi and other low-power wireless interfaces such as Bluetooth and ZigBee, we propose an implementation of a ZigBee-assisted Power Saving Management (ZPSM) scheme, leveraging the ZigBee interface to wake up WiFi interface based on the delay bound to improve energy efficiency. The results obtained by applying this scheme on a Linux environment shows that ZPSM can save energy significantly without violating delay requirements in various scenarios.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28005,Computer Sciences,"Linux,Power Management,Wireless Networks,ZigBee",Implementing ZigBee assisted power management for delay bounded communication on mobile devices,thesis
N/A,"Cheng, Shuxing",2020-11-13T21:09:22.000,"<p>Markov chains are a well-established and important tool to assess the performance and dependability of computer and communication systems. An important step in many types of Markov chain analysis is the classification of states into recurrent classes and the class of transient states. The state-space explosion problem requires us to use the symbolic methods to handle the Markov chains built to model the practical engineering systems. Current symbolic method for Markov chain state classification makes use of the pure random mechanism to select the ""seed state"", a core variable for controlling the required number of iterations. In this work, we present several distance based heuristics to improve the existing methods. These approaches are designed to re-gain some control over the selection of the seed state, which tries to reduce the required number of iterations. Experimental results indicate that our approaches can be quite effective in minimizing the required number of iterations. Extensive qualitative analysis is also conducted over several models to compare the performance of different heuristics.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97861,,Computer science,Implicit methods for Markov chain state classification,thesis
"Miner, Andrew S,Basu, Samik,Ciardo, Gianfranco,Lutz, Robyn R,Tavanapong, Wallapak","Biswal, Shruti",N/A,"Formal verification collectively defines mathematics and logic-based techniques that aim to prove the correctness of a system design or implementation in the context of specified properties. 
Among several mechanisms to conduct formal verification, 
model checking is an automated process that relies on the 
colossal task of generating the entire state-space of systems,
which in the real world can be highly complex and have an enormous number of states, 
before those states are verified.

This dissertation focusses on symbolic techniques of state-space generation and 
investigates several mechanisms to formalize system designs 
that can expedite the state-of-the-art algorithm, saturation, for reachability generation.
The proposed abstraction techniques of system design are novel as well as 
synthesized by combining with the best of the existing methods to 
complement certain phases of the saturation algorithm in a scalable manner.
The work proposes multiple enhanced versions of the saturation algorithm
that aim to achieve the scope generality of existing state-of-the-art techniques
with an improved performance cost, at times, by orders of magnitude, as presented in the results.",https://dr.lib.iastate.edu/handle/20.500.12876/kv7kjKGv,Computer science,"Formal Methods,Hybrid relations,Implicit relations,Saturation,Statespace Generation,Symbolic Model Checking",Implicit relations and their derivatives for symbolic state space generation,dissertation
"Jannesari, Ali,Zhang, Wensheng,Gao, Hongyang","Nguyen, Duy Phuong",N/A,"This thesis presents a novel approach to federated learning (FL) that addresses the challenges of model heterogeneity, computational diversity, and high communication costs. Unlike traditional FL methods that aggregate model weights across edge devices, our method focuses on aggregating local knowledge extracted from these models. By employing knowledge distillation, we distill this local knowledge into a robust global knowledge, forming the server model. This process is further refined through deep mutual learning, resulting in a compact knowledge network. Our resource-aware FL approach enables edge clients to deploy efficient models and perform multi-model knowledge fusion, optimizing both communication efficiency and model diversity. Empirical evaluations demonstrate that our method significantly outperforms existing FL algorithms in terms of reducing communication costs and enhancing generalization performance across heterogeneous data and model landscapes.",https://dr.lib.iastate.edu/handle/20.500.12876/RwyqJedw,Computer science,"Federated learning,Knowledge distillation,Machine learning",Improving diverse federated learning through knowledge acquisition and fusion of multiple models,thesis
N/A,"Tolman, Hubert",2018-08-25T01:03:07.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/76156,Genetics,"Poultry--Breeding,Computer Science,Poultry Breeding",Inbreeding in small populations of fowl undergoing selection,dissertation
N/A,"Henry, Sallie",2018-08-15T06:02:02.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/80066,Computer Sciences,Computers,Information flow metrics for the evaluation of operating systems' structure,dissertation
"Eulenstein, Oliver,Wang, Lizhi,Li, Qi","Ansarifar, Javad",N/A,"The classical Duplication-Loss-Coalescence parsimony model (DLC-model) is a powerful tool when studying the complex evolutionary scenarios of simultaneous duplication-loss and deep coalescence events in evolutionary histories of gene families. However, inferring such scenarios is an intrinsically difficult problem and, therefore, prohibitive for larger gene families typically occurring in practice. The parsimony duplication-loss-coalescence model (DLCParsimony model) appeared as an effective way to study commonly appearing gene families, whose evolutionary histories had duplication-loss events appearing alongside the incomplete lineage sorting events. This model allows practitioners to accurately infer ortholog/paralog relationships among extant genes and to study the relationship between the gene family evolution (gene tree) and the host species evolution (species tree). Unfortunately, the existing enumeration algorithm, DLCPar, which finds the optimum reconciliation between a gene tree and a species tree under the DLCParsimony model, is very limited in practice due to its advanced complexity. To overcome this stringent limitation, we first describe a non-trivial and flexible Integer Linear Programming (ILP) formulation for inferring DLC evolutionary scenarios. A comparative scalability study demonstrates that our ILP formulation commonly outperforms the standard enumeration algorithm DLCPar for inferring scenarios under the DLC-model. To make the DLC-model more practical, we then introduce two sensibly constrained versions of the model and describe two respectively modified versions of our ILP formulation reflecting these constraints. Using a simulation study, we showcase that our constrained ILP formulation computes evolutionary scenarios that are substantially larger than the scenarios computable under our original ILP formulation and DLCPar. Further, scenarios computed under our constrained DLC-model are overall remarkably accurate when compared to corresponding scenarios under the original DLC-model.",https://dr.lib.iastate.edu/handle/20.500.12876/kv7kjORv,"Computer science,Bioinformatics,Operations research","Bioinformatics,Duplication-Loss-Coalescence parsimony model,Operation research,Optimization",Integer linear programming formulation for the unified duplication-loss-coalescence model,thesis
"Leslie Miller,Masha Sosonkina","Sharda, Anurag",2018-08-11T21:40:58.000,"<p>Optimization techniques are finding their inroads into the field of nuclear physics calculations where the objective functions are very complex and computationally intensive. A vast space of parameters needs searching to obtain a good match between theoretical (computed) and experimental observables, such as energy levels and spectra. Manual calculation defies the scope of such complex calculation and are prone to error at the same time. This body of work attempts to formulate a design and implement it which would integrate the ab initio nuclear physics code MFDn and the VTDIRECT95 code. VTDIRECT95 is a Fortran95 suite of parallel code implementing the derivative-free optimization algorithm DIRECT. Proposed design is implemented for a serial and parallel version of the optimization technique. Experiment with the initial implementation of the design showing good matches for several single-nucleus cases are conducted. Determination and assignment of appropriate number of processors for parallel integration code is implemented to increase the efficiency and resource utilization in the case of multiple nuclei parameter search.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25352,Computer Sciences,"Derivative Free Optimization Technique,MFDn,No Core Shell Model,VTDIRECT95",Integration of ab-initio nuclear calculation with derivative free optimization technique,thesis
Wensheng Zhang,"Bai, Rui",2018-08-11T07:12:49.000,"<p>The wireless sensor network (WSN), once deployed, is usually expected to operate for months even years. In the WSN, each individual sensor node is typically equipped with small batteries which offer only a limited amount of energy. Hence, numerous energy-efficiency schemes have been developed for the WSN, aiming to help the WSN to achieve long lifetime under the stringent constraint of energy supply. These energy-efficiency schemes can reduce energy consumption, but they cannot address the issue of unbalanced energy consumption and energy replenishment in the WSN. In practice, different sensor nodes could have different initial energy supplies and/or play different roles in the network; consequently, some nodes may deplete their energy faster than the others. In such cases, the lifetime of the network becomes determined by the energy-bottleneck nodes who deplete their energy the fastest. To address this issue, the ideas of energy-balancing and lifetime-balancing have been proposed and energy-balancing and lifetime-balancing schemes have been developed for the MAC and routing layers of the WSN. However, there have been very few efforts on studying how to apply the idea to multiple layers of the WSN simultaneously and the effectiveness of such an integrated solution. To fill this gap, this thesis presents an integrated design, which includes lifetime-balancing schemes for the application, routing and MAC layers. Specifically, in the application layer where the sensing duty should be assigned to sensor nodes, we present a scheme in which leader nodes coordinate the sensing duty schedule among the sensor nodes based on their estimated nodal lifetime. In the routing layer, the routing metric is calculated based on both nodal lifetime and required end-to-end delay. In the MAC layer, we adopts the LB-MAC protocol, which balances nodal lifetime by adjusting the MAC parameters. Our design has been simulated in Network Simulator 2 (NS2) and compared with other solutions which do not apply lifetime-balancing idea or only applying the idea in some but not all of the three layers. The evaluation results have shown that our integrated design prolongs the network lifetime more effectively. Also, the integrated design achieves better performance in terms of data delivery ratio, end-to-end delay and wasted energy.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29287,Computer Sciences,Computer Science,Integration of lifetime-balancing schemes in wireless sensor networks,thesis
"Dickerson, Julie,Jannesari, Ali,Walley, Justin,Zhang, Wensheng","Fanijo, Samuel Olawale",N/A,"Aging in humans is characterized by the progressive deterioration of physical integrity and cellular functions, contributing to a myriad of age-related diseases such as cancer and several neurodegenerative diseases. Despite significant advancements in technology, the underlying mechanisms driving aging remain an open question. In this work, we characterize the epigenetic changes related to aging by employing transcriptomic and chromatin accessibility analyses.

Through the collection and analysis of samples from both young and aged humans, our research sheds light on the genetic changes and metabolic pathways implicated in the aging process. Leveraging next-generation sequencing technologies, we identify a set of hub genes that serve as potential targets for interventions aimed at modulating aging in humans.
By elucidating the biomarkers of aging, our research not only deepens our understanding of the aging process but also lays the foundation for the development of novel therapeutic strategies to enhance disease management in health systems.",https://dr.lib.iastate.edu/handle/20.500.12876/NveoLM3z,"Computer science,Bioinformatics","Aging,ATAC-seq,Bioinformatics,Computational Biology,Data Integration,RNA-Seq",Integration of RNA-seq and ATAC-seq reveals novel age-related hub genes in humans,thesis
Alexander Stoytchev,"Schenck, Connor",2018-07-21T23:30:12.000,"<p>Intelligence test scores have long been shown to correlate with a wide variety of other abilities. The goal of this thesis is to enable a robot to solve some of the common tasks from intelligence tests with the intent of improving its performance on other real-world tasks. In other words, the goal of this thesis is to make robots more intelligent. We used an upper-torso humanoid robot to solve three common perceptual reasoning tasks: the object pairing task, the order completion task, and the matrix completion task. Each task consisted of a set of objects arranged in a specific configuration. The robot's job was to select the correct solution from a set of candidate solutions.</p>
<p>To find the solution, the robot first performed a set of stereotyped exploratory behaviors on each object, while recording from its auditory, proprioceptive, and visual sensory modalities. It used this information to compute a set of similarity scores between every pair of objects. Given these similarity scores, the robot was able to deduce patterns in the arrangement of the objects, which enabled it to solve the given task. The robot repeated this process for all the tasks that we presented to it. We found that the robot was able to solve all the different types of tasks with a high degree of accuracy.</p>
<p>There have been previous computational solutions to tasks from intelligence tests, but no solutions thus far have used a robot. This thesis is the first work to attempt to solve tasks from intelligence tests using an embodied approach. We identified a framework for solving perceptual reasoning tasks, and we showed that it can be successfully used to solve a variety of such tasks. Due to the strong correlation between intelligence test scores and performance in real-world environments, this suggests that an embodied approach to learning can be very useful for solving a wide variety of tasks from real-world environments in addition to tasks from intelligence tests.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27509,"Artificial Intelligence and Robotics,Computer Sciences,Robotics","Artificial Intelligence,Developmental Robotics,Machine Learning",Intelligence tests for robots: Solving perceptual reasoning tasks with a humanoid robot,thesis
Johnny S. K. Wong,"Helmer, Guy",2018-08-23T13:26:43.000,"<p>Intelligent mobile agent systems offer a new approach to implementing intrusion detection systems (IDS). The prototype intrusion detection system, MAIDS, demonstrates the benefits of an agent-based IDS, including distributing the computational effort, reducing the amount of information sent over the network, platform independence, asynchronous operation, and modularity offering ease of updates. Anomaly detection agents use machine learning techniques to detect intrusions; one such agent processes streams of system calls from privileged processes. Misuse detection agents match known problems and correlate events to detect intrusions. Agents report intrusions to other agents and to the system administrator through the graphical user interface (GUI);A sound basis has been created for the intrusion detection system. Intrusions have been modeled using the Software Fault Tree Analysis (SFTA) technique; when augmented with constraint nodes describing trust, contextual, and temporal relationships, the SFTA forms a basis for stating the requirements of the intrusion detection system. Colored Petri Nets (CPN) have been created to model the design of the Intrusion Detection System. Algorithmic transformations are used to create CPN templates from augmented SFT and to create implementation templates from CPNs. The implementation maintains the CPN semantics in the distributed agent-based intrusion detection system.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/65684,Computer Sciences,Computer science,Intelligent multi-agent system for intrusion detection and countermeasures,dissertation
Vasant G. Honavar,"Pathak, Jyotishman",2018-08-22T23:06:26.000,"<p>Recent advances in networks, information and computation grids, and WWW have resulted in the proliferation of physically distributed and autonomously developed software components and services. These developments allow us to rapidly build new value-added applications from existing ones in various domains such as e-Science, e-Business, and e-Government. Towards this end, this dissertation develops solutions for the following problems related to Web services and Service-Oriented Architectures: (1)  Web service composition. The ability to compose complex Web services from a multitude of available component services is one of the most important problems in service-oriented computing paradigm. In this dissertation, we propose a new framework for modeling complex Web services based on the techniques of abstraction, composition and reformulation. The approach allows service developers to specify an abstract and possibly incomplete specification of the composite (goal) service. This specification is used to select a set of suitable component services such that their composition realizes the desired goal. In the event that such a composition is unrealizable, the cause for the failure of composition is determined and is communicated to the developer thereby enabling further reformulation of the goal specification. This process can be iterated until a feasible composition is identified or the developer decides to abort. (2) Web service specification reformulation . In practice, often times the composite service specification provided by the service developers result in the failure of composition. Typically, handling such failure requires the developer to analyze the cause(s) of the failure and obtain an alternate composition specification that can be realized from the available services. To assist developers in such situations, we describe a technique which given the specification of a desired composite service with a certain functional behavior, automatically identifies alternate specifications with the same functional behavior. At its core, our technique relies on analyzing data and control dependencies of the composite service and generating alternate specifications on-the-fly without violating the dependencies. We present a novel data structure to record these dependencies, and devise algorithms for populating the data structure and for obtaining the alternatives. (3)  Web service substitution. The assembly of a composite service that satisfy a desired set of requirements is only the first step. Ensuring that the composite service, once assembled, can be successfully deployed presents additional challenges that need to be addressed. In particular, it is possible that one or more of the component services participating in a composition might become unavailable during deployment. Such circumstances warrant the unavailable service to be substituted by another without violating the functional and behavioral properties of the composition. To address this requirement, we introduce the notion of context-specific substitutability in Web services, where context refers to the overall functionality of the composition that is required to be  maintained after replacement of its constituents. Using the context  information, we investigate two variants of the substitution problem, namely environment-independent and environment-dependent, where environment refers to the constituents of a composition and show how the substitutability criteria can be relaxed within this model.;The work described above contributed to the design and implementation of MoSCoE---an open-source platform for modeling and executing complex Web services (http://www.moscoe.org).</p>",https://dr.lib.iastate.edu/handle/20.500.12876/69233,"Artificial Intelligence and Robotics,Computer Sciences",Computer science,"Interactive and verifiable web services composition, specification reformulation and substitution",dissertation
Myra B Cohen,"McDevitt, Mikaela",2020-09-23T19:13:00.000,"<p>Users of bioinformatics software tools range from bench scientists with little computational experience, to sophisticated developers.  As the number and types of tools available to this diverse set of users grow, they are also increasing in flexibility. The customization of these tools makes them highly-configurable — where the end user is provided with many customization (configuration) options.  At the same time, biologists and chemists are engineering living organisms by programming their DNA in a process that mimics software development.  As they share their designs and promote re-use, their programs are also emerging as highly-configurable.</p>
<p>As these bioscience systems become mainstream tools for the biology and bioinformatics communities, their dependability, reliability, and reproducibility becomes critical. Scientists are making decisions and drawing conclusions based on the software they use, and the constructs designed by synthetic biologists are being built into living organisms and used in the real world.  Yet there is little help guiding users of bioinformatics tools or those building new synthetic organisms.</p>
<p>As an end user equipped with minimal information, it is hard to predict the effect of changing a particular configuration option, yet the choice of configuration can lead to a large amount of variation in functionality and performance.  Even if the configuration options make sense to an expert user, understanding all options and their interactions is difficult or even impossible to compute due to the exponential number of combinations.  Similarly, synthetic biologists must choose how to combine small DNA segments.  However, there can be millions of ways to combine these pieces, and determining the architecture can require significant domain knowledge.</p>
<p>In this dissertation we address these challenges of interpreting the effects of configurability in two areas in the biosciences: (1) bioinformatics software, and (2) synthetic biology.  We highlight the challenges of configurability in these areas and provide approaches to help users navigate their configuration spaces leading to more interpretable configurable software in the biosciences.</p>
<p>First, we demonstrate there is variability in both the functional and performance outcomes of highly-configurable bioinformatics tools, and find previously undetected faults.  We discuss the implications of this variability, and provide suggestions for developers.  Second, we develop a user-oriented framework to identify the effect of changing configuration options in software, and communicate these effects to the end user in a simplistic format.  We demonstrate our framework in a large study and compare to a state of the art method for performance-influence modeling in software.</p>
<p>Last, we define a mapping of software product line engineering to the domain of synthetic biology resulting in organic software product lines.  We demonstrate the potential reuse and existence of both commonality and variability in an open source synthetic biology repository.   We build feature models for four commonly engineered biological functions and demonstrate how product line engineering can benefit synthetic biologists.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/94336,,"Bioinformatics,Configurability,Interpretability,Software engineering",Interpretability of configurable software in the biosciences,dissertation
"Tavanapong, Wallapak,Zhang, Zhu,Li, Qi","Bang, Yun Qi",N/A,"Quantification learning is a relatively new deep learning task. Differing from a classic classification problem where the class of a single instance is predicted, a quantification model predicts the distribution of classes within a given set of instances. Quantification learning has applications in various domains. For example, in designing political campaign ads, it is important to know the proportion of different aspects voters care about. QuaNet is a recent deep learning quantification model that was shown to achieve good quantification performance. Like many deep learning models, there is no explanation about the contributions of different inputs QuaNet uses to predict a class distribution. In this study, we propose a method to provide such an explanation, which is important to increase users' trust in the model. Our method is the first work on interpreting deep learning quantification models.",https://dr.lib.iastate.edu/handle/20.500.12876/gwW7KdOw,Computer science,"Deep Learning,Interpretation",Interpreting deep text quantification models,thesis
Johnny S. Wong,"Stanley, Fred",2018-08-11T08:01:03.000,"<p>This work focuses on Intrusion Detection System (IDS) and Intrusion Response System (IRS) model for system and network attacks. For decades, IDS has evolved tremendously and has become highly sophisticated. However, the response to an attack is still manually triggered by an administrator who relies on static mapping to counteract the intrusion. The speed of attack-spread and its increased complexities in recent years have shown that it is highly critical to develop an automatic IRS. Moreover, manual responses are not flexible and effective in distributed environment without infrastructure.</p>
<p>This work presents a cost based response model that is tightly coupled with multi-source IDS. It is a known fact that any system can be broken down into smaller granules of services and resources. A dependency graph is employed to describe the relations between services and resources in a system. This dependency graph is also used to propagate the total value of the system down to the service and resource levels. The damage cost of the intrusion and the response cost of the responses are evaluated using the dependency graph. Using several performance metrics, a response which brings the most benefit to the system is deployed.</p>
<p>We demonstrate the abilities of our model by using buffer overflow attack caused by a computer worm on Optimized Link State Routing (OLSR) protocol on a wireless ad-hoc network environment. Experimental results show that our model is effective and is highly practical.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24890,Computer Sciences,"Automated response model,Dependency graph,Generic response model,Intrusion detection and response,OLSR worm,snort",Intrusion detection and response for system and network attacks,thesis
Guang Song,"Song, Jaekyun",2018-08-11T14:48:42.000,"<p>Classical normal mode analysis (CNMA) has been widely acknowledged as one of the most useful simulation tools for studying protein dynamics. CNMA uses a fine-grained all-atom model of proteins and a complex empirical potential. In addition, CNMA requires a structure that must be energetically minimized, which makes the method cumbersome to use, especially for large proteins. In contrast, elastic network models (ENM) use coarse-grained protein models and adopt a simplified potential function. ENM is much faster than CNMA but is less accurate. To take the advantages of both CNMA and ENM, the spring-based normal mode analysis (sbNMA) was developed. It uses a fine-grained all-atom model for proteins and an all-atom empirical force field to maintain accuracy while reducing the computing complexity by eliminating the minimization step. In the previous work on sbNMA, only the CHARMM force field was explored. In this work, we extend the analyses to AMBER, another widely-used force field. We investigate the dependence of sbNMA's performance on force fields. This work provides also insightful understandings of the differences between CHARMM and AMBER.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29995,"Bioinformatics,Biophysics,Computer Sciences","AMBER,ENM,NMA,Normal mode analysis,sbNMA,Vibrational spectrum",Investigating the effects of different force fields on spring-based normal mode analysis,thesis
Kelvin D. Nilsen,"Schmidt, William",2018-08-23T05:16:29.000,"<p>This dissertation proposes a new garbage-collected memory module architecture for hard real-time systems. The memory module is designed for compatibility with standard workstation architectures, and cooperates with standard cache consistency protocols. Processes read and write garbage-collected memory in the same manner as standard memory, with identical performance under most conditions. Occasional contention between user processes and the garbage collector results in delays to the user process of at most six memory cycles. Thus the proposed architecture guarantees real-time performance at fine granularity;This dissertation investigates the viability of the proposed architecture in two senses. First, it demonstrates that a fundamental component of the architecture, the object space manager, can be produced at a reasonable cost. Second, this dissertation reports the results of experiments that measure the performance of the proposed architecture under real workloads. Results of these experiments show that the architecture currently performs more slowly than traditional schemes; but this appears to be correctable by employing a more efficient function call mechanism that caches heap-allocated activation frames;Finally, this dissertation reports on some simple extensions to the C++ programming language to support slice objects. Slice objects, which are supported by the garbage collection architecture, are useful for implementing fragmentable arrays, i.e., arrays in which subarrays may be retained while unused elements become garbage and are collected. Experimental evidence demonstrates that slice objects can be used to implement strings more efficiently than at least some popular class libraries.[superscript]1 ftn[superscript]1This research was supported in part by NSF Grant MIP-9010412 and by a National Science Foundation Graduate Fellowship.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/63528,Computer Sciences,Computer science,Issues in the design and implementation of a real-time garbage collection architecture,dissertation
Hridesh Rajan,"Maddox, Jackson",2018-09-13T06:21:03.000,"<p>A majority of modern software is constructed using languages that compute by producing side-effects such as reading/writing from/to files, throwing exceptions, acquiring locks, etc. To understand a piece of software, e.g. a class, it is important for a developer to understand its side-effects. Similarly, to replace a class with another, it is important to understand whether the replacement is a safe substitution for the former in terms of its behavior, a property known as substitutability, because mismatch may lead to bugs. The problem is especially severe for superclass-subclass pairs since at runtime an instance of the subclass may be used in the client code where a superclass is mentioned. Despite the importance of this property, we do not yet know whether substitutability w.r.t. effects between subclass and superclass is preserved in the wild, and if not what sorts of substitutability violations are common and what is the impact of such violations. This thesis conducts a large scale study on over 20 million Java classes, in order to compare the effects of the methods of subclasses and superclasses in practice. Our comprehensive study considers the exception, synchronization, I/O, and method call effects. It reveals several interesting findings and provides useful guidance for bug detection, testing, and code smell detection tool design.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/30813,Computer Sciences,,Large-scale study of substitutability in the presence of effects,thesis
"Lutz, Robyn R,Basu, Samik,Miner, Andrew,Rajan, Hridesh,Dorman, Karin","Khoshmanesh, Seyedehzahra",N/A,"Developers of software product lines and highly configurable systems reuse and combine features (units of functionality) to build new or customize existing products. However, features can interact in ways that are contrary to developers' intent. Predicting whether a new combination of features will produce an unwanted or even hazardous feature interaction is a continuing challenge. Current techniques to detect unwanted feature interactions are costly, slow, and inadequate. In this thesis, we investigate how to detect unwanted feature interactions early in development and that are scalable to large software product lines or highly configurable systems. 
First, we propose a similarity-based method to identify unwanted feature interactions much earlier in the development process for early detection. It uses knowledge of prior feature interactions stored with the software product line's feature model to help find unwanted interactions between a new feature and existing features. Results show that the approach performs well, with 83% accuracy and 60% to 100% coverage of feature interactions in experiments, and scales to a large number of features.
Moreover, to learn and automate the detection, we show how detecting unwanted feature interactions can be effectively represented as a link prediction problem. We investigate six link-based similarity metrics and evaluate our approach on a software product line benchmark. Results show that the best machine learning algorithms achieve an accuracy of 0.75 to 1 for classifying feature interactions.

Finally, we develop a new approach based on program analysis that extracts feature-relevant learning models from the source code to obtain more semantic details of unwanted feature interactions. The method is capable of learning feature interactions whether constraints on feature combinations are specified or not. If specifications of feature constraints are unavailable, as is common in real-world systems, our approach infers the constraints using feature-related data-flow dependency information. Experimental evaluation on three software product line benchmarks and a highly configurable system shows that this approach is fast and effective.
The contribution is to support developers by automatically detecting those feature combinations in a new product or version that can interact in unwanted or unrecognized ways. This enables a better understanding of hidden interactions and identifies software components that should be tested together because their features interact in some configurations.",https://dr.lib.iastate.edu/handle/20.500.12876/8zn7VDnw,Computer science,"Feature Interaction,Highly Configurable Systems,Machine Learning,Program Analysis,Software Product Lines,Symbolic Execution Tree",Learning feature interactions with and without specifications,dissertation
"Jannesari, Ali,Lutz, Robyn R,Aduri, Pavan,Eulenstein, Oliver,Quinn, Christopher J","Dutta, Akash",N/A,"With the gradual stagnation of Moore's law, compiler developers and hardware vendors have increasingly parallelized source code to improve performance.
Such parallelization efforts have enhanced avenues of performance analysis and improvement.
Ubiquitous heterogeneity in modern parallel programming and High Performance Computing (HPC) also complicates performance analysis.
Meticulously designed compiler-driven optimizations often cannot exploit available performance and other runtime-based analysis come with high overheads.
It is essential, therefore, to identify techniques that improve performance, while reducing overhead.

This thesis presents five studies with this aim in mind.
Each study proposes a novel technique to optimize performance on various metrics, such as time and energy, relevant in HPC landscapes.
These studies provide unique insights into how adapting and improving various modeling techniques lead to significant gains in performance.

The first four chapters build on state-of-the-art deep learning (DL) techniques to improve performance of parallel kernels/applications.
These studies automate the feature generation and extraction process for DL-based performance tuning.
The first study aims to identify common patterns in source code to guide performance optimizations.
The second and third studies explore new ways of code modeling for reducing runtime and energy consumption of parallel applications.
The fourth study aims to further reduce overheads associated with the first three studies and streamlines the code modeling for DL-based optimization works.
Compiler-driven semantic and structural features along with dynamic runtime features are merged to enhance the selections made by the DL models.
Extensive evaluations across a variety of downstream tasks demonstrate that the proposed ideas do substantially help improve results compared to prior state of the arts.

The fifth work helps to address one of the primary shortcomings of the first four chapters and describes a novel online auto-tuner, HHOTuner that can easily and efficiently work with user-defined search spaces.
HHOTuner is a nature-inspired auto-tuner that works seamlessly with user-defined configurable search spaces, and improves results while significantly reducing overheads over prior state-of-the-art online auto-tuners.

The techniques developed through these studies outline a number of unique methods of optimizing performance that help improve configuration selection and resource consumption.
Moreover, this thesis is also self-contained and hopes to address its own shortcomings, by covering different varieties of optimization tasks in HPC.
This author hopes that the ideas presented herein, could help foster new lines of discussion and research, and broaden the techniques used for performance optimizations in the HPC community.",https://dr.lib.iastate.edu/handle/20.500.12876/WwPgEp5z,Computer science,"Auto-tuning,Code Representation,Deep Learning,Harris Hawks Optimization,LLVM IR,Performance Optimization",Learning-based auto-tuning for high performance computing,dissertation
"Rajan, Hridesh,Prabhu, Gurpur,Mitra, Simanta","Manke, Ruchira",N/A,"Deep Learning (DL) is a class of machine learning algorithms that are used in a wide variety of
applications. Like any software system, DL programs may have bugs. To support bug localization
in DL programs, several tools have been proposed in the past. As most of the bugs that occur
due to improper model structure known as structural bugs lead to inadequate performance during
training, it is challenging for developers to identify the root cause and address these bugs. To
support bug detection and localization in DL programs, in this work, we propose Theia, which
detects and localizes structural bugs in DL programs. Unlike the previous works, Theia considers
the training dataset characteristics to automatically detect bugs in DL programs developed using
two deep learning libraries, Keras and PyTorch. Since training the DL models is a time-consuming
process, Theia detects these bugs at the beginning of the training process and alerts the developer
with informative messages containing the bug’s location and actionable fixes which will help them to
improve the structure of the model. We evaluated Theia on a benchmark of 40 real-world buggy DL
programs obtained from Stack Overflow. Our results show that Theia successfully localizes 57/75
structural bugs in 40 buggy programs, whereas NeuraLint, a state-of-the-art approach capable of
localizing structural bugs before training localizes 17/75 bugs.",https://dr.lib.iastate.edu/handle/20.500.12876/ywAbM1nv,Computer science,"bug localization,debugging,deep learning,program analysis",Leveraging data characteristics for bug localization in deep learning programs,thesis
"Rajan, Hridesh,Cohen, Myra B.,Le, Wei,Mitra, Simanta,Zhang, Wensheng","Wardat, Mohammad Ahmad Salem",N/A,"Deep Neural Networks (DNNs) are increasingly used in a wide range of safety-critical applications (e.g., autonomous driving or medical diagnosis systems). However, like traditional software, DNN-based software has faults too, which manifest as poor model performance. Unfortunately, existing debugging techniques do not support localizing and fixing DNN bugs, as the entire model appears as a black box. Furthermore, Deep Learning (DL) programs are fundamentally different from traditional software due to the nonlinearity of the functions. Current DNN bug localization approaches do not understand model behavior, support silent bugs, and correlate bug symptoms with their fixes. Thus it hinders the developers’ ability to ensure the reliability of their systems under design. This dissertation presents three novel techniques for debugging deep neural networks (DNN). These novel techniques are the first to identify, diagnose, and localize faults and provide fix suggestions for DNN models. The first approach, DeepLocalize, converts the DNN into an imperative representation and uses probes to monitor its parameters and layers at training time. DeepLocalize can accurately identify and localize the root causes of numerical bugs, making it more effective than prior works. The second approach, DeepDiagnosis, diagnoses eight types of symptoms while providing actionable fix messages to patch buggy DNN models. DeepDiagnosis can detect silent bugs, which do not result in numerical errors during training. We evaluate DeepDiagnosis on a comprehensive set of 444 models, including 53 real-world examples from GitHub and Stack Overflow and 391 models curated by AUTOTRAINER. Our evaluation shows that DeepDiagnosis has better accuracy and performance than prior works, such as UMLUAT and DeepLocalize. Finally, our third approach, Deep4Deep, is a data-driven technique that leverages semantic features from DNN models to detect and diagnose faults. This technique extracts the faults’ semantic information during DNN training while leveraging it as a training dataset to learn and infer fault patterns. This approach automatically links bug symptoms to their root causes. Thus, differently from prior works, Deep4Deep does not rely on manually crafted symptoms to root cause mappings. We evaluated our approach on benchmarks with real-world and mutated models. We observed that it effectively detects and diagnoses various bug types. In particular, our evaluation shows that Deep4Deep outperforms prior works for mutated models in terms of accuracy, precision, and recall while achieving comparable results for real-world models. Overall, our approaches exhibit superior fault detection, bug localization, and symptom identification capabilities, thus helping developers efficiently and effectively patch DNN models. Lastly, this dissertation provides a comprehensive investigation and evaluation of our proposed techniques, shedding light on their effectiveness and limitations while providing several future work research directions.",https://dr.lib.iastate.edu/handle/20.500.12876/kv7kWLjv,Computer science,"Debugging,Deep learning bugs,Deep Neural Networks,Fault Location,Program Analysis",Localizing and repairing faults in deep learning programs,dissertation
N/A,"Brewbaker, Chad",2020-11-13T21:12:16.000,"<p>This thesis shows that the number of (0,1)-matrices with n rows and k columns uniquely reconstructible from their row and column sums are the poly-Bernoulli numbers of negative index, B[subscript n superscript (-k)] . Two proofs of this main theorem are presented giving a combinatorial bijection between two poly-Bernoulli formula found in the literature. Next, some connections to Fermat are proved showing that for a positive integer n and prime number p B[subscript n superscript (-p) congruent 2 superscript n (mod p),] and that for all positive integers (x, y, z, n) greater than two there exist no solutions to the equation: B[subscript x superscript (-n)] + B[subscript y superscript (-n)] = B[subscript z superscript (-n)]. In addition directed graphs with sum reconstructible adjacency matrices are surveyed, and enumerations of similar (0,1)-matrix sets are given as an appendix.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97848,,Computer science,"Lonesum (0,1)-matrices and poly-Bernoulli numbers of negative index",thesis
"Liu, Jia (Kevin),Rajan, Hridesh,Zhang, Wensheng,Aduri, Pavan,Zhang, Hongwei","Yu, Menglu",N/A,"This dissertation will focus on modeling and designing efficient resource scheduling and allocation algorithms for deep learning jobs in distributed machine learning systems or computing clusters based on mainstream frameworks (e.g., TensorFlow and PyTorch). Due to the rapid growth of training dataset size and model complexity, it becomes prevailing to leverage the data parallelism to expedite the training process. However, data communication between computing devices (e.g., GPUs) typically becomes the bottleneck to scaling the system. Thus how to alleviate the communication bottleneck when scheduling deep learning jobs in distributed systems attracts increasing attention both in academia and industry recently.

However, designing the resource allocation scheduling algorithms is highly non-trivial. Specifically, the problem typically has packing-type constraints (due to the resource capacity limit), covering-type constraints (due to the job workload requirements), and non-convex constraints (due to the topology, contention, etc), which is NP-Hard in general. Moreover, demanding integer variables adds another layer of difficulty to solve the problem. To overcome these challenges, we need
to design a suite of provable algorithms to schedule the jobs efficiently.

In this thesis, we start with a resource allocation algorithm design for the computing clusters, where we focus on resource allocation without considering the placement for DNN jobs. Then we extend our work to the distributed machine learning systems and computing clusters by jointly optimizing the placement and resource scheduling for DNN jobs. In this thesis, we design schedulers for deep learning jobs with various objectives (e.g., minimize the overall training completion time,
minimize the makespan and maximize the overall job utility). We first work on designing efficient scheduling algorithms based on simplified assumptions like reserved bandwidth for each job and the underlying network is a complete graph. Then we extend our work by taking into practical concerns (e.g., topology mapping and contention among multiple jobs) into consideration when developing schedulers for the distributed machine learning systems and computing clusters.",https://dr.lib.iastate.edu/handle/20.500.12876/Qr9m9omr,Computer science,"Deep Learning Jobs,Distributed Systems,Networking,Optimization,Resource Scheduling",Low-latency computing network resource scheduling and allocation algorithms for deep learning jobs,dissertation
"Ying Cai,Wensheng Zhang,Daji Qiao","Galdames S, Patricio",2018-08-22T17:35:32.000,"<p>A continuous k nearest neighbor (CKNN) query retrieves the set of k mobile nodes that are nearest to a query point, and provides real-time updates whenever this set of nodes changes. A CKNN query can be either stationary or mobile, depending on the mobility of its query point. Efficient processing of CKNN queries is essential to many applications, yet most existing techniques assume a centralized system, where one or more central servers are used for query management. In this thesis, we assume a fully distributed mobile peer-to-peer system, where mobile nodes are the only computing devices, and present a unified platform for efficient processing of both stationary and mobile CKNN queries. For each query, our technique computes a set of safe boundaries and lets mobile nodes monitor their movement with respect to these boundaries. We show that the result of a query does not change unless a node crosses over a safe boundary. As such, our technique requires a query to be re-evaluated only when there is a crossing event, thus minimizing the cost of query evaluation. For performance study, we model the communication cost incurred in query processing with a detailed mathematical analysis and verify its accuracy using simulation. Our extensive study shows that the proposed technique is able to provide real-time and accurate query results with a reasonable cost.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/68978,Computer Sciences,Computer science,Managing continuous k-nearest neighbor queries in mobile peer-to-peer networks,thesis
"Ying Cai,Wallapak Tavanapong","Kim, Kihwan",2018-08-11T06:35:01.000,"<p>Mobile devices have brought new applications into our daily life. However, ecient man-</p>
<p>agement of these objects to support new applications is challenging due to the distributed</p>
<p>nature and mobility of mobile objects. This dissertation describes a new type of mobile peer-</p>
<p>to-peer (M-P2P) computing, namely geotasking, and presents ecient management of mobile</p>
<p>objects to support geotasking. Geotasking mimics human interaction with the physical world.</p>
<p>Humans generate information using sensing ability and store information to geographical lo-</p>
<p>cations. Humans also retrieve this information from the physical locations. For instance, an</p>
<p>installation of a new stop sign at some intersection in town is analogous to an insertion of a</p>
<p>new data item into the database. Instead of processing regular data as in traditional data</p>
<p>management systems, geotasking manages a collection of geotasks, each dened as a computer</p>
<p>program bound to a geographical region. The hardware platform for geotasking consists of</p>
<p>popular networked position-aware mobile devices such as cell phones, personal digital assis-</p>
<p>tants, and laptops. We design and implement novel system software to facilitate programming</p>
<p>and ecient management of geotasks. Such management includes inserts, deletes, updates,</p>
<p>retrieval and execution of a geotask triggered by mobile object correlations, geotask mobil-</p>
<p>ity, and geotask dependency. Geotasking enables useful applications ranging from warning of</p>
<p>dangerous areas for military and search-and-rescue missions to monitoring the population in</p>
<p>a certain area for trac management to informing tourists of exciting events in an area and</p>
<p>other such applications. Geotasking provides a distributed and unied solution for supporting</p>
<p>various types of applications.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25237,Computer Sciences,,Managing motion triggered executables in distributed mobile databases,dissertation
James Oliver,"Mazurick, Adam",2021-06-11T00:48:19.000,"<p>Advertisers currently face a significant challenge marketing to consumers in 2021. It is well established that people in 2021 consume nearly five times as much information as in 1986. To manage this increase of information, consumers have developed complex coping strategies that involve increased multi-tasking among digital applications like Gmail, LinkedIn, Outlook Calendar and Facebook. These very coping strategies that involve multi-tasking are making consumers less efficient at focusing their attention. Whatsmore, consumers are dividing their attention across more interaction technologies. This combination of societal trends presents distinct challenges to advertisers who want consumers to recall their brands, services and products. In an age where consumers own multiple connected devices, how consumers recall ads across interaction technologies and what drives them to act in response to those advertisements are largely unexplored.</p>
<p>In order to explore how people recall the details of advertisements and what factors drive individual willingness to act in response to advertisements, a qualitative study was conducted in which twelve subjects experienced eight commercial variants across popular interaction technologies.</p>
<p>After experiencing commercials across Head-Mounted Virtual Reality, Head-Mounted Mixed Reality, Smart Phones, SmartTVs, Hearables, Smart Watches and Smart Speakers, consumers were able to recall the brands and details of commercials more frequently on Head-Mounted Mixed Reality and Wearable interaction technologies. Moreover, willingness-to-act was highest on Mobile AR, Mixed Reality and Hearable interaction technologies. On this basis, Mobile AR, head-mounted Mixed Reality, Wearable and Hearable interaction technologies should be taken into account when developing advertising communications.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/qzXBx3Pv,,"Hearables,HMD,Mixed Reality,Smart Glasses","Marketing in extended reality, a qualitative exploration of how people recall the  details of advertisements and what factors drive individual willingness to act  across interaction technologies.",thesis
Dimitris Margaritis,"Bromberg, Facundo",2018-08-22T19:42:00.000,"<p>We investigate efficient algorithms for learning the structure of a Markov network from data using the independence-based approach. Such algorithms conduct a series of conditional independence tests on data, successively restricting the set of possible structures until there is only a single structure consistent with the outcomes of the conditional independence tests executed (if possible). As Pearl has shown, the instances of the conditional independence relation in any domain are theoretically interdependent, made explicit in his well-known conditional independence axioms. The first couple of algorithms we discuss, GSMN and GSIMN, exploit Pearl's independence axioms to reduce the number of tests required to learn a Markov network. This is useful in domains where independence tests are expensive, such as cases of very large data sets or distributed data. Subsequently, we explore how these axioms can be exploited to ""correct"" the outcome of unreliable statistical independence tests, such as in applications where little data is available. We show how the problem of incorrect tests can be mapped to inference in inconsistent knowledge bases, a problem studied extensively in the field of non-monotonic logic. We present an algorithm for inferring independence values based on a sub-class of non-monotonic logics: the argumentation framework. Our results show the advantage of using our approach in the learning of structures, with improvements in the accuracy of learned networks of up to 20%. As an alternative to logic-based interdependence among independence tests, we also explore probabilistic interdependence. Our algorithm, called PFMN, takes a Bayesian particle filtering approach, using a population of Markov network structures to maintain the posterior probability distribution over them given the outcomes of the tests performed. The result is an approximate algorithm (due to the use of particle filtering) that is useful in domains where independence tests are expensive.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/69221,"Artificial Intelligence and Robotics,Computer Sciences",Computer science,Markov network structure discovery using independence tests,dissertation
"Gao, Hongyang,Liu, Hailiang,Liu, Jia (Kevin),Vaswani, Namrata,Quinn, Christopher","Gao, Tianxiang",N/A,"In recent years, despite having a significantly larger number of parameters than training samples, overparameterized neural networks have been widely adopted in practical applications, showcasing notable numerical and practical success. This success has sparked theoretical investigations aimed at understanding the underlying phenomenon, known as benign overfitting, where overparameterized neural networks can achieve zero training loss and maintain good performance on unseen data. While previous theoretical studies have produced interesting findings supporting these practical observations, they have primarily focused on shallow neural networks, particularly two-layer networks. However, in practice, deep neural networks with large depths are favored due to their greater expressive capacity and task performance.

To address this gap, we initiate a study on large-depth neural networks by investigating infinite-depth neural networks, specifically, deep equilibrium models (DEQs) and neural ordinary differential equations (neural ODEs). Our analysis reveals that for infinite-depth or large-depth neural networks, the dynamical stability is crucial not only for training but also for generalization. Consequently, to achieve faster training convergence and better generalization performance, a carefully selected scaling strategy and the use of different skip connections are essential to stabilize the information propagation in large-depth neural networks during both forward function evaluation and backward gradient computation.

We propose a simple yet effective scaling strategy and utilize different skip connections to ensure the stability of information propagation in infinite-depth neural networks. As a result, we demonstrate that gradient descent can train infinite-depth neural networks, including DEQs and Neural ODEs, to achieve zero training error and arbitrary small generalization error, provided that the neural network is overparameterized and the training sample is sufficient. Furthermore, we conduct numerical experiments to validate and support our theoretical findings. Additionally, we believe that the techniques introduced in this dissertation are not only applicable to DEQs and Neural ODEs but also extendable to other neural network architectures with large depths, such as residual neural networks, large-depth recurrent neural networks, and graph neural networks.",https://dr.lib.iastate.edu/handle/20.500.12876/jw27jL9v,Artificial intelligence,"Deep Equilibrium Models,Deep Neural Networks,Generalization,Gradient Descent,Neural Ordinary Differential Equations,Optimization",Mastering infinite depths: Optimization and generalization in deeper neural networks,dissertation
"Wong, Johnny,Mitra, Simanta,Tim, U. Sunday,Sukul, Adisak,Cai, Ying,Chang, Carl","Shandilya, Ritu",N/A,"In order to deduce more appropriate and useful recommendations, it has become essential to amalgamate additional information about the user and item rather than just the user-item correlation. In the past decade, researchers have developed various recommendation algorithms to improve the accuracy and efficiency of the recommender systems not only by employing traditional methods of user-item correlation but also by exploiting the additional user and item information such as the features of the user and item. In this research, first we implement a content-based recommender system MATURE, utilizing the additional information about both the user and item to predict the items for such a user who has current mandatory needs to accommodate as opposed to the past preferences.  
The MATURE-driven recommendations mandatorily satisfy the user’s mandatory requirements at a given time; this characteristic of MATURE distinguishes it from the existing algorithms, where recommendations are primarily based on the past preferences and fail to satisfy current mandatory requirements of the user, which might be different to the user’s past preferences. As MATURE is first recommender system, which promisingly undertakes the given mandatory requirements while recommending the most appropriate item set with pertinent justification of the recommendation, it is more suitable in the area of healthcare where users’ needs are critical and cannot be compromised. Therefore, in this research, we expand the applicability of MATURE and focus on its application as a food recommender system, named MATURE-Food, with mandatory nutrient values range. MATURE-Food recommends only those food items that fulfill all mandatory nutritional requirements while considering user’s past preferences.
We further extend our MATURE research in healthcare domain to predict the health status of critically ill patients, where an electrolytes imbalance poses a great threat to their health. Electrolytes imbalance can be an indication of the development of underlying pathophysiology as balancing electrolytes is critical and necessary for appropriate functioning of organs in human body. Monitoring electrolytes imbalance efficiently can increase the chances of early detection of disease and can help to prevent or delay further deterioration of the health by taking appropriate measures in a timely manner to balance the electrolytes. As the intake of nutrients performs a vital role in balancing electrolytes and other substances (metabolic parameters and acid-base), it can be achieved by strictly following a controlled nutrient diet. Therefore, in this research, a health recommender system MATURE-Health is proposed and implemented; which predicts the imbalance of mandatory electrolytes and other substances (metabolic parameters and acid-base) present in blood and recommends the food items with the balanced nutrients to avoid occurrence of the electrolytes imbalance. MATURE-Health forecasts the level of specified electrolytes and other substances based on user’s most recent laboratory reports and the daily nutritional intake to predict the electrolytes imbalance and updates user’s daily nutrient intake to avoid the occurrence of electrolytes imbalance. To recommend the food items having nutritional value according to user’s daily nutrient intake, MATURE-Health relies on MATURE-Food algorithm as latter recommends only those food items that satisfy all mandatory nutrient requirements while also considering user’s past food preferences. To validate the MATURE-Health, we rely on the data from dialysis unit of the University of IOWA and particularly sodium, potassium, and BUN levels have been predicted with prediction algorithm, Random Forest, using patients’ laboratory reports and daily food intake history. And the proposed model demonstrates 99.53%, 96.94% and 95.35% accuracy for sodium, potassium, and BUN respectively.
In summary, we first propose and implement a novel recommender system MATURE in this research work that guarantees and ensures the inclusion of the specified mandatory features, when recommending the items to the user and justifies the recommendation by using the information enclosed in mandatory features. Then, as an application of MATURE, we implement a food recommender system MATURE-Food. And finally, a new health recommender system, MATURE-Health, is proposed and implemented. MATURE-Health predicts the health status of a patient by predicting the imbalance of mandatory electrolytes level and other substances in the blood and prevent or reduce the risk of electrolytes imbalance. It uses MATURE-Food to recommend only food items, which contain the required amount of the nutrients to balance the electrolytes.",https://dr.lib.iastate.edu/handle/20.500.12876/EwpaPbGv,Computer science,"CKD Diet,Electrolyte Imbalance Prediction,Food Recommender System,Health Recommender System,Machine Learning,Nutrient Balanced Diet",MATURE: Recommender System for Mandatory Feature choices,dissertation
Carl K. Chang,"Zhang, Chuanhai",2019-08-21T14:28:34.000,"<p>Many medical image classification tasks have a severe class imbalance problem. That is images of target classes of interest, e.g., certain types of diseases, only appear in a very small portion of the entire dataset. These medical image classification tasks share two common issues. First, only a small labeled training set is available due to the expensive manual labeling by highly skilled medical experts. Second, there exists a high imbalance ratio between rare class and common class. The common class occupies a high percentage of the entire dataset and usually has a large sample variety, which makes it difficult to collect a good representative training set for the common class. Convolutional Neural Network (CNN) is currently a state-of-the-art method for image classification. CNN relies on a large training dataset to achieve high classification performance. However, manual labeling is costly and may not even be feasible, which limits CNN from offering high classification performance in practice. This dissertation addresses these two challenging issues with the ultimate goal to improve classification effectiveness and minimize manual labeling effort by the domain experts.</p>
<p>The main contributions of dissertation are summarized as follows. 1) We propose a new real data augmentation method called Unified LF&SM that jointly learns feature representation and a similarity matrix for recommending unlabeled images for the domain experts to verify in order to quickly expand the small labeled training set. Real data augmentation utilizes realistic unlabeled samples rather than synthetic samples. The key of real data augmentation is how to design an effective strategy to select representative samples for certain classes quickly from a large realistic unlabeled dataset. 2) We investigate the effectiveness of six different data augmentation methods and perform a sensitivity study using training sets of different sizes, varieties, and similarities when compared with the test set. 3) We propose a Hierarchical and Unified Data Augmentation (HuDA) method to collect a large representative training dataset for the common class. HuDA incorporates a class hierarchy: class differences on the high level (between the rare class and the common class) and class differences on the low level (between sub-classes of the rare class or the common class). HuDA is capable of significantly reducing time-consuming manual effort while achieving quite similar classification effectiveness as manual selection. 4) We propose a similarity-based active deep learning framework (SAL), which is the first approach to deal with both a significant class imbalance and a small seed training set as far as we know.</p>
<p>Broader Impact: Triplet-based real data augmentation methods utilize the similarity between samples to learn a better feature representation. These methods aim to guarantee that the computed similarity between two samples from the same class is always bigger than the computed similarity between two samples from two different classes. First, our sensitivity study on six different data augmentation methods shows that triplet-based real data augmentation methods always offer the largest improvement on both the recommendation accuracy and the classification performance. These real data augmentation methods are easily extendable to other medical image classification tasks. Our work provides useful insight into how to choose a good training image dataset for medical image classification tasks. Second, to the best of our knowledge, SAL is the first active deep learning framework that deals with a significant class imbalance. Our experiments show that SAL nearly obtains the upper bound classification performance by labeling only 5.6% and 7.5% of all images for the Endoscopy dataset and the Caltech-256 dataset, respectively. This finding confirms that SAL significantly reduces the experts’ manual labeling efforts while achieving near optimal classification performance. SAL works for multi-class image classification and is easily extendable to other medical image classification tasks as well.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31313,Artificial Intelligence and Robotics,,Medical image classification under class imbalance,dissertation
"Ciardo, Gianfranco,Basu, Samik,Lathrop, James,Miner, Andrew","Hosseini, Seyedehzahra",N/A,"MDDs (multi-valued decision diagrams) are a generalization of binary decision diagrams (BDDs).
In formal method verification tools, efficient MDD manipulation is critical. Despite the usefulness of MDDs in real-world tasks such as discovering the reachability set of a model, hardware resource limitations limit the efficacy of manipulation algorithms. Previous methodologies, such as variable ordering, demonstrate that choosing the right variable order has an influence on the size of the MDDs. Other studies on approximation and manipulation of high-density (finite but large) state systems shows the importance of efficient under-approximation techniques. Such studies have been conducted with an emphasis on binary decision diagrams. These algorithms function with limited memory, making them ideal for  any hardware with limited memory. In this thesis, we focus on memory constrained algorithms, especially under-approximations, for MDDs. 

We present a novel quadratic-time under-approximation technique for MDDs in this work. The method obtains the information by traversing an MDD and selecting either a node or a collection of nodes with the lowest density. The former is more accurate, but more time-consuming, whereas the latter has a better execution time, but it is more eager in deleting nodes, which may result in deleting too many nodes from the MDD.

The primary conclusion of this study is that selecting a collection of nodes for deletion performs better in larger real-world problems in terms of execution time, although selecting one node for removal at a time delivers a more precise outcome. This work highlights the effectiveness of using an under-approximation approach when manipulating multi-valued decision diagrams.",https://dr.lib.iastate.edu/handle/20.500.12876/aw4Ngd7r,Computer science,"binary decision diagrams,memory constrained algorithms,multi-valued decision diagrams,reachability,under-approximation",Memory constrained algorithms for multi-valued decision diagrams,thesis
N/A,"Kwinn, William",2018-08-15T07:39:21.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/79232,Computer Sciences,Computer science,Memory management policies for a hardware implemented computer operating system,dissertation
Steven Cannon,"Yadav, Akshay",2020-06-26T20:06:48.000,"<p>Gene families are groups of genes that have descended from a common ancestral gene present in the species under study. Current, widely used gene family building algorithms are prone to producing incomplete families (under-clustering) or families containing wrong or non-family sequences (over-clustering). In this work, we present a sequence-pair-classification-based method that, first, inspects given families for under-clustering and then predicts the missing sequences for the families using family-specific alignment score cutoffs. We test this method on a set of curated, gold-standard families from the Yeast Gene Order Browser (YGOB) database, including 20 yeast species. To check if the method can detect and correct incomplete families obtained using existing family building methods, we test this method on under-clustered yeast families produced using the OrthoFinder tool. We demonstrate the utility of the pair-classification method in merging small, fragmented legume families into larger families, built using the OrthoFinder tool, from 14 legumes species belonging to subfamily Papilionoideae of the plant family Leguminosae. We provide recommendations on different types of family-specific alignment score cutoffs that can be used for predicting the missing sequences based on the ""purity"" of under-clustered families and the chosen precision and recall for prediction. Finally, we provide the containerized version of the pair-classification method that can be applied on any given set of gene families.</p>
<p>In addition to the pair-based classification method, we present a simple hidden Markov model (HMM)-based protocol for merging fragmented families and a phylogeny-based protocol for detecting and splitting over-clustered families. We apply these methods for improving the legume gene families built from 14 legumes species belonging to subfamily Papilionoideae of the plant family Leguminosae, using a custom family building method, that utilizes differences in the synonymous-sites (Ks) in the gene sequences in order to capture the family clusters defined by the whole-genome duplication that occurred in the most recent common ancestor of the subfamily. We also analyze the improvements in the legume families obtained after the application of merging and splitting procedures by comparing the protein domain compositions of the new families against the original families. We also provide the containerized versions of family merging, splitting and scoring methods along with the new set of improved legume families.</p>
<p>We investigate the occurrence of whole-genome duplication events within the Cercidoideae subfamily of the plant family Leguminosae, using evolutionary, phylogenomic, and synteny analyses together with analysis of chromosome counts, from a diverse set of legume species. Based on diverse evidence, we conclude that one of the slow-evolving lineages within Cercidoideae may be unique among legumes in lacking evidence of an independent whole-genome duplication and can be a useful genomic model for the legumes. We are able to show that the genome duplication observed in the other sister lineage within Cercidoideae is most likely due to allotetraploidy involving hybridization between two progenitor species that existed in the Cercidoideae subfamily.</p>
<p>We present a method for tracking protein domain changes in a selected set of species with known phylogenetic relationships, by defining domains as ""features"" or ""descriptors,"" and considering the species (target + outgroup) as instances or data-points in a domain feature matrix. Protein domains can be regarded as sections of protein sequences capable of folding independently and performing specific functions that enable protein sequences to evolve through domain shuffling events like domain insertion, deletion, or duplication. We look for features (domains) that are significantly different between the target species and the outgroup species using a feature selection technique called Mutual-Information (MI) and non-parametric statistical tests (Fisher's exact test/Wilcoxon rank-sum test). We study the domain changes in two large, distinct groups of plant species: legumes (Fabaceae) and grasses (Poaceae), with respect to selected outgroup species, using four types of domain feature matrices: domain content, domain duplication, domain abundance, and domain versatility. The four types of domain feature matrices attempt to capture different aspects of domain changes through which the protein sequences may evolve - i.e. via gain or loss of domains, increase or decrease in the copy number of domains along the sequences, expansion or contraction of domains, or through changes in the number of adjacent domain partners. We report and study the biological functions of the top selected domains from all four feature matrices. In addition, we perform domain-centric Gene Ontology (dcGO) enrichment analysis on all selected domains from all the feature matrices to study the Gene Ontology terms associated with the significantly changing domains in legumes and grasses. We provide a docker container that can be used to perform this analysis on any user-defined sets of species.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/32242,,"Gene family building,Genome evolution,Machine learning,Protein domains,Sequence pair classification",Methods for correcting and analyzing gene families,thesis
"Eve Wurtele,David Fernandez-Baca","Vasanth, Achyuthan",2018-08-11T09:34:44.000,"<p>Background</p>
<p>Interpretation of large volumes of data that has information about genes, regulons, gene ontology and probes are important for the identification of the functionality of individual genes and their role with respect to the organism. Many software tools are available today, but they become difficult to interpret when visualizing large volumes of data and representing relationships between them.</p>
<p>Results</p>
<p>MetViz is an interactive web-based tool that uses novel visualization techniques to represent regulons, genes and the gene ontology hierarchy. The tool provides easier accessibility by making it available as a website requiring no download. Instead of displaying all data and the relationship with each other at once, as is the case with many software tools like Cytoscape, MetViz incrementally displays and associates them based on the user?s interaction with the tool. One can radially visualize and organize data based on different quantitative properties such as Intra regulon density, gene count in a regulon and degree of regulons. It minimizes navigation between different windows and pages by maximizing screen utilization. It is also aimed at reducing the number of clicks performed to view and obtain information by providing easily accessible buttons to different functionalities, multiple ways to interact with data and perform the same function, and also making use of unique techniques such as a modified version of the icicle graph to display gene ontology information.</p>
<p>Conclusions</p>
<p>MetViz allows users to search for Gene Ontology terms, Regulons or Genes of interest and obtain information regarding them. It helps in understanding the relationship between Gene Ontology Terms and Regulons, Genes and Regulons & Gene Ontology terms and Genes. It provides vital statistics like the genes present in a particular regulon, their count, evidence data, the pearson correlation matrix between the probes associated with genes, shortest path between regulons to show how closely they are related etcetera. MetViz also enables integration with other softwares such as MetaOmGraph.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26685,"Bioinformatics,Computer Sciences","gene,gene ontology,regulon,visualization","MetViz: an online visualization tool for regulons, genes and gene ontology",thesis
Wensheng Zhang,"Tong, Bin",2018-08-11T17:37:28.000,"<p>Over the past decade, sensor networks have consistently been a focus of the computer research community, and a large number of prototypes have been built for military and civilian applications. One of the fundamental research issues of sensor networks is the energy scarcity problem. Due to the nature of sensor networks, e.g., small-size sensor nodes and large-scale deployment, sensor nodes cannot carry a large amount of energy or be conveniently recharged. As a result, the amount of energy that can be carried by a sensor node fundamentally limits the use of sensor networks. A large number of schemes have been proposed to address this issue. These schemes, however, have one or more the following drawbacks: (i) energy cannot be replenished to the network, and thus the network lifetime is bounded by the amount of energy preloaded to sensor nodes; (ii) the ways of replenishing energy to the network may not be practical and reliable, and thus may not support normal operations of the network; and (iii) sensor nodes drained of energy are left in the deployment field, and thus may cause pollution to the environment.</p>
<p>Fundamentally addressing this problem requires energy to be continually replenished to sensor nodes. This can be achieved in two approaches (i) The Node Reclamation and Replacement Approach: Sensor nodes with low or no energy are reclaimed periodically, and are replaced with fully charged ones. (ii) The Wireless Recharging Approach: Sensor nodes are periodically recharged with energy transmitted from wireless chargers over radio. Both the approaches exploit mobility in accomplishing energy replenishment. Specifically, one or more mobile agents, which could be human technicians or robots, travel around the network, and perform sensor reclamation and replacement or wireless recharging task.</p>
<p>In this dissertation, we propose an array of new mobility-assisted energy replenishment schemes.</p>
<p>Firstly, for the node reclamation and replacement approach, we propose node replacement and reclamation (NRR) strategy, with which a mobile robot or human labor periodically traverses the sensor network, reclaims nodes with low or no power supply, replaces them with fully-charged ones, and brings the reclaimed nodes back to an energy station for recharging. To effectively and efficiently realize the NRR strategy for different application scenarios, we present several implementing schemes for NRR under point coverage and area coverage models, respectively. We also present schemes to improve reliability in implementation of NRR.</p>
<p>Secondly, the wireless recharging approach takes advantage of emerging wireless recharging technology to continually transfer energy into the network. To support long network lifetime with the wireless recharging approach, the recharging agents' activities and sensors' activities can be scheduled in a similar way as with the node reclamation and replacement approach. Therefore, in this line of research, we focus on a unique problem with the wireless recharging technology, that is, how wireless recharging affects sensor network deployment and routing arrangement. We prove the problem is NP-complete, and propose heuristic algorithms to solve it.</p>
<p>Extensive analysis and simulations have been conducted to verify the effectiveness and efficiency of the proposed schemes. As the battery technology lags far behind that of MEMS, we believe energy replenishment is necessary to long-lived surveillance sensor networks. To the best of our knowledge, our works of sensor node reclamation and replacement and wireless recharging are among the first efforts on studying how to re-design sensor networks to fully leverage different energy replenishment techniques.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25084,Computer Sciences,"Energy Replenishment,Sensor Networks",Mobility-assisted energy replenishment for sensor networks,dissertation
"Rozier, Kristin Y.,Ciardo, Gianfranco,Basu, Samik,Lutz, Robyn,Rajan, Hridesh","Dureja, Rohit",N/A,"In the early stages of design, there are frequently many different models of the system under development constituting a design space. The different models arise out of a need to weigh different design choices, to check core capabilities of system versions with varying features, or to analyze a future version against previous ones in the product line. Every unique combinations of choices yields competing system models that differ in terms of assumptions, implementations, and configurations. Formal verification techniques, like model checking, can aid system development by systematically comparing the different models in terms of functional correctness, however, applying model checking off-the-shelf may not scale due to the large size of the design spaces for today’s complex systems. We present scalable algorithms for design-space exploration using model checking that enable exhaustive comparison of all competing models in large design spaces. 

Model checking a design space entails checking multiple models and properties. Given a formal representation of the design space and properties expressing system specifications, we present algorithms that automatically prune the design space by finding inter-model relationships and property dependencies. Our design-space reduction technique is compatible with off-the-shelf model checkers, and only requires checking a small subset of models and properties to provide verification results for every model-property pair in the original design space. We evaluate our methodology on case-studies from NASA and Boeing; our techniques offer up to 9.4× speedup compared to traditional approaches. 

We observe that sequential enumeration of the design space generates models with small incremental differences. Typical model-checking algorithms do not take advantage of this information; they end up re-verifying “already-explored” state spaces across models. We present algorithms that learn and reuse information from solving related models against a property in sequential model-checking runs. We formalize heuristics to maximize reuse between runs by efficient “hashing” of models. Extensive experiments show that information reuse boosts runtime performance of sequential model-checking by up to 5.48×.

Model checking design spaces often mandates checking several properties on individual models. State-of-the-art tools do not optimally exploit subproblem sharing between properties, leaving an opportunity to save verification resource via concurrent verification of “nearly-identical” properties. We present a near-linear runtime algorithm for partitioning properties into provably high-affinity groups for individual model-checking tasks. The verification effort expended for one property in a group can be directly reused to accelerate the verification of the others. The high-affinity groups may be refined based on semantic feedback, to provide an optimal multi-property localization solution. Our techniques significantly improve multi-property model-checking performance, and often yield >4.0× speedup. 

Building upon these ideas, we optimize parallel verification to maximize the benefits of our proposed techniques. Model checking tools utilize parallelism, either in portfolio mode where different algorithm strategies run concurrently, or in partitioning mode where disjoint property subsets are verified independently. However, both approaches often degrade into highly-redundant work across processes, or under-utilize available processes. We propose methods to minimize redundant computation, and dynamically optimize work distribution when checking multiple properties for individual models. Our techniques offer a median 2.4× speedup for complex parallel verification tasks with thousands of properties.",https://dr.lib.iastate.edu/handle/20.500.12876/erLKEMev,Computer science,"design space exploration,equivalence checking,formal methods,model checking,parallel verification,redundancy removal","Model checking large design spaces: Theory, tools, and experiments",dissertation
Jin Tian,"Kang, Changsung",2018-08-12T03:08:53.000,"<p>Finding cause-effect relationships is the central aim of many studies in the physical, behavioral, social and biological sciences. We consider two well-known mathematical causal models: Structural equation models and causal Bayesian networks. When we hypothesize a causal model, that model often imposes constraints on the statistics of the data collected. These constraints enable us to test or falsify the hypothesized causal model. The goal of our research is to develop efficient and reliable methods to test a causal model or distinguish between causal models using various types of constraints.</p>
<p>For linear structural equation models, we investigate the problem of generating a small number of constraints in the form of zero partial correlations, providing an efficient way to test hypothesized models. We study linear structural equation models with correlated errors focusing on the graphical aspects of the models. We provide a set of local Markov properties and prove that they are equivalent to the global Markov property.</p>
<p>For causal Bayesian networks, we study equality and inequality constraints imposed on data and investigate a way to use these constraints for model testing and selection. For equality constraints, we formulate an implicitization problem and show how we may reduce the complexity of the problem. We also study the algebraic structure of the equality constraints. For inequality constraints, we present a class of inequality constraints on both nonexperimental and interventional distributions.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25351,Computer Sciences,"Bayesian network,causal model,graphical model",Model testing for causal models,dissertation
Yan-bin Jia,"Tian, Jiang",2018-08-11T09:07:36.000,"<p>Deformable modeling of thin shell-like and other objects have potential application in robot grasping, medical robotics, home robots, and so on. The ability to manipulate electrical and optical cables, rubber toys, plastic bottles, ropes, biological tissues, and organs is an important feature of robot intelligence. However, grasping of deformable objects has remained an underdeveloped research area. When a robot hand applies force to grasp a soft object, deformation will result in the enlarging of the finger contact regions and the rotation of the contact normals, which in turn will result in a changing wrench space. The varying geometry can be determined by either solving a high order differential equation or</p>
<p>minimizing potential energy. Efficient and accurate modeling of deformations is crucial for grasp analysis. It helps us predict whether a grasp will be successful from its finger placement and exerted force, and subsequently helps us design a grasping strategy.</p>
<p>The first part of this thesis extends the linear and nonlinear shell theories to describe extensional, shearing, and bending strains in terms of geometric invariants including the principal curvatures and vectors, and the related directional and covariant derivatives. To our knowledge, this is the first non-parametric formulation of thin shell strains. A computational procedure for the strain energy is then offered for general parametric shells. In practice, a shell deformation is conveniently represented by a subdivision surface. We compare the results via potential energy</p>
<p>minimization over a couple of benchmark problems with their analytical solutions and the results generated by two commercial softwares ABAQUS and ANSYS. Our method achieves a convergence rate an order of magnitude higher. Experimental validation involves regular and freeform shell-like objects (of various materials) grasped by a robot hand, with the results compared against scanned 3-D data (accuracy 0.127mm). Grasped objects often undergo sizable shape changes, for which a much higher modeling accuracy can be achieved using the nonlinear elasticity theory than its linear counterpart.</p>
<p>The second part numerically studies two-finger grasping of deformable curve-like objects under frictional contacts. The action is like squeezing. Deformation is modeled by a degenerate version of the thin shell theory. Several differences from rigid body grasping are shown. First, under a squeeze, the friction cone at each finger contact rotates in a direction that depends on the deformable object's global geometry, which implies that modeling is necessary for grasp prediction. Second, the magnitude of the grasping force has to be above certain threshold to achieve equilibrium. Third, the set of feasible finger placements may increase significantly compared to that for a rigid object of the same shape. Finally, the ability to resist disturbance is bounded in the sense that</p>
<p>increasing the magnitude of an external force may result in the breaking of the grasp.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25718,Computer Sciences,,Modeling and grasping of thin deformable objects,dissertation
Leslie Miller,"Ramalingam, Karthik Narayanan",2018-08-11T11:44:50.000,"<p>The advantage of bringing computation to field applications has led to an increase in location-based computing by many government and industrial organizations. Putting computing devices in the field creates a number of interesting challenges. Issues such as smaller screen size and weather complications are added to issues like varying levels of user ability. Adaptive interfaces provide the designer with the possibility of mitigating some of these challenges.</p>
<p>This thesis describes an error-based approach for adapting user interfaces to enhance performance in field settings. The proposed approach builds on vision and motor errors as a means of triggering the adaptive interface. The model and the current status of the approach are discussed.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24318,Computer Sciences,,Modeling error-based Adaptive User Interfaces,thesis
"Jia, Yan-Bin,Bastawros, Ashraf,Oliver, James,Tian, Jin,Wongpiromsarn, Tichakorn","Jamdagni, Prajjwal Prem",N/A,"There is a huge potential for automating the process of cutting fruits and vegetables in the food industry. The task is labor-intensive with a high risk of injury due to working in the proximity of sharp cutting tools. There has also been an increasing focus on hygiene and safety after the event of the recent pandemic. This thesis takes the first step to address the challenges of the robotic cutting of fruits and vegetables (which often deform during the action).

For the robot to control the knife skillfully, estimates are needed for the various forces experienced by the knife as it fractures a soft food item. We leverage the finite element analysis (FEA) and energy-based linear elastic fracture mechanics (LEFM). FEA is used for calculating the deformation and cutting force, while LEFM is utilized for predicting when the fracture happens and how much the crack propagates for an amount of work done by the knife.

The use of a 3D mesh model in FEA can be computationally prohibitive for achieving the desired accuracy since numerous tiny elements are required near the knife's moving edge. Instead, we represent the object as evenly spaced slices normal to the cutting plane such that the fracturing of each slice is modeled with a 2D mesh based on plane strain.  Fracture, deformation, and involved forces are then interpolated between every two adjacent slices. In addition to the dimensional reduction, this scheme also enables parallelization to further reduce the time of modeling.

The robot needs to demonstrate skills such as slice, rock-chop, etc. to reduce the cutting effort and generate pieces of desired shapes.  We perform a strain-based analysis to characterize the reduction in force as a result of slicing in comparison with pressing by the knife. This approach is then used to model the effect of knife geometry on the cutting force for a translating knife. A further extension is carried out to explain the effect of the knife’s rotation via a piecewise linear approximation of its trajectory. The results are used to control the knife while rock chopping an eggplant. We have conducted experiments with an ADEPT robot and 4-DOF WAM arm to demonstrate accuracies of force and shape modeling, and in knife control.",https://dr.lib.iastate.edu/handle/20.500.12876/azJ4ZWgv,"Robotics,Computational physics,Computer science","Dexterous manipulation,Fracture mechanics,Modeling,Robotic cutting,Simulation","Modeling of fruit and vegetable cutting: Analysis, algorithms, and robot experiments",dissertation
Robyn R. Lutz,"Nakayama, Brian",2018-08-11T11:08:29.000,"<p>The creation of correctly assembling DNA origami often requires several iterations wherein a researcher tries and troubleshoots an incremental design. In each iteration there exists one or more costly failures that often take immense time or materials to find. These failures occur in part due to a lack of in-depth understanding of how DNA origami self-assembles and functions. To aid researchers in developing correct DNA origami designs, this thesis describes the creation of a DNA origami failure catalog as well as models for elucidating as-of-yet only partially understood properties of DNA origami. The failure catalog helps laboratory scientists gather requirements to preempt failures in their origami designs, and helps laboratory scientists troubleshoot their experiments after the implementation of a design by querying the catalog. Use of the catalog then helps verify the properties of new macro and micro models for DNA origami introduced here. These micro and macro models open up future ways to evaluate DNA origami through a mathematically more rigorous framework. By using both captured knowledge of previous design failures and novel theoretical modeling techniques, this work seeks to reduce the gap in understanding between design and implementation of DNA origami.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29277,"Biochemistry,Computer Sciences","Computer Science,DNA,Failures,Modeling,Nanotechnology,Origami,Requirements Engineering",Modeling technologies and methods for DNA origami,thesis
Gary T. Leavens,"Ruby, Clyde",2018-08-25T03:46:42.000,"<p>The documentation of object-oriented frameworks and class libraries needs to provide enough information so programmers can reason about the correctness of subclass methods without superclass code. Even though a superclass method satisfies its specification and behaves properly in the context of the superclass itself, a new subclass may cause that method to have unexpected or unverifiable behavior. For example, inherited superclass code can call down to subclass methods which may cause a superclass method to no longer satisfy its specification. Furthermore, without superclass code, downcalls can result in unverifiable side-effects. Aliasing can also result in unexpected or unverifiable side-effects;In this dissertation, we present a reasoning technique that allows programmers, who have no access to superclass code, to avoid the problems caused by downcalls and aliasing. The rules use the specification of the abstract data representation of a class and the frame axiom of each method to determine when a method override is necessary and when verifiers can safely reason about the behavior of super-calls;We describe a type system and propose a tool that would warn when a super-call is unsafe or when a superclass method needs to be overridden. A verification logic is also presented and proved sound. The verification logic is based on specifications given in the Java Modeling Language (JML) and uses superclass and subclass specifications to modularly verify the correctness of subclass code;A set of guidelines is proposed for class library implementers that, if followed, guarantees that superclass methods will always be safe to call and that our verification logic can safely be used. These guidelines make our technique easy to use in practice.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/72705,Computer Sciences,Computer science,Modular subclass verification: safely creating correct subclasses without superclass code,dissertation
Gary T. Leavens,"Shaner, Steve",2018-08-11T12:57:22.000,"<p>Formal specification languages improve the flexibility and reliability of software. They capture program properties that can be verified against implementations of the specified program. By increasing the expressiveness of specification languages, we can strengthen the argument for adopting formal specification into standard programming practice.</p>
<p>The higher-order method (HOM) is a kind of method whose behavior critically depends on one or more mandatory calls in its body. Programmers using HOMs would like to reason about the HOM's behavior, but revealing the entire code for such methods restricts writers of HOMs to a specific implementation.</p>
<p>This thesis presents a simple, intuitive extension of JML, a formal specification language for Java, that enables client reasoning about the behavior of HOMs in a sound and modular way. Furthermore, our particular technique is capable of fully automatic checking with lower specification overhead than previous solutions.</p>
<p>Supporting client reasoning about HOMs enables formal verification of some of the behavioral properties of HOM-using object-oriented design patterns, like Observer and Template Method. The technique also applies to specifying HOM behavior in any procedural language.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25399,Computer Sciences,"greybox,programming,specification,verification",Modular verification of higher-order methods with mandatory calls specified by model programs,thesis
"Le, Wei,Tavanapong, Wallapak,Quinn, Christopher","Thapa, Samrajya",N/A,"In this thesis, we introduce a novel Multi-Modal Contrastive Pre-training Framework that synergistically combines X-rays, electrocardiograms (ECGs), and radiology/cardiology Reports. This approach leverages transformers to encode these diverse modalities into a unified representation space, aiming to enhance diagnostic accuracy and facilitate comprehensive patient assessments. To the best of our knowledge, this is the first work to propose an integrated model that combines X-ray, ECG, and Radiology/Cardiology Report. By utilizing contrastive loss, MoRE effectively aligns modality-specific features into a coherent embedding, which supports various downstream tasks such as zero-shot classification and multimodal retrieval. Employing our proposed methodology, we achieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and PtbXl downstream datasets, surpassing existing multimodal approaches. Our proposed framework shows significant improvements in capturing intricate inter-modal relationships and its robustness in medical diagnosis that sets a precedent for future research in multimodal learning in the healthcare sector.",https://dr.lib.iastate.edu/handle/20.500.12876/5w5pgj5z,Artificial intelligence,"Artificial Intelligence,Cardiology,Healthcare,Interpretability,Multimodal,Radiology","MoRE: Multi-Modal contrastive pre-training with transformers on X-Rays, ECGs, and radiology/cardiology report",thesis
Yan-Bin Jia,"Xie, Shengwen",2021-01-16T18:26:58.000,"<p>Many peg-hole-insertion strategies have been developed for autonomous assembly tasks. Unfortunately, none of them appear applicable to the simple task of mounting a screwdriver, which has a thin tip, onto a screw, whose small head and narrow drive significantly limit the work space. The task has thus remained a challenging test for a robot's dexterity. In this thesis, we have developed a mounting strategy for a robotic arm that proceeds in two phases: a) screwdriver tip localization relative to the screw's drive via sliding and b) tip insertion into the slot via rotation. During the entire operation, the contact between the screwdriver's tip and the screw head is maintained with a sequence of three hybrid controllers.</p>
<p>The hybrid controller has two goals---motion tracking and force regulating. As long as these two goals are not mutually exclusive, they can be decoupled in some way. In this thesis, we make use of the smooth and invertible mapping from the joint space to the task space to decouple the two control goals and design controllers separately. The traditional motion controller in the task space is used for motion control, while the force controller is designed through manipulating the desired trajectory to regulate the force indirectly.</p>
<p>Two case studies---contour tracking/polishing surfaces and grabbing boxes with two robotic arms---are presented to show the efficacy of the hybrid controller, and simulations with physics engines are carried out to validate the efficacy of the proposed method. The mounting strategy using hybrid control is simulated on the same platform for demonstration as well.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/94581,,"hybrid control,mounting,screwdriver",Mounting a screwdriver onto a screw using hybrid control,dissertation
N/A,"Leiva, Héctor",2020-08-05T05:05:53.000,"<p>Many real-world data sets are organized in relational databases consisting of multiple tables and associations. Other types of data such as in bioinformatics, computational biology, HTML and XML documents require reasoning about the structure of the objects. However, most of the existing approaches to machine learning typically assume that the data are stored in a single table, and use a propositional (as opposed to relational) language for discovering predictive models. Hence, there is a need for data mining algorithms for discovery of a-priori unknown relationships from multi-relational data. This thesis explores a new framework for multi-relational data mining. It describes experiments with an implementation of a Multi-Relational Decision Tree Learning (MRDTL) algorithm for induction of decision trees from relational databases based on an approach suggested by Knobbe et al., 1999. Our experiments with widely used benchmark data sets (e.g., the carcinogenesis data) show that the performance of MRDTL is competitive with that of other algorithms for learning classifiers from multiple relations including Progol (Muggleton, 1995) FOIL (Quinlan, 1993), Tilde (Blockeel, 1998). Preliminary results indicate that MRDTL, when augmented with principled methods for handling missing attribute values, is likely to be competitive with the state-of-the-art algorithms for learning classifiers from multiple relations on real-world data sets drawn from bioinformatics applications (prediction of gene localization and gene function) used in the KDD Cup 2001 data mining competition (Cheng et al., 2002).</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97506,,Computer science,MRDTL: a multi-relational decision tree learning algorithm,thesis
Arthur E. Oldehoeft,"Moukaddam, Samir",2018-08-15T06:51:13.000,"<p>Multilevel security deals with the problem of controlling the flow of classified information. We present multilevel information flow control mechanisms for distributed systems that allow concurrent accesses to shared data. In a distributed computing environment, the different sites communicate through message passing. Our security mechanisms check the security of information flows caused by computations within individual sites as well as ones caused by communications among the sites. The correct behavior of the security mechanisms cannot be guaranteed if the allowed concurrency is left uncontrolled in the system. We present concurrency control mechanisms for the security mechanisms. In the presence of such concurrency control mechanisms, the consistency of the security data, which the security mechanisms rely upon, is preserved. Finally, we also present schemes to increase the efficiency and the precision of the security mechanisms.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/83012,Computer Sciences,Computer science,Multilevel security and concurrency control for distributed computer systems,dissertation
Samik Basu,"Nudurupati, Sai Sravanthi",2018-08-11T14:40:56.000,"<p>Study of diffusion or propagation of information over a network of connected entities play a vital role in understanding and analyzing the impact of such diffusion, in particular, in the context of epidemiology, and social and market sciences. Typical concerns addressed by these studies are to control the diffusion such that influence is maximally (in case of opinion propagation) or minimally (in case of infectious disease) felt across the network. Controlling diffusion requires deployment of resources and often availability of resources are socio-economically constrained. In this context, we propose an agent-based framework for resource allocation, where agents operate in a cooperative environment and each agent is responsible for identifying and validating control strategies in a network under its control. The framework considers the presence of a central controller that is responsible for negotiating with the agents and allocate resources among the agents. Such assumptions replicates real-world scenarios, particularly in controlling infection spread, where the resources are distributed by a central agency (federal govt.) and the deployment of resources are managed by a local agency (state govt.).</p>
<p>If there exists an allocation that meets the requirements of all the agents, our framework is guaranteed to find one such allocation. While such allocation can be obtained in a blind search methods (such as checking the minimum number of resources required by each agent or by checking allocations between each pairs), we show that considering the responses from each agent and considering allocation among all the agents results in a “negotiation” based technique that converges to a solution faster than the brute force methods. We evaluated our framework using data publicly available from Stanford Network Analysis Project to simulate different types of networks for each agents.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29575,Computer Sciences,,Negotiation Based Resource Allocation to Control Information Diffusion,thesis
Vasant Honavar,"Chen, Chun-Hsien",2018-08-23T11:30:25.000,"<p>Artificial neural networks (ANN), due to their inherent parallelism, potential for fault tolerance, and adaptation through learning, offer an attractive computational paradigm for a variety of applications in computer science and engineering, artificial intelligence, robotics, and cognitive modeling. Despite the success in the application of ANN to a broad range of numeric tasks in pattern classification, control, function approximation, and system identification, the integration of ANN and symbolic computing is only beginning to be explored. This dissertation explores to integrate ANN and some essential symbolic computations for content-based associative symbolic processing. This offers an opportunity to explore the potential benefits of ANN's inherent parallelism in the design of high performance computing systems for real time content-based symbolic processing applications. We develop methods to systematically design massively parallel architectures for pattern-directed symbol processing using neural associative memories as key components. In particular, we propose neural architectures for content-based as well as address-based data storage and recall, information retrieval and database query processing, elementary logical inference, sequence processing, and syntax analysis. Their potential advantages over conventional serial computer implementations of the same functions are examined in the dissertation.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/65134,"Artificial Intelligence and Robotics,Computer Sciences",Computer science,"Neural architectures for database query processing, syntax analysis, knowledge representation, and inference",dissertation
Shashi Gadia,"Cheng, Tsz-Shing",2018-08-23T15:08:52.000,"<p>We present an object-relational model for uniform handling of dimensional data. Spatial, temporal, spatio-temporal and ordinary data are special cases of dimensional data. The said uniformity is achieved through the concept of dimension alignment, which automatically allows lower dimensional data and queries to be used in a higher dimensional context;Unlike ordinary data, dimensional objects are interwoven. We introduce object identity (oid) fragments to circumvent data redundancy at logical level. Computed types are placed appropriately in a type hierarchy to allow maximal use of existing methods. A query language for spatio-temporal data is presented for associative navigation. A framework for algebraic optimization of the query language is suggested;A pattern matching language is designed for complex querying of spatio-temporal data which seamlessly extends the associative navigation in our query language. The pattern matching language recognizes special features of time and space providing an appropriate level of abstraction for application development compared to traditional languages. This reduces the need for embedding the query language in a lower level language such as C++. The pattern matching language is also dimensionally extensible. The pattern matching allows query of data with multiple granularities and continuous data. It also provides hooks for direct query of scientific data (observations);Our model is dimensionally extensible, and also an extension of a relational model for dimensional data. Moreover the dimensionality and addition of oids are mutually orthogonal concepts. Thus starting from classical ordinary data, one may migrate to higher forms of relational or object-relational data in any sequence, without having to recode application software. Our model does not deal with complex objects, which is left as a future extension.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/63955,Computer Sciences,Computer science,Object-relational spatio-temporal databases,dissertation
"Cai, Ying,Tavanapong, Wallapak,Wong, Johnny S.,Tian, Jin,Sukul, Adisak,Peterson, David A. M.","Khaleel, Mohammed Ismael",N/A,"Deep neural networks have achieved remarkable success in various fields, ranging from natural language processing to image recognition. However, the inherent opacity of deep neural networks has raised concerns about their explainability and interpretability. Explainability and interpretability are critical issues for the future deployment of artificial intelligence in several domains, including healthcare. They improve model transparency, which increases the trustworthiness of the machine learning models and their outputs. This dissertation explores critical challenges surrounding the explanation and interpretation of deep neural networks across various domains, such as natural language processing and image recognition.
The first challenge addressed focuses on the evaluation of interpretation methods for text classification. Existing methods rely heavily on classification accuracy or prediction confidence, lacking a quantifiable measure of alignment with human interpretation. This is due to the lack of a large publicly available interpretation ground truth. Such a ground truth will help advance interpretation methods by providing better quantitative evaluation. Manual labeling of important words for each document to create a large interpretation ground truth is very time-consuming and prone to significant disagreement among human annotators. To tackle this issue, we introduce the Interpretation methods for Deep text Classification benchmark (IDC), comprising three pseudo-interpretation ground truth datasets and three performance metrics. The pseudo-interpretation ground truth generated by IDC agrees with human annotators on sampled movie reviews. The IDC benchmark facilitates the quantitative evaluation of six recent interpretation methods.
The second challenge delves into the limitations of existing pixel-based interpretation methods in providing meaningful insights into complex concepts, particularly in medical image recognition. Concept-based interpretation methods, on the other hand, require adding self-explainable layers to the neural network, which will impact the prediction accuracy. Furthermore, interpretation methods that provide multiple levels of concepts require manual labeling of these concepts. We propose the Hierarchical Visual Concept (HVC) interpretation framework, which hierarchically presents the most relevant visual concepts at multiple semantic levels to enhance model interpretability without sacrificing accuracy. This approach automatically learns concepts during training, eliminating the need for costly manual labeling.
Additionally, we adapted the HVC framework to introduce VisActive, a visual concept-based active learning method for image classification under class imbalance. VisActive recommends the most informative images from a large unlabeled and highly imbalanced dataset for manual labeling. The goal is to improve the performance of an image classifier while minimizing manual labeling efforts. VisActive effectively leverages visual concepts to expand the labeled dataset size, promoting diversity and balance in class representations. VisActive achieved significant performance improvements in comparison to the state-of-the-art deep active learning methods.
The third challenge focuses on explaining deep learning model failures. We introduce ClarifyConfusion, an explanation method that learns visual prototypes of each class to identify the causes of model confusion. By visualizing the image features contributing to incorrect classifications, ClarifyConfusion provides insights into model failure cases, which is essential for enhancing the safety and reliability of deep learning systems.
Overall, this dissertation offers novel solutions to enhance the explanation and interpretation of deep neural networks, contributing to the advancement of transparent and trustworthy artificial intelligence systems across various application domains.",https://dr.lib.iastate.edu/handle/20.500.12876/OrD8Djpr,"Computer science,Mechanical engineering","Deep neural network,Explainable AI,Image Recognition,Machine learning interpretation",On interpretation methods for deep neural networks,dissertation
"Albert L. Baker,Gary T. Leavens","Wahls, Timothy",2018-08-23T01:26:25.000,"<p>Executable specifications can serve as prototypes of the specified system and as oracles for automated testing of implementations, and so are more useful than non-executable specifications. Executable specifications can also be debugged in much the same way as programs, allowing errors to be detected and corrected at the specification level rather than in later stages of software development. However, existing executable specification languages often force the specifier to work at a low level of abstraction, which negates many of the advantages of non-executable specifications. This dissertation shows how to execute specifications written at a level of abstraction comparable to that found in specifications written in non-executable specification languages. The key innovation is an algorithm for evaluating and satisfying first order predicate logic assertions written over abstract model types. This is important because many specification languages use such assertions. Some of the features of this algorithm were inspired by techniques from constraint logic programming.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/63911,Computer Sciences,Computer science,On the execution of high level formal specifications,dissertation
"stephen gilbert,jin tian","Ramaswamy, Nandhini",2018-08-11T09:10:50.000,"<p>English is the most prominent second language used in educational programs throughout the world. Unfortunately, there is a limitation of time and skill to guide students with learning the language and for evaluating their writings. Automated Writing Evaluation (AWE) tools would help in addressing this gap.</p>
<p>In this thesis, I document a contribution to the field of Automated Writing Evaluation in the form of a new AWE tool called the Research Writing Tutor (RWT). The system design, user interface design, and features of this tool are introduced first, and then the findings obtained from an user evaluation study are reported. The website has been designed and developed to be user friendly. This tool could be of great use to graduate students and undergraduates in writing research reports, articles, and thesis or dissertations.</p>
<p>Unlike most studies that concentrate on the accuracy of the AWE systems, this study aims at the usability and utility of the RWT in addition to the trust on automated systems.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26932,"Computer Sciences,Linguistics","automated writing evaluators,intelligent tutors,research writing tutor,usability of writing tools,user interface evaluation",Online tutor for research writing,thesis
"Jannesar, Ali,Le, Wei,Zhang, Wensheng","Sudduth, Trenton",N/A,"The increase in scale of high performance computing (HPC) workloads and the need of faster (close to real-time) executions has necessitated the increase in computing power of HPC systems. 
Parallel computing allows for the workload of a program to be run in parallel either on a CPU with multiple threads or through offloading work to the GPU. 
This has the potential of better utilizing system resources to improve performance without upgrading to more expensive SoCs.
GPUs were first introduced to computer architecture to enhance video and 2D/3D graphics processing capabilities of existing desktops/workstations.
Recent innovations and re-purposing of GPUs has led to the rise of GPGPUs which are being frequently used for enhancing the performance of parallel workloads.
Deciding which device is the most optimal for execution is difficult to ascertain, as it is hard to predict the performance on the CPU or GPU.
We introduce a method to predict the best device for program execution using semantically and structurally aware flow graphs augmented with performance counters as inputs to a graph neural network.  
We obtained the flow graphs of individual kernels and augmented them with performance counters to input information about how the flow of a kernel translated to the performance of the kernel into our graph neural network model. 
Our method achieved favorable accuracy results at 87\%.
For the kernels that benefited from using the GPU, we measured a speedup of 1.5x in 60\% of the cases.",https://dr.lib.iastate.edu/handle/20.500.12876/EwpaPQGv,Computer science,"Code Optimization,Graph Neural Networks,OpenMp",OpenMP Device Mapping Prediction using GNNs,thesis
N/A,"Narayanan, Mahesh",2020-08-21T23:08:00.000,"<p>DNA-protein alignment algorithms can be used to discover coding sequences in a genomic sequence, if the corresponding protein derivatives are known. They can also be used to identify potential coding sequences of a newly sequenced genome by using proteins from related species. Previously known algorithms for computing DNA-protein alignments have one or more of the following drawbacks: not taking into account all aspects in problem formulation, providing optimal solutions that are run-time/memory expensive, and sacrificing optimality to achieve practical implementation. In this thesis, we present a comprehensive formulation of the DNA-protein alignment problem including indels, substitutions, frameshift errors, and intronic insertions between and within codons. We then provide an algorithm to compute an optimal alignment in O(mn) time using only four dynamic programming tables of size (m+1)x(n+1), where m and n are the lengths of the DNA and protein sequences, respectively. We developed a Protein and DNA Alignment program (PanDA) that implements the proposed solution. Experimental results indicate that our algorithm provides alignments that accurately reproduce GenBank annotation in nearly all cases when tested on gene and protein sequences from the same organism. We also present experimental evidence that our algorithm produces high-quality alignments and exon-intron predictions when aligning DNA sequences with proteins corresponding to orthologous genes from other species. We also present a parallel software that can be used to annotate, validate, and improve the quality of an assembly of a genome in a large scale. Spliced alignments between DNA sequences of the assembly and protein sequences from other organisms are done to achieve the same. Experimental results indicate that our software can produce putative annotations, while detecting candidate contigs to improve quality of an assembly.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97578,,Computer science,Optimal DNA-protein alignments with application to large-scale genome analysis,thesis
Yan-Bin Jia,"Lin, Huan",2018-08-11T12:59:23.000,"<p>Robot grasping of deformable objects is an under-researched area. The difficulty comes from both mechanics and computation. First, deformation caused by the grasp operations changes object's global geometry. Second, under deformation, an object's contacts with the fingers grow from points into areas. Inside such a contact area, points that stick to the finger may later slide while points that slide may later stick. The torques exerted by the grasping fingers vary, in contrast with rigid body grasping whose torques are invariant under forces.</p>
<p>In this thesis the object's deformation and configuration of contact with fingers and the plane are tracked with finite element method(FEM) in an event-driven manner based on the contact displacements induced by the finger movements.</p>
<p>The first part of the thesis analyzes two-finger squeeze grasping of deformable objects with a focus on two special classes: stable squeezes, which minimize the potential energy of the object among squeezes of the same depth, and pure squeezes, which eliminate all euclidean motions from the resulting deformations. Based on them an algorithm to characterize the best resistance by a grasp to an adversary finger is proposed which minimizes the work done by the grasping fingers. An optimization scheme is offered to handle the general case of frictional segment contact. Simulations and multiple experiments with a Barrett Hand on a rubber foam object are presented.</p>
<p>The second part of this thesis describes a strategy for a two-finger robot hand to grasp and lift a 3D deformable object resting on the plane. Inspired by the human hand grasping, the strategy employs two rounded fingers to squeeze the object until a secure grasp is achieved under contact friction. And then lift it by translating upward to pick up the object. During the squeeze, a lift test is repeatedly conducted until it is successful based on the metrics and then trigger the upward translation. The gravitational force acting on the object is accounted for. Simulation is presented and shows some good promise for the sensorless grasping approach for deformable objects.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27767,"Computer Sciences,Robotics","deformation,displacement-based grasping,finite element analysis,grasp resistance,stable and pure squeezes",Optimal grasping of soft objects with two robotic fingers,thesis
"Luecke, Glenn R,Prabhu, Gurpur,Chaudhuri, Soma,Huang, Xioaqiu,Maris, Pieter,Vary, James P","Weeks, Nathan",N/A,"This dissertation comprises published or accepted papers encompassing two areas in High Performance Computing research: optimization and parallelization of bioinformatics applications, and fault tolerance in parallel Fortran applications.

The bioinformatics application optimization papers examine the computationally-expensive problems of epistasis detection in quantitative-trait genome-wide association studies (GWAS) and sequence alignment sorting.

First, epiSNP, an application for identifying pairwise epistasis (genetic marker interactions), is subject to performance analysis and subsequent algorithmic and data structure optimizations, resulting in a ~12X speedup vs. the original (serial) application. Combined with distributed- and shared-memory techniques for dynamically load balancing pairwise operations across processes, a 38.43X speedup over the original parallel implementation (EPISNPmpi) is achieved on 126 nodes (each with 2 Intel Xeon Phi coprocessors) of the TACC Stampede supercomputer.

For sequence-alignment sorting, optimizations to the popular open-source application SAMtools are described. These include more efficient data structures to reduce memory-management overhead, an improved external sorting implementation that reduces I/O, and the use of OpenMP tasks to better load balance compression, decompression, and sorting. The optimizations resulted in a 5.9X speedup for the benchmarked in-memory sort, and a 1.98X speedup for an external sort.

In the domain of High Performance Computing fault tolerance for parallel Fortran applications, the first paper surveys the landscape of HPC technologies and techniques for developing resilient Fortran applications that are parallelized using the Message Passing Interface (MPI). MPI fault tolerance extensions are categorized and analyzed for Fortran compatibility, and issues pertaining to the use of Fortran I/O and MPI I/O for checkpoint/restart are discussed.

The final paper both proposes changes to the Fortran standard to make its recent facilities for handling failed images (processes) more useful to and usable by application programmers, and introduces a prototype implementation that demonstrates the proposed semantics.",https://dr.lib.iastate.edu/handle/20.500.12876/jrl84a2r,Computer science,"Bioinformatics,Fault Tolerance,Fortran,Message Passing Interface,OpenMP","Optimizing parallel sequence alignment sorting and epistasis detection, and parallel Fortran application resilience",dissertation
"Basu, Samik,Chaudhuri, Soma,Miner, Andrew","Dixon, Michael",N/A,"Model checking is an automatic technique for verifying whether the dynamics of a system
satisfy the properties expressed in temporal logic. This is achieved by algorithmically verifying
that a Kripke structure representation of system dynamics is a model of the temporal logic formula.
Model checking relies on the fact that the Kripke structure representation is complete and well-defined. In other words, it is necessary that the truth valuations of the propositions that describe
the Kripke structure are either true or false.
In this thesis, we consider the problem of verifying system dynamics whose Kripke structure
representation is incomplete; in particular, we focus on Kripke structures where valuations of state-propositions can be true, false or unknown (or indeterminate). Due to this incompleteness, model
checking these structures against a temporal property can lead to true (structure is a model of the
property), false (structure is not a model of the property) or unknown (it cannot be determined
whether or not the structure is a model of the property) results.
Our objective is to find whether there exists some truth assignments to the unknown state-propositions such that the Kripke structure, with the new truth assignments, conforms to the
temporal properties under consideration. Furthermore, we pose the optimization question: what is
the minimal set of unknown propositions whose truth-assignments, if updated to true or false, will
provide such a guarantee? This problem is computationally hard akin to the min-cost satisfiability
problem. We present a strategy that reduces the problem of finding the optimal set of truth
assignments to a search problem in a directed acyclic graph over truth assignments (the solution
space) and then apply branch-and-bound heuristic to search for an optimal solution.",https://dr.lib.iastate.edu/handle/20.500.12876/qzoD410w,Computer science,"Formal Verification,Temporal Logic,Three Valued Logic",Optimum satisfaction of CTL formulae for indeterminate labelled Kripke structures,thesis
N/A,"Kalyanaraman, Anantharaman",2020-08-05T05:05:35.000,"<p>Expressed sequence tags, abbreviated ESTs, are DNA molecules experimentally derived from expressed portions of genes. Clustering of ESTs is essential for gene recognition, understanding important genetic variations such as those resulting in diseases and removing redundancies in gene indices. Currently, the software programs that are mostly widely used for EST clustering are those that are developed for solving the related problem of fragment assembly. Due to the differences in the nature of the problems and the input the fragment assembly programs are not an ideal match for clustering large EST data sets. In this thesis, we present the design and development of a parallel software system that targets large-scale EST clustering. The novel features of our approach include 1) design of space efficient algorithms to keep the space requirement linear in the size of the input data set, 2) a combination of algorithmic techniques to reduce the total work without sacrificing the quality of EST clustering, and 3) use of parallel processing to reduce the run-time and facilitate the clustering of large data sets. Using a combination of these techniques, we report the clustering of 144,870 Arabidopsis ESTs in 9.5 minutes on a 64-processor IBM xSeries cluster with 512 MB memory per processor, a problem that does not execute on 512 MB due to insufficient memory using CAP3, a state-of-the-art fragment assembly sequential software and takes 247 minutes to run when the memory is increased to 1 GB. We also clustered 327,632 rat ESTs in 47 minutes on 64 processors with 512 MB memory per processor.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97485,,Computer science,Parallel clustering of expressed sequence tags,thesis
Ying Cai,"Hu, Zhenbi",2018-08-11T14:49:56.000,"<p>Our research has developed a Parity-based Data Outsourcing (PDO) model. This model outsources a set of raw data by associating it with a set of parity data and then distributing both sets of data among a number of cloud servers that are managed independently by different service providers. Users query the servers for the data of their interest and are allowed to perform both authentication and correction. The former refers to the capability of verifying if the query result they receive is correct (i.e., all data items that satisfy the query condition are received, and every data item received is original from the data owner), whereas the latter, the capability of correcting the corrupted data, if any. Existing techniques all rely on complex cryptographic techniques and require the cloud server to build verification objects. In particular, they support only query authentication, but not error correction. In contrast, our approach enables users to perform both query authentication and error correction, and does so without having to install any additional software on a cloud server, which makes it possible to take advantage of the many cloud data management services available on the market today.</p>
<p>This thesis makes the following contributions. 1) We extend the PDO model, which was originally designed for one-dimensional data, to handle multi-dimensional data. 2) We implement the PDO model, including parity coding, data encoding, data retrieval, query authentication and correction. 3) We evaluate the performance of the PDO model. We compare it with Merkle Hash Tree (MH-tree) and Signature Chain, two existing techniques that support query authentication, in terms of storage, communication, and computation overhead.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29508,Computer Sciences,"Authentication,Correction,Data Outsourcing,Parity-based","Parity-based Data Outsourcing: Extension, Implementation, and Evaluation",thesis
"Zhang, Wensheng,Ceylan, Halil,Prabhu, Gurpur","Gorthy, Venkata Ashish",N/A,"Road distress detection is crucial for road maintenance. Identifying the problem areas comes first prior to applying remediation methods. Latest technological advances in smartphones have eased the efforts of the engineers in the process of survey. This research focuses on utilizing the smartphones to recognize the defects/cracks/distress on the pavements.

In this thesis, we design a computer-vision based mobile application that utilizes a trained object detection model to assist in distress detection. We have explored different object detection algorithms and have made comparison amongst SSD 320, SSD 640 and EfficientDet 320 using the Global Road Damage Detection Challenge 2020 dataset. The model ingested in the application is chosen based on their accuracy and more importantly by their latency (total prediction time) while running on the smartphone. Based on the comparison of models, EfficientDet provided a good mAP of 43.73% and an average latency between 40-80 ms (variable based on speed) with an IOU of 0.5.",https://dr.lib.iastate.edu/handle/20.500.12876/gwW7Jd0w,Computer science,"Classification,Data collection,Deep learning,Object detection,Road Distress Detection,Smartphones",Pavement distress detection using smartphones,thesis
"Zhang, Wensheng,Miao, Chenglin,Biswas, Rana","Zhang, Yu",N/A,"Fault tolerance is a cornerstone of distributed systems, ensuring their uninterrupted operation and resilience in the face of failures. This thesis investigates fault tolerance within distributed key-value storage systems through the lens of the Raft consensus algorithm. The Raft consensus algorithm, recognized for its design simplicity and effectiveness in fault tolerance, stands out as a crucial component in these systems. Yet, its practical application in dynamic environments, marked by variable network and disk latencies, presents intricate challenges that are not fully understood.
This thesis critically examines the impact of network and disk latencies on the operational performance and scalability of Raft-based key-value storage systems. It delves into how these latencies affect the key performance metrics of the consensus process, including leader election speed, client request handling, and data shard redistribution during scaling processes. This exploration is pivotal for the design of resilient distributed storage solutions that can maintain high performance under the influence of inherent latency challenges.
Furthermore, this study assesses the potential of Raft as a comprehensive solution for achieving fault tolerance and scalability in distributed storage services. Through empirical analysis, the research aims to clarify Raft’s ability to support a system’s expansion and manage data efficiently amidst growth, while concurrently ensuring system reliability against node failures and network partitions.
The findings aim to contribute significant empirical insights and practical guidance to the field, helping understand how disk and network configuration can affect the performance of fault-tolerant applications.",https://dr.lib.iastate.edu/handle/20.500.12876/YvkAgnez,Computer science,"Consensus Algorithm,Distributed Systems,Fault Tolerance,Raft",Performance analysis on Raft-based fault-tolerant and scalable applications,thesis
"Radkowski, Rafael,Tian, Jin,Jia, Yan-Bin","Raul, Supriya Vishal",N/A,"Modern virtual reality (VR) head-mounted displays render more than 4 million pixels, and each pixel needs to be rasterized, lit, shaded, and colored. High-resolution displays and increased visual quality of VR content put a high burden on the graphics equipment; thus, only increasingly powerful graphics processing units (GPUs) handle the demand. Foveated rendering (FR) is a technique with the potential to reduce the required processing performance for graphics-intensive VR applications significantly. The technique adapts the rendering quality dynamically and only renders high-quality content at the user's focus point. Previous research proposed promising FR techniques. Research is often focused on the technical implementation of FR. However, it remains inconclusive whether or not it is beneficial in all situations. 

This research aims to investigate the performance gain break-even for a dynamic two-layered FR technique. The research approach is experimental. Three different scenes with changing geometrical or rendering complexity are analyzed on a low-end and a high-end GPU using rotating FR settings. The performance of the FR renderer and a standard renderer is profiled and compared to a theoretical model. The results indicate that a low-end mobile GPU benefits with FR implementation when the scene demands high occupancy with >121,000 vertices, >=3  lights, and until a high-resolution circular region around the gaze point covers <=30% of the scene. On the other hand, a high-end desktop GPU benefits with FR implementation when the scene is vast with >900,000  vertices, illuminated by >=15 lights, and enabled with shadows while covering 10% of the scene with the high-resolution foveal region. The results meet the theoretical expectations and indicate that FR's performance gain is majorly affected by the number of threads through the graphics pipeline. In a nutshell, factors such as the scene characteristics, the order of complexity of the shading algorithms, graphics hardware capacity, and foveal region features need monitoring to maximize FR's usefulness.",https://dr.lib.iastate.edu/handle/20.500.12876/GvqXPALw,Computer science,"Dynamic Foveated Rendering,Experimental Investiagtion,Performance Gain,Virtual Reality",Performance-gain investigation of dynamic foveated rendering technique for virtual reality applications,thesis
N/A,"Jennings, Steven",2018-08-16T09:31:23.000,"<p>The computer systems of the eighties are expected to be designed using powerful low-cost distributed parts to achieve increases in computing power and concurrency. The resulting complexity due to the interaction and communication between these parts requires new methods for the analysis of the behavior of these systems. One such class of architectures, based on the concept of data flow, is designed to exploit the inherent parallelism within a program. In these computers, traditional sequencing constraints are removed and an operation is enabled for execution as soon as its operands are available;A method for approximating the time required to execute a data flow program (assuming adequate computing resources) is described. This method is applied to the static program graph at compile time and yields a parameterized equation for execution time performance. Based on a Petri net analysis and combined with more traditional approaches, this method is recursively applied to abstract operations in the program graph using a top-down approach. While this approach may introduce approximations at each stage, the major benefit is a significant reduction over other techniques in the time required for the analysis.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/80026,Computer Sciences,Computer science,Petri net models of program execution in data flow environments,dissertation
"Fernandez-Baca, David,Slutzki, Giora,Martin, Ryan,Aduri, Pavan,Eulenstein, Oliver","Parvini, Ghazaleh",N/A,"Suppose we aim to build a phylogeny for a set of taxa X using information from a collection of loci, where each locus offers information for only a fraction of the taxa.  The question is whether, based solely on the pattern of data availability, called a taxon coverage pattern, one can determine if the data suffices to construct a reliable phylogeny.   The decisiveness problem expresses this question combinatorially.  Informally, a taxon coverage pattern is decisive if the following holds for any binary phylogenetic tree T for X: the collection of phylogenetic trees obtained by restricting T to the subset of X covered by each locus uniquely determines T.  The decisiveness problem is the problem of determining whether a given coverage pattern is decisive.  Here we relate the decisiveness problem to a hypergraph coloring problem.  We use this connection to (1) show that the decisiveness problem is co-NP complete, (2) obtain lower bounds on the amount of coverage needed to achieve decisiveness, (3) devise an exact algorithm for decisiveness, (4) develop problem reduction rules, and use them to obtain efficient algorithms for inputs with few loci, (5) apply local search and devise a deterministic and a randomized algorithm for decisiveness(6) devise Boolean satisfiability (SAT) and integer linear programming formulations (ILP) of decisiveness, which allow us to analyze data sets that arise in practice.  For data sets that are not decisive, we use our SAT and ILP formulations to obtain decisive subsets of the data.",https://dr.lib.iastate.edu/handle/20.500.12876/nrQBO40z,"Computer science,Bioinformatics","Algorithms,Graph Coloring,Local Search,No-rainbow Hypergraph Coloring,Phylogenetic Tree,Taxon Coverage",Phylogenetic decisiveness and no-rainbow hypergraph coloring,dissertation
"Eulenstein, Oliver,Anderson, Tavis K,Li, Qi","Grover, Siddhant",N/A,"Understanding viral diversity dynamics is crucial for unraveling evolutionary patterns, tracing transmission routes, and devising targeted intervention strategies. Diversity indices can offer profound insights into viral ecosystems' underlying structure and dynamics by quantifying the evolutionary relatedness among viral taxa within communities. This thesis emphasizes two such indices:- Phylogenetic Diversity (PD) and Fair Proportion (FP). PD is the minimum total length of the branches in a phylogeny required to cover a specified set of its taxa, whereas (FP) quantifies the sum of all the edge lengths from the root of the phylogeny to the taxon of interest divided by the number of descendants on the edge. A general goal in the application of PD has been identifying taxa that maximize PD on a given phylogeny; this has been mirrored in active research to develop efficient algorithms for the problem. However, there has been limited or no research on computing other statistics(minimum, average, etc.), especially when required for each clade in a phylogeny, enabling direct comparisons of PD between clades. This thesis introduces efficient algorithms for computing PD and the associated descriptive
statistics- minimum, maximum, average, and variance for a given phylogeny on each of its clades. These statistics unveil the richness and distribution of viral lineages and provide invaluable insight into
the distribution of PD across a phylogeny. By applying these sophisticated computational algorithms,  we can effectively measure PD and FP in swine IAV. I further present our hypothesis on whether the rapid expansion in the genetic diversity of clades of IAV in swine represents a zoonotic risk. We assess the FP index's and PD statistics' utility in developing an objective ranking of the likelihood that a swine IAV strain or clade has zoonotic potential.",https://dr.lib.iastate.edu/handle/20.500.12876/azJ4mdqv,Computer science,"Algorithms,Classification Models,Dynamic Programming,Phylogenetics",Phylogenetic diversity measures as a proxy for zoonotic risk in Influenza A virus,thesis
"Eulenstein, Oliver,Miller, Les,Anderson, Tavis,Gauger, Phillip,Huang, Xiaoqiu","Mubaraki, Fathi",N/A,"Phylogenetic networks become a significant scheme for enhancing our understanding of how different groups of species associate with each other, which provides substantial knowledge about the relationship among species. RNA virus diversity is the result of mutation, recombination, and reassortment. Traditional virus evolution studies rely on single-gene phylogenetic trees that do not account for all these processes. Phylogenetic network algorithms can do so but are hampered by computational limitations. We introduce an efficient software package with a graphical user interface called PhyloVirus that constructs and allows visualization of phylogenetic trees, median trees, and networks. We apply our software to swine influenza A virus (IAV), quantifying reassortment events in the evolution of H3N2 viruses and identify novel reassorted viruses. We developed PhyloVirus, a multi-platform Java application which accepts a set of Newick-format gene trees and constructs a species tree using our Robinson-Foulds Network (RF-Net) algorithm. Phylogenetic networks are visualized by displaying a phylogeny string in the extended Newick format and drawing the network with k reticulation events. We apply RF-Net to a swine IAV H3N2 dataset, estimating a reassortment network with over 500 strains.
We apply a usability study to confirm that the PhyloVirus system meets potential users' expectations and investigates the participant's behavior and preferences during the study. Involving users in the early stage of improving systems can lead to more valuable and helpful features and functions. Our usability study has been undertaken with biologists from different groups and different laborites to identify any usability issues, gather quantitative and qualitative data, and determine the participant's fulfillment with PhyloVirus software.",https://dr.lib.iastate.edu/handle/20.500.12876/1wgeNVjr,Computer science,"Hypothesis,Phylogenetic Networks,Phylogenetic Tree,Research Questions,Usability Testing,Virus Reassortment","PhyloVirus: Inferring virus reassortment, and a visualization tool for phylogenetic networks  (Development and Usability Study)",dissertation
"Bhattacharya, Sourabh,Jia, Yan-bin,Oliver, James,Song, Guang,Wongpiromsarn, Tichakorn","Gao, Tianshuang",N/A,"Limited on-board power poses a significant challenge in field deployment of mobile robots for persistent and long-term autonomy. Refuel scheduling is a class of vehicle routing problems that arise in autonomous recharging of mobile robots. In this work, we consider the refuel scheduling problem for a team of mobile robots using a fleet of aerial vehicles. The contributions of this work are as follows. (1) We present a quadratic-time deconfliction algorithm for planning the motion of the mobile robots on aisle graphs. (2) We present a non-cooperative charger-station allocation strategy based on game-theoretic techniques, and present bounds on the efficiency of the equilibrium. (3) We propose schedules for the chargers to serve the robots, and derive scheduling parameters for persistent operation.",https://dr.lib.iastate.edu/handle/20.500.12876/aw4NgD5r,Robotics,"Muti-robot System,Path Planning,Robot Recharging","Planning, scheduling and coordination for refuel scheduling on aisle graphs",dissertation
"Joseph Zambreno,Leslie Miller","Krishna, Amar",2018-08-11T16:02:05.000,"<p>For the past several years YouTube has been by far the largest user-driven online video provider. While many ofthese videos contain a significant number of user comments, little work has been done to date in extracting trends from these comments because of their low information consistency and quality. In this paper we perform sentiment analysis of the YouTube comments related to popular topics using machine learning techniques. We demonstrate that an analysis of the sentiments to identify their trends, seasonality and forecasts can provide a clear picture of the influence of real-world events on user sentiments.Results show that the trends in users' sentiments are well correlated to the real-world events associated with therespective keywords.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27857,Computer Sciences,"Forecasting,Polarity,Sentiment,Trends,Weka,YouTube",Polarity trend analysis of public sentiment on YouTube,thesis
"Lathrop, James I,Lutz, Jack H,Aduri, Pavan R,Miner, Andrew S,Weber, Eric","Nye, Dawn",N/A,"Recent research in analog computing has relied heavily on the ability to create and initialize a chemical reaction network (CRN) precisely as specified. For example, Huang, Klinge, Lathrop, Li, and Lutz defined a notion of computing real numbers in real-time with chemical reaction networks (CRNs) that is fragile in the sense that any small perturbation from its intended construction will cause it to fail. In practice, it is unrealistic to assume the infinite precision such requires. Many theorems are thus proven about the CRN model rather than the reality the model is intended to reflect. Here, we require a CRN to withstand these perturbations and explore the consequences of this restriction. In doing so, we arrive at a paradoxically discrete model of analog computation. This approach has several advantages. First, a CRN may compute values in finite time. Second, a CRN may tolerate error in its construction. Third, a measurement of a CRN's state only requires precision proportional to the exactness of the approximations. Lastly, if a CRN requires only finite memory, this model and Turing machines are equivalent under real-time simulation.

Further, we show a key limitation of a CRN's ability to compute even with exact values. We prove that no CRN, given an arbitrary real value α, can transform α into its binary expansion. This is true both exactly and in approximation of either input or output. It can, however, convert a measure one subset of [0,1) (and hence almost all reals) that is totally disconnected in the limit as permitted error goes to zero.

Lastly, we present a generalized version of the weighted discrete Fourier transform. In particular, we provide an orthonormal basis for the underlying product space and give the weighted discrete Fourier transform and the usual tools of Fourier analysis in terms of it. Using this generalization, we state a new characterization of algorithmic randomness. The definition we provide is a derivative of the martingale definition provided by Schnorr in terms of submartingales (martingales which are subadditive). We show that a sequence S is random if, as with a martingale, a broader class of well behaved functions cannot succeed on S. By well behaved, we mean that the Fourier coefficients we extract from such a function are uniformly bounded and, for the moment, require that they be nonnegative. As a ready derivative of this new characterization, we also provide the analogous definition of Hausdorff dimension to demonstrate its wide applicability.",https://dr.lib.iastate.edu/handle/20.500.12876/nrQBLmaz,"Computer science,Mathematics","Analog-to-Digital Converter,Chemical Reaction Network,Dimension,Fourier Transform,Martingale,Randomness",Practical computation with chemical reaction networks and algorithmic randomness,dissertation
N/A,"Kim, Kihwan",2020-11-09T01:15:59.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/98032,,,Priority forwarding: An efficient flooding technique for wireless ad hoc networks,thesis
"Zhang, Wensheng,Cai, Ying,Mitra, Simanta","Muhr, Trent",N/A,"With federated learning, local learners train a shared global model using their own data, and report model updates to a server to aggregate and then update the global model. Such a learning paradigm may suffer from two attacks: privacy attacks by the untrusted server; adversarial attacks (e.g., poisoning attacks) by malicious learners. There is extensive research on addressing each of the attacks separately, but there is no scheme that can address both of them. In this paper, we propose a scheme that enables both privacy-preserving aggregation and poisoning attack detection at the server, by utilizing additive homomorphic encryption and a trusted execution environment (TEE). Our evaluation based on an implemented prototype system demonstrates that our scheme can attain a similar level of detection accuracy as the state-of-the-art poisoning detection scheme, and that the increased computational workload can be parallelized and mostly executed outside of the TEE. A privacy analysis shows that the proposed scheme can protect individual learners' model updates from being exposed.",https://dr.lib.iastate.edu/handle/20.500.12876/Dw88mGkw,Computer science,"Applied Cryptography,Federated Learning,Poisoning Attacks,Privacy Attacks,Trusted Execution Environment",Privacy-preserving detection of poisoning attacks in federated learning,thesis
"Aduri, Pavan,Lutz, Jack,Chaudhuri, Soma,Lidicky, Bernard,Fernandez-Baca, David","Dixon, Peter",N/A,"Random algorithms have a unique place in complexity theory as a model of computation that ispotentially more powerful than “normal” algorithms, and is also practical.  However, it is still notclear how much more power randomness adds.  The primary goal in studying random algorithmsis derandomization – some method to simulate random algorithms without actually using random-ness.  While full derandomization is quite difficult, we show some weak derandomization results –one using advice, and one using multi-pseudodeterminism.  We show that improving these resultswould have major implications.  Finally, we show new containments and oracle separations betweentraditional random classes and zero-knowledge proofs.",https://dr.lib.iastate.edu/handle/20.500.12876/9z0K1YQr,Computer science,"Cryptography,Pseudorandomness,Randomized algorithms,Theoretical computer science",Probabilistic computations:  Mild derandomizatons and zero-knowledge classes,dissertation
"Basu, Samik,Cohen, Myra,Tian, Jin","Pal, Sanchayani",N/A,"Program verification, in general, is undecidable. In the recent years, SMT/SAT solver
based proof assistants and theorem provers have been deployed to prove desirable (e.g.,
necessary pre-/post-conditions of functions) properties of programs in a semi-automatic
fashion relying on some expert knowledge and guidance. In this context, we investigate the
viability of Software Analysis Workbench (SAW) infrastructure and Daikon invariant
detector tool in verifying properties of programs. Our main objective is to deploy Daikon to
generate relevant invariants of programs automatically, and then utilize those invariants to
develop functional specifications of programs. We conjecture that such specifications can be
automatically translated into executable languages (such as Cryptol) and can be effectively
utilized to prove desired properties of programs. This is likely to minimize expert
knowledge necessary to develop program specifications, thus improving the applicability of
proof assistants and theorem provers in effectively checking the properties of programs.",https://dr.lib.iastate.edu/handle/20.500.12876/EzR2lj7z,Computer science,"Cryptol,Daikon,Program,SAW,Splitter,Verification",Program verification using dynamic invariants and theorem prover: Exploratory study on Daikon and SAW,thesis
"Lutz, Jack H,Lathrop, James I,Gilbert, Stephen B,Lutz, Robyn R,Basu, Samik","Potter, Hugh",N/A,"Molecular programming is an exciting field that combines techniques from biology, chemistry, and computer science to achieve remarkable results at the molecular scale.  Most conventional engineering techniques do not apply apply at nanoscale; to create nanoscale structures, we must often program them to assemble themselves.
We identify two challenges in this domain.

First, design complexity: the molecular formalisms that we use to design nanoscale systems are powerful but complex. 
 What abstractions can we create to access this power in a way that is straightforward for humans to understand and reason about?  We present ALCH, an imperative language that compiles into the chemical reaction network-controlled tile assembly model.

Second, verification: if we want to use nanotechnological devices in industry and medicine, we must be able to prove that they are safe, and function as expected.  In this work, we focus on verification via machine-checkable mathematical proofs; we discuss two efforts to apply this technique to chemical reaction networks (CRNs).  In one effort, we incorporate machine-checked proofs into a larger verification process to augment model checking results.  In a subsequent effort, we encode CRN semantics from the ground up in a machine-checkable proof language and fully verify a rateless CRN with a population-induced phase transition.  We attempt to apply a variety of other verification techniques to this CRN to address the larger question of what role machine-checked proofs have to play in molecular verification.",https://dr.lib.iastate.edu/handle/20.500.12876/jw270xxv,Computer science,"molecular programming,nanotechnology,verification",Programs and proofs: A molecular toolchain,dissertation
N/A,"Nguyen, Long",2018-08-15T13:43:19.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/78178,Computer Sciences,Computer science,Projection-based methods for solving systems of n nonlinear equations in n unknowns,dissertation
"Jernigan, Robert,Song, Guang,Gao, Hongyang,Quinn, Christopher,Li, Qi","Sayeed, Suri Dipannita",N/A,"Protein fold classification reveals key structural information about proteins that is essential for understanding their function. While numerous approaches exist in the literature that classifies protein fold from sequence data using machine learning, there is hardly any approach that classifies protein fold from the secondary or tertiary structure data using deep learning. This work proposes a novel protein fold classification technique based on graph neural network and protein topology graphs. Protein topology graphs are constructed according to definitions in the Protein Topology Graph Library from protein secondary structure level data and their contacts. To the best of our knowledge, this is the first approach that applies graph neural network for protein fold classification. We analyze the SCOPe 2.07 data set, a manually and computationally curated database that classifies known protein structures into their taxonomic hierarchy and provides predefined labels for a certain number of entries from the Protein Data Bank. We also analyze the latest version of the CATH data set. Experimental results show that the classification accuracy is at around 86% − 100% under certain settings. Due to the rapid growth of structural data, automating the structure classification process with high accuracy using structural data is much needed in the field. This work introduces a new paradigm of protein fold classification that meets this need. The implementation of the model for protein fold classification and the datasets are available here https://github.com/SuriDipannitaSayeed/ProteinFoldClassification.git",https://dr.lib.iastate.edu/handle/20.500.12876/6wBlE3yr,"Computer science,Bioinformatics","Graph Neural Network (GNN),Protein Data Bank (PDB),Protein fold,Protein structure Classification,Protein Topology Graph Library (PTGL),Structural Classification of Proteins (SCOP)",Protein fold classification using Graph Neural Network and Protein Topology Graph,thesis
"Wensheng Zhang,Johnny S. Wong,Daji Qiao","Panchapakesan, Santosh",2018-08-22T18:32:07.000,"<p>Wireless sensor networks are data centric because in many applications, sensor nodes are required to generate data, collect data, storage data and process data queries. Meanwhile, wireless sensor networks are vulnerable to security attacks because they are deployed in unattended (often hostile) environments and do not have tamper resistant hardware. Therefore, secure and efficient data management schemes are necessary to sensor networks. In this thesis work, we study how to secure a representative type of sensor data management approach called data centric storage based (DCS) schemes, with focus on protecting data confidentiality and integrity.;Considerable efforts have been made for securing DCS, however, existing work has the limitations of (i) not considering user node compromise, (ii) lack of studies on real system implementation and detailed experiments, and (iii) lack of studies on integrating security schemes to defend against multiple attacks simultaneously. To overcome these limitations, we have conducted the following research: Firstly, we have designed a new data confidentiality protocol called DKVP (data and key vulnerability protection) scheme to protect sensor data confidentiality in presence of user node compromise. Secondly, we have implemented three polynomial-based sensor data confidentiality and integrity protection schemes, namely, the adaptive polynomial-based scheme for secure data storage and query (APB), the message authentication function based schemes for data integrity (MAF), and the DKVP scheme, on top of TinyOS/Mote platform. Thirdly, we have developed a prototype system that consists of (i) integrated data confidentiality and integrity protection modules (i.e., the APB, MAF and DKVP schemes), (ii) effective and friendly interfaces to application developers to facilitate inclusion of security features into application programs, and (iii) example programs to demonstrate the integration suite developed by us.;Extensive experiments have been conducted to study the feasibility and performance of the above designs and implementations. The results show that, if system parameters are properly chosen, desired security level can be achieved which is cost affordable by the current generation of sensor nodes such as MICA motes. In particular, our study shows that running the three integrated protocols together consumes only 27 msec of processing time and 60% of CPU usage.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/69048,Computer Sciences,Computer science;,"Protocol design, implementation and integration for the protection of sensor data confidentiality and integrity",thesis
"Li, Qi,Eulenstein, Oliver,Gao, Hongyang,Aduri, Pavankumar R,Zhang, Wensheng","Kulkarni, Adithya",N/A,"Hiring domain experts to annotate a corpus is costly in terms of money and time. Alternative to domain experts, crowd workers or readily available pre-trained language models can be employed to obtain annotations with considerably less time and money. However, the crowd workers are non-experts resulting in noisier annotations than experts. Furthermore, the annotation task may be complex and the crowd workers may not have sufficient knowledge required for the task. Moreover, pre-trained language models can also provide noisier annotations and these models may have similar architectures or be trained on similar datasets, resulting in correlations in the provided annotations. 

To tackle these challenges, we propose to detect the true information from conflicting and noisier information provided by heterogeneous sources by modeling source reliability and complex dependencies in data aggregation. Intuitively, the inferred aggregation results will likely be accurate if supported by many sources and follow the data dependencies. If a source provides many truths, then the source is more reliable. Existing literature in annotation aggregation assumes that workers are independent and instances are independent and, thus, cannot handle complex data dependencies and worker correlations. In this dissertation, we design truth discovery approaches to handle aggregation challenges for complex tasks such as sequential labeling, constituency parse tree, and dependency parse tree. Effective optimization-based strategies and strategies employing probabilistic models are proposed, and their performance is evaluated on real-world applications.

Truth discovery approaches are employed as a post-processing step and require atleast one annotation for all instances in the corpus. When the budget is insufficient to obtain annotations for all instances in the corpus and pre-trained language models are unavailable for the task, we can design better mechanisms to obtain annotations from crowd workers. Specifically, the unlabeled samples in the corpus can have label dependencies, and exploiting this label dependency during crowd annotations can improve data labeling quality within the given budget. This dissertation proposes two optimization-based strategies that exploit label dependencies to achieve good classification performance within the given budget on a graph of instances. The first strategy assumes all the connected instances in the graph have the same label dependencies, and the second strategy relaxes this assumption and estimates the label dependencies between connected instances in the graph. The proposed methods help requesters manage the provided budget wisely by choosing influential instances to obtain crowd annotations.",https://dr.lib.iastate.edu/handle/20.500.12876/JwjbLqEw,Computer science,"Budget Allocation,Crowdsourcing,Machine Learning,Natural Language Processing",Quality aware annotations with limited budget,dissertation
"Johnny Wong,Wallapak Tavanapong","Putthividhya, Wanida",2018-08-25T00:36:03.000,"<p>This dissertation studied issues pertaining to QoS provision for multimedia applications at the application layer. We initially studied Internet routing pathology and Internet routing stability by repeating experimental and analytical methods conducted by Paxson in 1996. No similar study was done in recent years. Our findings show that routing behavior of the Internet in 2006 are different from those reported in 1996 in some important aspects. Second, we investigated different stochastic models (e.g. self-similar processes, Auto-Regressive Integrated Moving-Average (ARIMA)) in order to find a suitable model that describes available bandwidth over time of an end-to-end path between two Internet hosts. Our finding of the suitable model is beneficial to predicting of future values of available bandwidth along an end-to-end path. To the best of our knowledge, no similar study was conducted. Third, we designed and evaluated a new path monitoring algorithm inferring available bandwidth of an end-to-end path without monitoring all the paths to minimize monitoring overhead. Our algorithm does not rely on underlying network-layer topology information as required in topology-aware path monitoring techniques. Finally, to complement the above study, we introduced our multicast protocol named ""core-set routing"" for transmitting multimedia data from a set of senders to a set of receivers, taking QoS into account. The protocol is suitable for interactive multi-sender multimedia applications such as video conferencing and network gaming.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/72650,Computer Sciences,Computer Science,Quality of service (QoS) support for multimedia applications in large-scale networks,dissertation
"Tavanapong, Wallapak,Peterson, David A. M.,Sukul, Adisak,Tian, Jin,Wong, Johnny S.","Qi, Lei",N/A,"Quantification learning has been explored in several disciplines, resulting in different terminologies such as prior probability shift, prevalence estimation, or class ratio estimation. In many subject domains, it is important to estimate and track prevalence over time, for instance, tracking of the prevalence of diseases, customer complaint issues, trending topics in customer demands, support for political candidates in blog posts, and popular policy areas of interest to federal and state governments. Quantification learning is a task that “Given a labeled training set, induce a quantifier that takes an unlabeled test set as input and returns its best estimate of the class distribution.” However, quantification learning has received relatively far less attention compared to classification learning in computer science. The main reason is because of the mistaken belief that quantification is easily solved using a straightforward approach by classifying individual documents and calculating the ratio of instances in each predicted class to the total instances. Although both quantification and classification are in the family of supervised learning, there is a significant difference between the two learning tasks. Quantification learning does not require training data and test data to be independent and identically distributed. Unlike a classifier, a quantifier makes a prediction for a group of instances, not a single instance. 
The main contributions of the dissertation are summarized as follows. 
1) We propose a new data augmentation method called Context-Aware-AUG to reduce quantification errors of classification-based quantification learning methods. A fair classifier gives a better estimate of class ratios. To obtain a fair classifier, Context-Aware-AUG generates additional training data for the classes with much fewer instances. The generated text documents are readable by humans, which is achieved by keeping the order of the words as in the original documents and replacing important words with their most similar words using knowledge from three external data sources. 
2) We address an under-explored problem of quantification learning by proposing the first end-to-end Deep Quantification Neural Network (DQN) framework. DQN jointly learns effective feature representations and class distribution estimates. We formulate the quantification learning problem as a maximum likelihood problem. DQN can be seen as learning a mapping function of the input, a set of instances, and the output is the class ratios. We introduce two strategies to select a set of instances (termed a tuplet) for training to investigate how well the induced quantifier generalizes to test sets with class distributions different from that of the training set. We present the effectiveness of DQN by evaluating quantification tasks of text documents on four public datasets with 2, 4, and 20 classes. We performed a sensitivity analysis of DQN performance by varying tuplet sizes and training dataset sizes. Compared to that of classification-based quantification learning, DQN performance is less impacted by the size of the training dataset. In other words, DQN is a promising method when the manual labeling budget is limited. 
3) We evaluate whether DQN is also effective for binary quantification and multi-class quantification for other types of data such as political text documents and image data, ranging from nature images, medical images, animal images, and agricultural images. DQN achieves the best results on almost all datasets. Compared to the best existing method in our study, DQN reduces the mean absolute error by 55% on two datasets of political documents and 38.34% on six image datasets. We found that transfer learning also helps improving performance of classification-based quantification methods.",https://dr.lib.iastate.edu/handle/20.500.12876/aw4NgK7r,Computer science,"Data augmentation,Deep quantification neural network,Neural networks,Quantification error,Quantification learning",Quantification learning with deep neural networks,dissertation
"Wongpiromsarn, Tichakorn,Basu, Samik,Lutz, Robyn","Islam, Md Rayhanul",N/A,"The safety of autonomous systems (vehicles) largely depends on their ability to synthesize policies in a dynamic and uncertain environment. Traditional methods for policy synthesis use formal verification techniques by relying on precise models (such as Markov Chains or Markov Decision Processes) of other agents within the system. However, in the real-world scenario, uncertainty exists in the transition behavior of different agents, which makes it challenging to construct accurate models. This leads to situations of partial observability due to the agent's inability to observe the actual transition dynamics of other agents or environments. Therefore, in the presence of this type of partial observability, agents may build different models of the same agent. Research shows that inconsistencies in the perception of agents regarding other agents' models could make the overall system unsafe.

This research presents an approach to quantify the safety of the overall system in the partially observable environment among interacting heterogeneous agents. Agents consider the behavior of the environment as a set of Markov chains, and each Markov chain describes the possible behavior of the environment or an agent. In order to synthesize policies, the agent constructs a belief environment model to construct the complete system. The findings show that constructing policies without considering the inconsistencies among other agents may increase the probability of making the complete system unsafe. Finally, this approach is run on the autonomous vehicle simulator CARLA to support our findings.",https://dr.lib.iastate.edu/handle/20.500.12876/dvmq9ZRv,Computer science,"Autonomous Vehicles,Formal Methods,Formal Verificaiton,Probabilistic Verification,Safety of Autonomous Vehicles",Quantifying safety in the partially observable environment: application to autonomous vehicles,thesis
"Tavanapong, Wallapak,Dickerson, Julie,Li, Qi","Song, Seok Hwan",N/A,"Quantitative reasoning capabilities of Large Language Models (LLMs) have improved enormously in recent years. The improvements have been shown through solving Math Word Problems (MWPs) and Chart Question Answering. However, real-world data often contains irrelevant information. Ideally, LLM should also perform accurately when given irrelevant information (i.e., noise) to derive answers to a user’s question. To assess LLM’s quantitative reasoning ability on noisy data, we propose a new dataset, Math Word Problems with Noises (MPN). This dataset has three types of noises added to MWP problems selected from four public datasets. We propose a new solution, Noise Reduction Prompting (NRP) and its variant (NRP+), to handle noisy data. We evaluated LLM’s quantitative reasoning ability on MPN and two datasets with noises: GSM-IC and PlotQA. Some key findings are as follows. Both ChatGPT (gpt-3.5-turbo) and PaLM2 have a significant drop in absolute accuracy on MPN compared to the same MWP problems without noises. The state-of-the-art methods, namely, Chain of Thought Prompting, Least-To-Most Prompting, Progressive-Hint Prompting, and Program-aided Language Models have an average accuracy drop of 14.4%, 12.2%, 17.2%, and 24.1%, respectively, while our NRP and NRP+ limit a drop in average accuracy to only around 1.9% and 5.1%, respectively. NRP+ performs the best on MPN without noises. Compared to GSM-IC, our types of noise are more difficult. NRP+ outperforms existing prompt methods, such as CoT and LTM by 2.7% and 11.2% on average.",https://dr.lib.iastate.edu/handle/20.500.12876/jrl8xPOr,"Computer science,Artificial intelligence","Artificial Intelligence,Natural Language Processing,Prompt Engineering,Question Answering",Quantitative reasoning ability of large language models under noisy data,thesis
N/A,"Pan, Chunrong",2020-11-09T18:52:39.000,"<p>XML is becoming the de facto standard for data exchange over the Internet. In particular, many XML databases contain a large set of small XML documents with similar structure. It creates a new requirement to store and retrieve information from these XML documents efficiently. XML-based bibliography file is such a large set of XML documents with similar structure. The problem we are concerned is to build index and support query for this kind of XML documents in a fast and effective way. Since each document is small, the query result is not necessarily a tree or subtree of XML document. We can return the document ID for each query in order to retrieve the entire document. In order to solve this problem, we propose to store a set of XML documents into B+ tree inverted files and query the information based on B+ tree structure. This project uses XML as a bridge and combines the database and information retrieval into one application with supporting storage, indexing and querying. It allows creating index based on the entire path or keywords so that it could retrieve the document ID for keyword query and path query. Based on the assumption that the document is small, this project can deal with the queries with simple path structure or terms very efficiently. We have constructed DBLP B+ tree, DBLP Author B+ tree and DBLP Title B+ trees. All these B+ trees provide the user with a good opportunity to search and retrieve the information from a large xml-based bibliography database very efficiently. We have conducted many experiments to test the performance of construction time for each B+ tree and to observe the query efficiency.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/98185,,Computer science,"Query, indexing and benchmark for XML-based bibliography databases",thesis
"Lutz, Jack H,Aduri, Pavan,Miner, Andrew,Lathrop, James,Ciardo, Gianfranco,Smith, Arthur","Migunov, Andrei Nikolai",N/A,"In this thesis, we study the notion of algorithmic randomness and its refinement, algorithmic dimension, in the contexts of learning theory and analog computation. Zaffora Blando (2021) recently characterized algorithmic randomness in terms of computable learning functions. We extend this work to characterize the Hausdorff and computable dimensions in terms of learning functions, and we give an upper bound on the algorithmic dimension. Having worked with randomness and dimension in the traditional setting, we move on to study randomness in the analog framework. We discuss a specific form of analog computation known as stochastic chemical reaction networks (SCRNs) and review the theory of randomness due to Huang et al. (2019) that obtains for this class of analog computers. We use these tools to discuss randomness properties of sequences of real numbers that represent the sojourn times between chemical system state transitions. Finally, we discuss an analog version of the classical randomized complexity class BPP.",https://dr.lib.iastate.edu/handle/20.500.12876/erLKZqMv,"Computer science,Mathematics,Logic","algorithmic dimension,analog computation,chemical reaction networks,computational learning theory,continuous time Markov chain",Randomness and dimension in computational learning and analog computation,dissertation
Carl K. Chang,"Feng, Yunfei",2019-09-14T06:02:16.000,"<p>Activities of daily living (ADL) are things we normally do in daily living, including any daily activity such as feeding ourselves, bathing, dressing, grooming, work, homemaking, and leisure. The ability or inability to perform ADLs can be used as a very practical measure of human capability in many types of disorder and disability. Oftentimes in a health care facility, with the help of observations by nurses and self-reporting by residents, professional staff manually collect ADL data and enter data into the system.</p>
<p>Technologies in smart homes can provide some solutions to detecting and monitoring a resident’s ADL. Typically multiple sensors can be deployed, such as surveillance cameras in the smart home environment, and contacted sensors affixed to the resident’s body. Note that the traditional technologies incur costly and laborious sensor deployment, and cause uncomfortable feeling of contacted sensors with increased inconvenience.</p>
<p>This work presents a novel system facilitated via mobile devices to collect and analyze mobile data pertaining to the human users’ ADL. By employing only one smart phone, this system, named ADL recognition system, significantly reduces set-up costs and saves manpower.</p>
<p>It encapsulates rather sophisticated technologies under the hood, such as an agent-based information management platform integrating both the mobile end and the cloud, observer patterns and a time-series based motion analysis mechanism over sensory data. As a single-point deployment system, ADL recognition system provides further benefits that enable the replay of users’ daily ADL routines, in addition to the timely assessment of their life habits.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31317,Computer Sciences,"Activity of daily living,Auditory analysis,Behaviour recognition,Mobile data analysis,Smart phone",Recognition of activities of daily living,dissertation
"Sarkar, Soumik,Tian, Jin,Basu, Samik","Gangopadhyay, Tryambak",N/A,"In many cyber-physical systems, imaging can be an important but expensive or `difficult to deploy' sensing modality. One such example is detecting combustion instability using flame images where deep learning frameworks have demonstrated high performance. The proposed frameworks are shown to be quite trustworthy such that the domain experts can have sufficient confidence to use these models in real systems to prevent unwanted incidents. However, flame imaging is not a common sensing modality in combustion engines today. Therefore, the current roadblock exists on the hardware side regarding the acquisition and processing of high-volume flame images. On the other hand, the acoustic pressure time series is a more feasible modality for data collection in real combustors. Therefore, the optimal solution can be to utilize acoustic time series as a sensing modality and, at the same time, to implement the image-based deep learning frameworks for more efficient detection of combustion instability. To achieve this, we propose a novel model that can reconstruct cross-modal visual features from acoustic pressure time series in combustion systems. The results demonstrate that the detection accuracy can be enhanced using only time series data by using our approach. Our proposed approach provides the unique benefit of generating synthetic visual features corresponding to time series information. With our proposed model, we anticipate that deep learning based instability detection frameworks will become more feasible by avoiding the use of imaging which is an expensive sensing modality. By deploying such frameworks on a real system, instability can be detected effectively, preventing revenue loss from unwanted incidents (e.g., flame blowout and structural damage to the engines). By providing the benefit of cross-modal reconstruction, this model can prove useful in different domains well beyond the power generation and transportation industries.",https://dr.lib.iastate.edu/handle/20.500.12876/GvqXxZgw,Computer science,"Combustion Instability,Cross-Modal Reconstruction,Deep Learning,Time Series",Reconstruction of cross-modal visual features from acoustic pressure time series in combustion systems,thesis
"Quinn, Chris J,Jannesari, Ali,Huang, Xiaoqiu","Hu, Zefu",N/A,"Various code intelligence tasks have made massive progress in terms of performance due to the advance of code language models. 
However, the research on the interpretability of these models is left far behind, bottlenecking the reliability and efficiency of the inference. 
One obstacle is redundant layers and neurons. Identifying redundant neurons helps better understand the models and guides research on compact and efficient models. Focusing on the token tagging task and seven (code) models: BERT, RoBERTa(RBa), CodeBERT(CB), GraphCodeBERT(GCB), UniXCoder(UC), CodeGPT-Python(CGP), and CodeGPT-Java(CGJ), we leverage redundancy and concept analyses to perform layer- and fine-grained neuron-level analyses to show that not all the layers/neurons are necessary to encode those concepts. 
In the analysis, we study how much general and task-specific redundancy the models exhibit at the layer and more fine-grained neuron levels. We find that over 95\% neurons are redundant, and removing these neurons would not have a serious negative impact on the accuracy. We also identify several compositions of neurons that can make predictions with the same accuracy as the entire network. Through concept analysis, we explore the traceability and distribution of human-recognizable concepts. We determine neurons that respond to specific code properties; for example, neurons that respond to ``number"", ``string,"" and higher-level ``text"" properties for the token tagging task are found. Our insights guide future research about compact and efficient code models.",https://dr.lib.iastate.edu/handle/20.500.12876/GvqXQpqw,Computer science,"Machine learning,Software engineering",Redundancy and concept analysis for code models on the token tagging task,thesis
"Mitra, Simanta,Prabhu, Gurpur,Cai, Ying","Bhowmik, Souradeep",N/A,"Code written in modern programming languages (such as Java) can be almost impossible to understand and maintain due to poor design and coding practices used during its development. Instead of redeveloping the entire code from scratch (which is an expensive and time-consuming proposition), typically a series of refactoring steps are applied to make the software better in terms of both design and coding quality, which translates to better user experience because the maintainability and scalability of the application is increased. In this project we consider an existing code base that was written hastily in Java and was really poor in terms of design and code quality. We share our experiences in refactoring this code base in order to make it modular and with improved design and code quality. We first analyzed the existing code base to identify areas for improvement and then used certain benchmark metrics to guide the refactoring. We present a comparison of the final state of the code with the original code base to demonstrate the use of good software development practices.",https://dr.lib.iastate.edu/handle/20.500.12876/dv6l1exz,Computer science,"Code quality,Moularity in code,Refactoring",Refactoring an existing code base to improve modularity and quality,thesis
"Le, Wei,Gao, Hongyang,Cohen, Myra","Steenhoek, Benjamin Jeremiah",N/A,"Software vulnerabilities allow attackers to take down important services and steal users’ private data. Many new vulnerabilities are reported each year, showing that vulnerabilities are prevalent in software programs. Therefore, it is critically important for developers to detect vulnerabilities before releasing their software. Recently, deep learning models have been successfully trained to detect vulnerabilities by learning to classify vulnerable and non-vulnerable code from open-source projects on Github. However, the existing datasets suffer from limited and imbalanced data, and both factors hurt the models’ performance.

We implemented a framework for automatically applying refactoring as a data augmentation technique to increase the diversity of program datasets and address data imbalance. Our refactoring framework can be tuned for different models and datasets. We evaluated our approach by using it to train state-of-the-art deep learning models. Our results show that naively refactoring programs does not significantly improve model performance. We found that some refactorings decrease model performance because they introduce tokens that are outside of the model’s vocabulary, and that naive applications of refactoring do not produce sufficiently diverse programs. Our method can be tuned to improve model performance above state-of-the-art methods by producing diverse programs and targeting imbalanced data. We also found that our method can be applied to the majority of programs in practice. Based on our results, we believe that refactoring is a useful data augmentation technique that will benefit further research and applications of deep learning for vulnerability detection.",https://dr.lib.iastate.edu/handle/20.500.12876/avVO95xr,Computer science,"bug detection,deep learning,machine learning,refactoring,semantic-preserving transformations,vulnerability detection",Refactoring programs to improve the performance of deep learning for vulnerability detection,thesis
"Cohen, Myra B,Lutz, Jack H,Iadecola, Thomas","Kitt, Linsey",N/A,"Quantum computing is a field that has drawn a lot of attention over the years as it has shown great potential to solve new problems and has seen rapid development. To meet the demands of this rapid development, software engineering researchers have been studying methods to address the unique challenges presented by the probabilistic nature of quantum computing in its software platforms. One important aspect of this is understanding how to test and validate quantum software platforms when the expected outputs when running a program are not always known. Recent research using metamorphic testing was able to detect 13 faults in the Qiskit quantum computing platform; however, this is a rapidly evolving platform. Since their work, Qiskit has already advanced ahead several versions, which brings up more questions regarding research for quantum software platforms: do these tools researchers create hold up across versions of these frequently-updated platforms, and how much work does it take to maintain and extend these tools? 

In this thesis, we study the ease of regression testing in the context of this quantum computing platform. We update to the latest version of Qiskit, extend two existing metamorphic transformations on a metamorphic testing platform for Qiskit, add six new metamorphic transformations, and perform a set of metamorphic testing runs to analyze how our changes perform. In doing so, we were able to discover five additional new faults on the Qiskit platform; however, we also uncovered several challenges, which we synthesize for the reader. We also offer discussion on where the future work of metamorphic testing on quantum software platforms may lead based on our challenges and successes during this process.",https://dr.lib.iastate.edu/handle/20.500.12876/aw4NZV7r,Computer science,"quantum computing,software testing",Regression testing of an evolving quantum platform,thesis
"Leslie Miller,Eve S Wurtele","Balakrishnan, Aravindh kumar",2018-08-11T16:32:42.000,"<p>One of the greatest challenges in the field of biology today is the determination of unknown gene function. A number of web applications are currently being developed which aim at providing gene related information, such as gene expression and co-expression data for individual genes. Integrating gene expression networks with external data sources may give researchers additional information about these genes.</p>
<p>A regulon is a collection of highly co-expressed genes which can be identified by clustering a network of genes obtained from transcriptomic analysis. Regulon analysis offers a potential way to evaluate functions of genes in a given gene family or for developing hypothesis about the function of unknown genes. Hence, it is important that information about regulons, genes, and co-expression data be available to all biologists in a user friendly manner. Currently, information about regulons and its associated genes are not easily accessible, and in order to view regulon information, manual look up into flat files such as text files is required. In order to avoid this, we come up with an application which aim at providing regulon, gene, and co-expression data for three important species in a user-friendly manner using a web interface.</p>
<p>RegulonIT is a web based tool that aims at providing information about regulons and other gene related information. Currently, the tool provides regulon, gene and co-expression information for three species (Arabidopsis thaliana, Saccharomyces cerevisiae and Homo sapiens) for very large transcriptomic datasets using a web interface. RegulonIT is a user friendly platform independent web tool and is available at http://metnetdb.org:9090/regulonit.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26459,"Bioinformatics,Computer Sciences",,"RegulonIT - A web based tool for regulon, gene, and co-expression data",thesis
"Sarkar, Soumik,Chowdhury, Ratul,Reuel, Nigel,Li, Qi","Ferdous, Md Sakib",N/A,"Reinforcement learning (RL), a subset of machine learning (ML), can potentially optimize and control biomanufacturing processes, such as improved production of therapeutic cells. Here, the process of CAR-T cell activation by antigen presenting beads and their subsequent expansion is formulated in-silico. The simulation is used as an environment to train RL-agents to dynamically control the number of beads in culture with the objective of maximizing the population of robust effector cells at the end of the culture. We make periodic decisions of incremental bead addition or complete removal. The simulation is designed to operate in OpenAI Gym which enables testing of different environments, cell types, agent algorithms and state-inputs to the RL-agent. Agent training is demonstrated with three different algorithms (PPO, A2C and DQN) each sampling three different state input types (tabular, image, mixed); PPO-tabular performs best for this simulation environment. Using this approach, training of the RL-agent on different cell types is demonstrated, resulting in unique control strategies for each type.  Sensitivity to input noise (sensor performance), number of control step interventions, and advantage of pre-trained agents are also evaluated. Therefore, we present a general computational framework to maximize the population of robust effector cells in CAR-T cell therapy production.",https://dr.lib.iastate.edu/handle/20.500.12876/5w5pVD5z,Artificial intelligence,"CAR T-cell,Cell Therapy,Deep Reinforcement Learning,Machine Learning,Reinforcement Learning",Reinforcement learning-guided control strategies for CAR T-cell activation and expansion,thesis
"Aduri, Pavan,Chaudhuri, Soma,Quinn, Christopher","Chen, Michael Qi Yin",N/A,"This thesis studies the class of languages solvable by Adaptive Massively Parallel Computations in constant rounds from a computational complexity theory perspective. A language $L$ is in the class $\AMPC^0$ if for every $\varepsilon \in(0,1)$, there is a deterministic AMPC algorithm running in constant rounds with a $\textup{poly}(n)$ processors, where the local memory of each machine $O(n^\varepsilon)$. This thesis proves $\L\subsetneq\AMPC^0$ and then further improves it by showing $\ReachUL\subsetneq \AMPC^0$. The complexity class $\ReachUL$ lies between the well-known space-bounded complexity classes $\L$ and $\NL$. We also show that $\AMPC^0\subseteq\cap_{\varepsilon\in(0,1)}\DSPACE(n^\varepsilon)$, which is stronger than $\AMPC^0\subseteq\SubEXP$. We establish that it is unlikely that $\PSPACE$ admits $\AMPC$ algorithms, even with polynomially many rounds. We also establish that showing $\PSPACE$ is a subclass of (nonuniform) $\AMPC$ with polynomially many rounds leads to a significant separation result in complexity theory, namely $\PSPACE$ is a proper subclass of $\EXP^{\Sigma_2^{\P}}$.",https://dr.lib.iastate.edu/handle/20.500.12876/7wbOKBYv,Computer science,"AMPC,Complexity Classes,LogSpace,Massively Parallel Computation,NL,PSPACE",Relations between Space-Bounded and Adaptive Massively Parallel Computations,thesis
"Li, Qi,Le, Wei,Cohen, Myra","Sium, Yonas Afewerki",N/A,"The major success of any machine learning algorithm heavily relies on an efficient representation learning of the data. Having inefficient representation learning of the data leads to loss of important information that can lead to poor performance of the downstream tasks. Graph representation learning is getting more popular due to the introduction of structural information with its own feature to get effective representation learning on graph data. In this work, we developed an efficient graph-based program representation learning for an effective search for algorithms implementation from repositories using pseudo-code as a query. We designed the {\it p-language} to specify an algorithm, which we call {\it p-code}. By automatically analyzing {\it p-code}, we extract control flow, math, and natural language information from the algorithm descriptions. The three types of features of {\it p-code} and source code are encoded by a graph neural network and used by the code retrieval technique for search. We implemented our technique in a tool called {\it Beryllium} and performed the experiments on Java, C, and the mixed languages of Java and C. The results of our search tool Beryllium, show that the effective graph representation learning for algorithm and program greatly help to identify the algorithm implementations in different languages and outperform the state-of-the-art code search tools.",https://dr.lib.iastate.edu/handle/20.500.12876/JvNVB43v,"Computer science,Computer science","algorithms implementation search,graph auto encoder,graph representation learning,node grouping,p-code,search using pseudo code",Representation learning for algorithms implementation search using pseudo code,thesis
Vasant Honavar,"Bao, Jie",2018-08-22T14:49:38.000,"<p>Realizing the full potential of the semantic web requires the large-scale adoption and use of ontology based approaches to sharing of information and resources. In such a setting, instead of a single, centralized ontology, it is much more natural to have multiple distributed ontologies that cover different, perhaps partially overlapping, domains. Such ontologies represent the local knowledge of the ontology designers, that is, knowledge that is applicable within a specific context. Hence, many application scenarios, such as collaborative construction and management of complex ontologies, distributed databases, and large knowledge base applications, present an urgent need for ontology languages that support localized and contextualized semantics, partial and selective reuse of ontology modules, flexible ways to limit the scope and visibility of knowledge (as needed for selective knowledge sharing), federated approaches to reasoning with distributed ontologies, and structured approaches to collaborative construction of large ontologies. Against this background, this dissertation develops a family of description logics based modular ontology languages, namely Package-based Description Logics (P-DL), to address the needs of such applications. The main contributions of this dissertation include: (1) The identification and theoretical characterization of the desiderata of modular ontology languages that can support selective sharing and reuse of knowledge across independently developed knowledge bases; (2) The development of a family of ontology languages called P-DL, which extend the classical description logics (DL) to support selective knowledge sharing through a novel semantic importing mechanism and the establishment of a minimal set of restrictions on the use of imported concepts and roles to support localized semantics,  transitive propagation of imported knowledge, and different interpretations from the point of view of different ontology modules; (3) The development of a family of sound and complete tableau-based federated reasoning algorithms for distributed, autonomous, P-DL ontologies including   ALCP  and   SHIQP , i.e., P-DL onologies where the individual modules are expressed in the P-DL counterpart of DL   ALC  and   SHIQ  respectively, that can be used to efficiently reason over a set of distributed, autonomous, ontology modules from the point of view of any specific module, that avoid the need to integrate ontologies using message exchanges between modules as needed; (4) The formulation of criteria for answering queries against a knowledge base using hidden or private knowledge, whenever it is feasible to do so without compromising hidden knowledge, and the development of privacy-preserving reasoning strategies for the case of the commonly used hierarchical ontologies and   SHIQ  ontologies, along with a theoretical characterization of the conditions under which they are guaranteed to be privacy-preserving; (5) The development of some prototype tools for collaborative development of large ontologies, including support for concurrent editing and partial loading of ontologies into memory.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/69515,"Artificial Intelligence and Robotics,Computer Sciences",Computer science;,Representing and reasoning with modular ontologies,dissertation
Vasant G. Honavar,"Santhanam, Ganesh Ram",2018-08-11T15:36:47.000,"<p>Many applications call for techniques for representing and reasoning about preferences, i.e., relative desirability over a set of alternatives. Preferences over the alternatives are typically derived from preferences with respect to the various attributes of the alternatives (e.g., a student's preference for one course over another may be influenced by his preference for the topic, the time of the day when the course is offered, etc.). Such preferences are often qualitative and conditional. When the alternatives are expressed as tuples of valuations of the relevant attributes, preferences between alternatives can often be expressed in the form of (a) preferences over the values of each attribute, and (b) relative importance of certain attributes over others. An important problem in reasoning with multi-attribute qualitative preferences is dominance testing, i.e., to find if one alternative (assignment to all attributes) is preferred over another. This problem is hard (PSPACE-complete) in general for well known qualitative conditional preference languages such as TCP-nets.</p>
<p>We provide two practical approaches to dominance testing. First, we study a restricted unconditional preference language, and provide a dominance relation that can be computed in polynomial time by evaluating the satisfiability of an appropriately constructed logic formula. Second, we show how to reduce dominance testing for TCP-nets to reachability analysis in an <i>induced preference graph</i>. We provide an encoding of TCP-nets in the form of a Kripke structure for CTL. We show how to compute dominance using NuSMV, a model checker for CTL.</p>
<p>We address the problem of identifying a preferred outcome in a setting where the outcomes or alternatives to be compared are composite in nature (i.e., collections of components that satisfy certain functional requirements). We define a dominance relation that allows us to compare collections of objects in terms of preferences over attributes of the objects that make up the collection, and show that the dominance relation is a strict partial order under certain conditions. We provide algorithms that use this dominance relation to identify only (sound), all (complete), or at least one (weakly complete) of the most preferred collections. We establish some key properties of the dominance relation and analyze the quality of solutions produced by the algorithms. We present results of simulation experiments aimed at comparing the algorithms, and report interesting conjectures and results that were derived from our analysis.</p>
<p>Finally, we show how the above formalism and algorithms can be used in preference-based service composition, substitution, and adaptation.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26040,Computer Sciences,"Compositional Systems,Decision Theory,Dominance Testing,Model Checking,Preference Reasoning,Qualitative Preferences",Representing and reasoning with qualitative preferences for compositional systems,dissertation
"Guang Song,Robert Jernigan","Vammi, Vijay",2018-08-11T19:36:52.000,"<p>The important structural and functional roles played by proteins in the proper functioning of cellular processes cannot be overstated. To comprehensively understand their functional behaviors, structural models derived from experimental data have been developed and these models have played a significant role in explaining the functional mechanisms of proteins. The paradigm ""structure drives function"" had been active for many years until recent evidence suggested that the complex functions of proteins could not be fully explained by a single structure and dynamics played a very important role in deciphering their functions. To incorporate dynamics into structural representations, ensembles of conformations, instead of a single structure, are used frequently in recent literature and are found to be successful in explaining the functions of many proteins. The work described in this thesis focuses on methods used to construct such ensemble representations of proteins. A careful investigation of the issues and challenges in obtaining such ensembles is undertaken.</p>
<p>In the first part of the thesis, we focus on representing the native state of a given protein using a weighted ensemble representation, where relative populations (or Boltzmann weights) are assigned for individual members of the ensemble. This representation has the advantage of representing the dynamics using only a few conformational states, thereby minimizing the potential of over-fitting, while capturing the dynamics of the protein that a single average structure misses. Using Ubiquitin as an example, we show that determination of such a weighted ensemble representation is feasible when using RDCs as constraints. Moreover, the conformational states of the weighted ensemble are biologically relevant to the functional behaviors of the protein. We then compare the quality of the weighted ensemble representation with other representations available for Ubiquitin and show that the weighted ensemble representation can successfully reproduce a series of experimental data (RDCs, Residual Chemical Shift Anisotropies, Amide Exchange reactivities and solution scattering profiles) equally well or even better than other representations and without over-fitting. We then extend this work and determine a weighted ensemble representation for Hen Egg White Lysozyme (HEWL). To establish the quality of this ensemble, we perform a series of rigorous cross-validation of this ensemble against extensive amount of experimental data available for HEWL. Lastly, we perform a series of NMR structure refinements under synthetic and controlled conditions to evaluate the structural quality of obtained solutions by various refinement protocols. Our results indicate that ensemble refinement protocols without using weights and good initial conformations may not result in better descriptions of protein native states even though they appear to fit experimental data better and even pass cross-validation tes</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28646,"Bioinformatics,Biophysics","Bioinformatics and Computational Biology,HEWL,NMR,Residual dipolar coupling,Ubiquitin,weighted ensembles",Representing protein native states using weighted conformation ensembles,dissertation
"Lutz, Robyn,Cohen, Myra,Basu, Samik","Khor, Chin",N/A,"Variability constraints are an integral part of the requirements for a configurable system. The constraints specified in the requirements on the legal combinations of options define the space of potential valid configurations for the system-to-be. This thesis reports on our experience with the variability-related requirements constraints of a flight software framework used by multiple space missions. A challenge that we saw for practitioners using the current framework, now open-sourced, is that the specifications of its variability-related requirements and constraints are dispersed across several documents, rather than being centralized in the software requirements specification. Such dispersion can contribute to misunderstandings of the side-effects of design choices, increased effort for developers, and bugs during operations. Based on our experience, we propose a new software variability model, similar to a product-line feature model, in the flight
software framework. We describe the structured technique called VarCORE by which our model is developed, demonstrate its use, and evaluate it on a key service module of the flight software. Results show that our lightweight modeling technique helped find missing and inconsistent variability-related requirements and constraints. More generally, we suggest that a variability modeling technique such as this can be an efficient way for developers to centralize the specification and improve the analysis of dispersed variability-related requirements and constraints in other configurable systems.",https://dr.lib.iastate.edu/handle/20.500.12876/3wxabAev,Computer science,"Configurable system,Feature model,Requirement analysis,Variability constraints,Variability requirements",Requirements analysis of variability constraints in a configurable flight software system,thesis
"Jia, Yan-Bin,Bhattacharya, Sourabh,Gao, Hongyang,Tavanapong, Wallapak,Wongpiromsarn, Tichakorn","Xue, Yuechuan",N/A,"Human manipulation skills are rich in the creative use of physical contact to enable objects to act with the external environment and achieve the desired effects under the control of the hand. However, robotic manipulation is difficult to benefit from this dexterity because contact can cause manipulation tasks to fail by introducing large forces or unexpected changes in constraints, especially in the presence of modeling uncertainties and disturbances. A properly designed model of robot-object dynamics with imposed compliance control can provide the robot with the resilience and reliability to handle contact.

This thesis tackles several contact control problems arising in robotics dexterous manipulations such as robotic cutting, contact mode control, finger-object dynamics under joint coupling constraints, and with a focus on robotic grasping of tools via pivoting and finger gaiting.

In some practical task, the hand changes its grasp status and relocate its fingers in order to perform the manipulation. Such a strategy with not only the manipulation of the object but also the relocation of the fingers is called a finger gait. Successful execution of a sequence of finger gaits does not simply reduce to planning collision-free paths for the involved fingers.
The central question is how to control the movement of one finger during a gait while not losing existing finger contacts with the object, which inevitably experiences a motion due to contact force changes under the main control policy.


This thesis focuses on a single finger gait executed by an anthropomorphic hand driven by an arm.
To improve stability, the object's tip is used as a pivot on the supporting plane.
The gait consists of three stages: removal, during which the contact force by its performing finger gradually decreases to zero; relocation, during which the finger is controlled to follow a preplanned path (relative to the moving object) to establish a new contact; and addition, during which the contact force on the finger increases to some desired level.
Hybrid position/impedance controls are devised, with reference finger forces obtained via optimization to satisfy the friction cone constraints and be dynamically consistent with the object’s motion, which in turn provides pose references for the fingertip to maintain contact.",https://dr.lib.iastate.edu/handle/20.500.12876/9z0KLx7r,"Robotics,Computer science","Contact Modelling,Dexterous Manipulation,Force Control,Grasping,Motion Control of Manipulators,Multifingered Hands",Robotic grasping of tools via pivoting and finger gaiting,dissertation
"Wongpiromsarn, Tichakorn (Nok),Sharma, Anuj,Sarkar, Soumik","Shihab, Ibne Farabi",N/A,"The study highlights a robust ensemble model’s effectiveness for accurate sidewalk detection,
vital for both road safety and efficient curb space management. To evaluate the proposed
ensemble model, three distinct datasets were utilized: Cityscapes, Ade20k, and the Boston
Dataset. The results demonstrated the superiority of the ensemble model over its individual
components, as manifested by mIOU scores of 93.1%, 90.3%, and 90.6% on the Cityscapes,
Ade20k, and Boston datasets respectively, in optimal conditions. Under exposure to various noise
types, such as Salt-and-Pepper and Speckle, across low to high intensities, the model revealed a
steady, controlled decline in performance. This stands in contrast to the rapid drop experienced
by individual models. Overall, the model demonstrated robustness and dependability. The
ensemble model’s resilience and dependability make a strong case for its application in enhancing
road safety through reliable sidewalk detection and efficient curb space management.",https://dr.lib.iastate.edu/handle/20.500.12876/2vaZL1gr,"Artificial intelligence,Computer science","CityScapes Dataset,Enhancing road safety,Ensemble method,Image Segmentation,Robustness,Sidewalk",Robust and precise sidewalk detection with ensemble learning: Enhancing road safety and facilitating curb space management,thesis
Deepinder P. Sidhu,"Lin, Ta",2018-08-15T07:09:08.000,"<p>The routing is one very important function implemented in computer communication networks. It collects information about optimal paths within a network;The purpose of this dissertation is to study the routing function in large networks which are characterized by frequent topological changes. The study focuses on constructing routing protocols with some desirable properties such as distributed computation, adaptation to flow variations within the network, failsafe against arbitrary topological changes, loop-free route tables for all destinations at all times, bounded values for variables, and fast recovery from topological changes;At present, most routing protocols use the next-node routing technique, a technique in which each node keeps only the next node identification for a particular destination. A different type of routing scheme provided in some protocol standards is called source routing. A source routing protocol builds complete paths from a source to all destinations in the network. Several source routing algorithms are derived with desirable properties;Most distributed routing algorithms use one or more variables to store unbounded values such as update cycle numbers. Distributed routing protocols with bounded update cycle numbers are proposed. The proposed protocols possess desirable properties and are obtained by applying the sliding window idea for flow control in networks to the routing protocols which employ unbounded update cycle numbers;How fast an algorithm provides optimal paths from every node to every other node in a network after a topological change is an important consideration in the design of routing algorithms. Several algorithms for achieving shortest paths to all destinations in the network with improved recovery speed from topological changes are presented.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/82356,Computer Sciences,Computer science,Routing issues for dynamic networks,dissertation
N/A,"Guan, Zhe",2020-08-05T05:01:50.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/97230,,,"Scalability and performance of MPI, HPF and OpenMP on an Origin 2000",thesis
"Huang, Xiaoqiu,Phillips, Gregory,Dickerson, Julie,Dorman, Karin,Jannesari, Ali","Chon, Alvin",N/A,"Technology, data processing, and availability have led to large increases in data set sizes and computing resources. Traditional solutions can neither handle large data sets nor leverage more resources, prompting a need for advanced data structures, algorithms, and solutions. In particular, phylogenetics is a long-standing field with a rich history in the design of novel algorithms, heuristics, and visualization. It is a challenging field; no polynomial-time algorithms are known for many problems and many heuristics techniques have been developed and used in solving these problems. 

 Scalable and extensible approaches are key requirements for modern solutions. We explore data structures and algorithms to generalize problems faced in genomics with examples in phylogenetics in a scalable and extensible manner. Scalable explicitly refers to the ability to process increasing inputs efficiently. In general, the solution should be unrestricted to small inputs or niche cases and be efficient regarding time complexity. Extensibility can be defined as the ability for the solution to be general, not restricted to special cases, and the solution can be extended for multiple purposes or applied to various applications. Numerous problem-specific approaches in phylogenetics, as in many fields, work extremely well but are limited to that problem. 
 
 We present a step back in data structures for a common problem and show how this data structure can be both a scalable and extensible base for many algorithms as solutions to common phylogenetic problems. This successful approach to developing a common data structure shows promise as an alternative to inter-node distance matrix calculation as a precursor or prerequisite. Additionally, many algorithms can take advantage of this bipartition frequency hash to reduce complexity by at least an order of magnitude. Use cases showing the value of this data structure are presented, including Robinson-Foulds calculations, fast alternative tree inference, and tree refinement sampling alternatives demonstrated on multiple popular real-world data sets and simulations. The real-world data sets give us first-hand comparisons with other methods and show the advantages and disadvantages of the methods, while the simulations give us data points for time and space complexity for a scalability analysis. We also demonstrate a reference based alternative to a specific usage of Multiple Sequence Alignment. Lastly, we explore state-of-the-art Machine Learning platforms to learn features in a semi-supervised manner. Examples of data structures, use cases, and this platform are discussed.",https://dr.lib.iastate.edu/handle/20.500.12876/aw4N7M7r,"Bioinformatics,Computer science,Artificial intelligence","Algorithms,Bioinformatics,Machine Learning,Phylogenetics",Scalable and extensible data Structures and methods for big data in phylogenomics and phylogenetics,dissertation
"Basu, Samik,Aduri, Pavan,Dorius, Shawn","Bhatt, Rishabh Rajendra",N/A,"Existing strategies for solving multiobjective submodular optimizations are not scalable and cannot be applied to large networks, also these strategies work under the assumption that the exact value of the submodular function is known to us. In our work we relax the requirement of an exact oracle and experimentally evaluate δ-approximate oracle for the multi-objective submodular optimization problem. We show that group influence maximization in the context of online social networks is an instance of this optimization problem with cardinality constraint and δ-oracle. We develop a new scalable strategy for solving multiobjective problems where the objective functions closely represent influence maximization objectives and create a prototype implementation of our solution strategy to solve the group influence maximization and experiment on networks of varying sizes. We empirically evaluate our new algorithm against various known existing strategies comparing runtime, individual influence spread, groups activated above threshold and various other metrics. Our algorithm being scalable to larger net- works allows us to experiment with really large social networks and evaluate the effectiveness of the multi objective optimization strategy on some large real world social networks which had not been extensively experimentally evaluated previously. We run our algorithm on various real world networks like Facebook, DBLP etc and also experiment with a few group assignment strategies and show how different group assignment policies affect the effectiveness of the algorithm.",https://dr.lib.iastate.edu/handle/20.500.12876/9z0Km4qr,Computer science,"approximate oracle,Group influence maximization,Multiobjective submodular optimization,submodular optimization",Scalable approximate algorithm for group influence maximization in online social network: An experimental evaluation,thesis
David Fernandez-baca,"Deepak, Akshay",2018-08-11T16:54:45.000,"<p>Phylogenetic trees providing high quality information and at the same time covering large number of species are essential for comparative biology. It is a widely accepted fact that with the currently available resources we are far from assembling one completely sampled phylogenetic tree for all life (or one based on a very large subset of species), hence a need for an interim solution arises. Here we describe SearchTree, a software tool that allows users to query efficiently on an arbitrary user taxon list and returns high scoring matches from approximately one billion phylogenetic trees being constructed from molecular sequence data in GenBank. The core of SearchTree has two parts. The first is a pre-computed collection of phylogenetic species trees from GenBank sequence data consisting of approximately 10,000,000 data sets with 100 bootstrap trees for each set for a total of around 1 billion trees. The goal here is to ensure high `coverage' (i.e., each taxon occurring in many trees). The second part is the search-retrieval process. The goal is to quickly retrieve the clusters and the subsequent trees from the large data set described above, maximizing the scoring function for the resultant set of trees and all the while keeping computational resources within a limit. Both parts were dealt separately due to their complexity; here we focus on the second part.</p>
<p>The complete pre-computed data set of phylogenetic trees will be around 500 GB. Fast response times are achieved by SearchTree through a combination of techniques from information retrieval, notably inverted indexing, and from computational phylogenetics, especially for constructing consensus trees. The use of Redwood cluster, an advanced hardware configuration specifically tuned for this kind of work, has further improved the query times by 100%.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25419,Computer Sciences,"AToL,SearchTree",SearchTree: Mining robust phylogenetic trees,thesis
"Zhang, Wensheng,Cai, Ying,Guan, Yong,Liu, Jia,Tavanapong, Wallapak","Liu, Pinglan",N/A,"With the popularity and prosperity of cloud services, outsourcing computation to cloud servers
has become an irreversible trend which can not only reduce the computation burden of clients, but
also fully utilize the resources of and make profits for servers. However, the cloud servers may not be
fully trustworthy. For example, due to external attacks or internal malice, cloud servers may steal
sensitive information involved in the outsourced computation, or may not execute the computation
honestly but instead fake the results. Hence, it is desired to retain information confidentiality and
computation integrity. The main goal of our study is to construct effective and efficient solutions for
preserving confidentiality and assuring integrity in computation outsourcing. We are particularly
interested in exploring the solutions based on the integration of homomorphic encryption (HE),
trusted execution environment (TEE) and game theory.
Our research includes two parts. In the first part, we propose game-theoretic schemes to pro-
tect the integrity of outsourced generic computation, which includes: (1) a scheme with trusted
third party (TTP); (2) a scheme with blockchain instead of TTP; (3) algorithms for optimizing
fund allocation in game theory based computation outsourcing. In the second part, we target at
protecting the confidentiality of data and model for the outsourced inference and training of neural
network models, based on the integration of leveled-HE (LHE) and TEE. More specifically:
• To assure the integrity of outsourced computation, we propose an effective and collusion-
resilient scheme on top of the state-of-the-art approaches that employ redundant servers. We
consider a more general collusion contract between servers, and propose a game-theoretic
scheme for the TTP to conduct probabilistic checking to defeat the collusion.
• To avoid the reliance on TTP (or trusted authority in general), we propose a scheme on
top of blockchain, where outsourced tasks are posted as smart contracts based on our defined
templates and executed by a tunable number of executors. Our game-theoretic design enforces
economically-rational executors to honestly execute the tasks individually or collectively.
• For the efficiency of game-theoretic outsourcing models, we propose algorithms to optimize the
fund allocation at servers and clients, with the objectives of minimizing the latency for task
computation/verification, maximizing the payoffs for the servers, and assuring economically-
rational servers to honestly execute each task.
• To preserve the confidentiality of data and model in outsourced convolutional neural net-
work (CNN) inference, we propose a new framework based on integration of LHE and TEE
which enables collaboration among parties that do not trust each other. We also propose a
generic and efficient LHE-based inference scheme, along with optimizations, as an important
performance-determining component of the framework.
• To protect the confidentiality of data and model for autonomous and continuous CNN model
refining in cloud, we design a model-refining scheme that integrates LHE and TEE. The core
of the scheme is the application of LHE for protecting the confidentiality of training data and
model in the backward propagation process of model training, and the application of TEE
for periodically reducing noises in ciphertexts.
We have conducted comprehensive quantification and verification of the security of our proposed
schemes, and detailed evaluations of their performance efficiency.",https://dr.lib.iastate.edu/handle/20.500.12876/2vaZm67r,Computer science,"Computation Outsourcing,Confidentiality,Game Theory,Homomorphic Encryption,Neural Network,Trusted Execution Environment","Securing outsourced computation based on Game Theory, HE and TEE",dissertation
"Zhang, Wensheng,Ceylan, Halil,Jannesari, Ali","Jiang, Hao",N/A,"Road roughness affects riding comfort and vehicle operational cost, and to maintain good road quality, federal and state transportation agencies must routinely survey road roughness. Since traditional methods for such surveys require specialized equipment and training, more efficient methods that collect measurement data from sensors in off-the-shelf mobile/smart devices (such as Android phones and iPhones), have recently been developed.  Such sensor data, along with labelled road roughness index value, can be used to construct machine-learning models for inferring road roughness from the sensor data. Since the sensor data generated by different device types/models 
have different characteristics, in current practice a model is often constructed from the data of one single or small set of device type/model, using only the data from the same configuration for predictions. This means that, despite the potentially large amount of sensor data from a greater variety of devices, labelled data may still be lacking when applying machine learning to construct a model in a specific setting.
Transfer learning focuses on extracting knowledge from a source domain, then applying it to a related target domain, and this method has been used to reduce training/labeling costs for a target domain. Based on existing extensive research, in this work we propose a clustering-based seeded-transfer learning approach to address the road roughness modeling and prediction problem. We specifically develop a complete solution for transferring data from a source domain (sensor data collected from devices of type/model A) to a target domain (sensor data collected from devices of type/model B) for model training. Our contributions include: a data scaling step, an implementation to match source clusters and target seeds, and the exploration of clustering methods, optimal number of clusters, and seeds percentage of transfer learning.
We evaluate the performance of our approach using both the sensor data set collected from the field and some public data sets. The results show that: K-means clustering is less stable than hierarchical clustering; a moderate seeds percentage is preferred; using a greater number of clusters positively affects model prediction accuracy.",https://dr.lib.iastate.edu/handle/20.500.12876/PrMBL64z,Computer science,"Machine learning,Road roughness,Software,Transfer learning",Seeded transfer learning for road roughness regression,thesis
"Gao, Hongyang,Huai, Mengdi,Bao, Forrest","Hergenreter, Nathan",N/A,"The field of computer vision and more specifically image classification is an ever-popular and
ever-useful area of research. With deep advances into the problem of classifying images contained
within some predefined domain of classes having been made, new directions expanding upon this
base problem are increasingly worth investigating. In this paper, we propose a method that seeks to
follow that notion and tackle a slightly different problem: novel class determination. The proposed
method utilizes a CNN image classifier as a base and extends it to produce a feature vector as
an output for a given input sample. Utilizing a set of training sample feature vectors, analysis is
performed on these vectors to produce a model. The proposed model is then able to classify an
image as a known or unknown class (denoted in this paper as seen and unseen respectively). The
results produced in this paper show that the outlined problem can be solved with a light-weight
solution that minimally builds off existing CNN classifiers.",https://dr.lib.iastate.edu/handle/20.500.12876/KrZJV8qr,"Computer science,Artificial intelligence","Artificial Intelligence,Computer Vision,Image Classification,Machine Learning",Seen/Unseen classification using feature vector analysis,thesis
"Rajan, Hridesh,Tavanapong, Wallapak,Li, Qi","Ahmed, Shibbir",N/A,"We propose PASS, a O(n) algorithm for data reduction that is specifically aimed at preserving the semantics of time series data visualization in the form of line chart. Visualization of large trend line data is a challenge and current sampling approaches do produce reduction but result in loss of semantics and anomalous behavior. We have evaluated PASS using seven large and well-vetted datasets (Taxi, Temperature, DEBS challenge 2012-2014 dataset, New York Stock Exchange data, and Integrated Surface Data) and found that it has several benefits when compared to existing state-of-the-art time series data reduction techniques. First, it can preserve the semantics of the trend. Second, the visualization quality using the reduced data from PASS is very close to the original visualization. Third, the anomalous behavior is preserved and can be well observed from the visualizations created using the reduced data. We have conducted two user surveys collecting 3,000+ users’ responses for visual preference as well as perceptual effectiveness and found that the users prefer PASS over other techniques for different datasets. We also compare PASS using visualization metrics where it outperforms other techniques in five out of the seven datasets.",https://dr.lib.iastate.edu/handle/20.500.12876/9z0Kml7r,Computer science,"Anomaly,Sampling,Semantics,Time series data visualization",Semantics and anomaly preserving sampling strategy for large-scale time series data,thesis
N/A,"Wang, Yanmei",2020-11-09T01:34:55.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/98357,,,Sending non-contiguous data in MPI programs,thesis
"Vasant Honavar,Drena Dobbs","Xue, Li",2018-08-11T10:54:33.000,"<p>Protein-protein interactions play a central role in the formation of protein complexes and the biological pathways that orchestrate virtually all cellular processes. Three dimensional structures of a complex formed by a protein with one or more of its interaction partners provide useful information regarding the specific amino acid residues that make up the interface between proteins. The emergence of high throughput techniques such as Yeast 2 Hybrid (Y2H) assays has made it possible to identify putative interactions between thousands of proteins (but not the interfaces that form the structural basis of interactions or the structures of protein complexes that result from such interactions). Reliable identification of the specific amino acid residues that form the interface of a protein with one or more other proteins is critical for understanding the structural and physico-chemical basis of protein interactions and their role in key cellular processes, for predicting protein complexes, for validating protein interactions predicted by high throughput methods, for ranking conformations of protein complexes generated by docking, and for identifying and prioritizing drug targets in computational drug design.</p>
<p>However, given the high cost of experimental determination of the structures of protein complexes, there is an urgent need for reliable and fast computational methods for identifying interface residues and/or predicting the structure of a complex formed by a protein of interest with its interaction partners. Given the large and growing gap between the number of known protein sequences and the number of experimentally determined structures, sequence-based methods for predicting protein-protein interfaces are of particular interest. Against this background, we develop HomPPI ( http://homppi.cs.iastate.edu/), a class of sequence homology based approaches to protein interface prediction. We present two variants of HomPPI: (i) NPS-HomPPI (non-partner-specific HomPPI), which can be used to predict interface residues of a query protein in the absence of knowledge of the interaction partner. NPS-HomPPI is based on the results of a systematic analysis of the conditions under which interface residues of a query protein are conserved among its sequence homologs (and hence can be inferred from the known interface residues in proteins that are sequence homologs of the query protein). Our experiments suggest that when sequence homologs of the query protein can be reliably identified, NPS-HomPPI is competitive with several state-of-the-art interface prediction servers including those that exploit the structure of the query proteins. (ii) PS-HomPPI (partner-specific HomPPI), which can be used to predict the interface residues of a query protein with a specific target protein. PS-HomPPI is based on a systematic analysis of the conditions under which the interface residues that make up the interface between a query protein and its interaction partner are preserved among their homo-interologs, i.e., complexes formed by their respective sequence homologs. To the best of our knowledge, with the exception of protein-protein docking (which is computationally much more expensive than PS-HomPPI), PS-HomPPI is one of the first partner-specific protein-protein interface predictors. Our experiments with PS-HomPPI show that when homo-interologs of a query protein and its putative interaction partner can be reliably identified, the interface predictions generated by PS-HomPPI are significantly more reliable than those generated by NPS-HomPPI.</p>
<p>Protein-Protein Docking offers a powerful approach to computational determination of the 3-dimensional conformation of protein complexes and protein-protein interfaces. However, the reliability of conformations produced by docking is limited by the efficacy of the scoring functions used to select a few near-native conformations from among tens of thousands of possible conformations, generated by docking programs. Against this background, we introduce DockRank, a novel approach to rank docked conformations based on the degree to which the interface residues inferred from the docked conformation match the interface residues predicted by a partner-specific sequence homology based interface predictor PS-HomPPI. We compare, on a data set of 69 docked cases with 54,000 decoys per case, the ranking of conformations produced using DockRank's interface similarity scoring function applied to predicted interface residues obtained from four protein interface predictors: PS-HomPPI, and three NPS interface predictors NPS-HomPPI, PRISE, and meta-PPISP, with the rankings produced by two state-of-the-art energy-based scoring functions ZRank and IRAD. Our results show that DockRank significantly outperforms these ranking methods. Our results that NPS interface predictors (homology based and machine learning-based methods) failed to select near-native conformations that are superior to those selected by DockRank (partner-specific interface prediction based), highlight the importance of the knowledge of the binding partners in using predicted interfaces to rank docked models. The application of DockRank, as a third-party scoring function without access to all the original docked models, for improving ClusPro results on two benchmark data sets of 32 and 56 test cases shows the viability of combining our scoring function with existing docking software. An online implementation of DockRank is available at http://einstein.cs.iastate.edu/DockRank/.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26720,Bioinformatics,"conservation,docking,partner-specific,prediction,protein-protein interface,sequence homology",Sequence homology based protein-protein interacting residue predictions and the applications in ranking docked conformations,dissertation
"Vasant Honavar,Drena Dobbs","Walia, Rasna",2018-08-11T14:23:56.000,"<p>The interaction of RNAs with proteins is fundamental for executing many of the key roles they play in living systems, including translation, post-transcriptional regulation of gene expression, RNA splicing, and viral replication. Recently, new roles for RNA-protein interactions have emerged, following the discovery that the human genome is pervasively transcribed and produces thousands of non-coding RNAs (ncRNAs). Although the functions of many ncRNAs are not yet known, one emerging theme is that long non-coding RNAs (lncRNAs) often drive the formation of ribonucleoprotein (RNP) complexes, which in turn influence the regulation of gene expression. Although the human genome is predicted to encode almost as many different RNA-binding proteins as DNA-binding transcription factors, our current understanding of the cellular roles of RNA-binding proteins, how they recognize their targets, and how they are regulated, lags far behind our understanding of transcription factors.</p>
<p>To improve our comprehension of RNA-protein recognition and the regulation of RNA-protein interaction networks within cells, this dissertation has four related goals: (i) performing a rigorous and systematic evaluation of sequence- and structure-based methods for predicting RNA-binding residues in proteins; (ii) developing improved method for predicting interfacial residues in RNA-binding proteins, using only sequence information; (iii) generating a comprehensive collection of RNA-protein interaction motifs (RPIMs); and (iv) developing improved methods for RNA-protein interaction partner prediction.</p>
<p>First, we present a systematic evaluation of state-of-the-art machine learning methods for predicting RNA-binding residues in proteins, using three carefully curated benchmark datasets and a rich set of data representations. We show that sequence-based methods trained using position-specific scoring matrices (PSSMs) perform better than structure-based methods, which use more complex features extracted from the 3D structures of proteins. Second, we present RNABindRPlus, a new method for predicting RNA-binding residues in proteins, using only sequence information. The predictor combines output from an optimized Support Vector Machine (SVM) classifier with the output from a novel homology-based method (HomPRIP). We show that RNABindRPlus performs better than all currently available methods for predicting interfacial residues in proteins. Third, we extract more than 30,000 unique RNA-protein interfacial motifs (RPIMs), consisting of contiguous residues from both the RNA and protein chains of characterized RNA-protein complexes. Lastly, we demonstrate the utility of RPIMs in predicting RNA-protein interaction partners. We employ them in an innovative and significantly improved method for partner prediction and show that it has both a high true positive rate and a much lower false positive rate than other available methods. Taken together, the results presented here provide important new insights into the determinants of RNA-protein recognition, in addition to valuable new software tools for interrogating and predicting RNA-protein complexes and interaction networks.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28423,Bioinformatics,Bioinformatics and Computational Biology,Sequence-based prediction of RNA-protein interactions,dissertation
Xiaoqiu Huang,"Agrawal, Ankit",2018-08-11T18:43:40.000,"<p>Sequence comparison is one of the most fundamental computational problems in bioinformatics for which many approaches have been and are still being developed. In particular, pairwise sequence alignment forms the crux of both DNA and protein sequence comparison techniques, which in turn forms the basis of many other applications in bioinformatics. Pairwise sequence alignment methods align two sequences using a substitution matrix consisting of pairwise scores of aligning different residues with each other (like BLOSUM62), and give an alignment score for the given sequence-pair. The biologists routinely use such pairwise alignment programs to identify similar, or more specifically, related sequences (having common ancestor). It is widely accepted that the relatedness of two sequences is better judged by statistical significance of the alignment score rather than by the alignment score alone. This research addresses the problem of accurately estimating statistical significance of pairwise alignment for the purpose of identifying related sequences, by making the sequence comparison process more sequence-specific.</p>
<p>The major contributions of this research work are as follows. Firstly, using sequence-specific strategies for pairwise sequence alignment in conjunction with sequence-specific strategies for statistical significance estimation, wherein accurate methods for pairwise statistical significance estimation using standard, sequence-specific, and position-specific substitution matrices are developed. Secondly, using pairwise statistical significance to improve the performance of the most popular database search program PSI-BLAST. Thirdly, design and implementation of heuristics to speed-up pairwise statistical significance estimation by an factor of more than 200. The implementation of all the methods developed in this work is freely available online.</p>
<p>With the all-pervasive application of sequence alignment methods in bioinformatics using the ever-increasing sequence data, this work is expected to offer useful contributions to the research community.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24854,Computer Sciences,"Homology detection,Pairwise statistical significance,Sequence alignment,Sequence comparison",Sequence-specific sequence comparison using pairwise statistical significance,dissertation
N/A,"Mi, Liangchuan",2020-07-27T20:45:52.000,"<p>Shapes reconstruction bridges real objects and their computer models. Most of the shape reconstruction techniques were derived for computer vision applications. A very important sense of human, tactile sensing can be applied to acquire shape information about 2D and 3D objects. Nevertheless, tactile data usually has a lot of noise. In this thesis, I present an applicable scheme that acquires shape data using a simple joystick sensor and then reconstructs 2D shapes and 3D patches. The 2D shapes are tracked by an Adept Cobra robot and represented as polynomial functions determined by the 3L fitting algorithm. The 3D shapes are composed of multiple patches, each of which is described by a polynomial function generated by least-square fitting. Experiments have been carried out with the robot. A display environment for 3D objects has also been developed.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97001,,Computer science,Shapes reconstruction from robot tactile sensing,thesis
"Tian, Jin,Tavanapong, Wallapak,Sukul, Adisak","Madahali, Lale",N/A,"Social bots have garnered increasing attention and prominence, coinciding with the pervasive integration of artificial intelligence (AI) into numerous applications and our daily routines. These automated agents, conceived to streamline repetitive and mundane tasks, have found their presence across various platforms, including Wikipedia, Facebook, and Twitter. This research, however, specifically delves into the realm of Twitter bots. On Twitter, bots often operate incognito, concealing their true identities. Regrettably, some leverage this anonymity to propagate misinformation and disinformation. Within this study, we narrow our focus to Twitter bots, meticulously characterizing their diverse manifestations. While prior research has predominantly centered on malicious bots, this study introduces the concept of genuine bots, embarking on a comparative analysis between these two categories. We define benign bots as helpful bots that are identifiable from their names on Twitter (containing ``bot"" in their handle on Twitter). Notably, we have assembled a new dataset encompassing humans, malicious bots, and genuine bots, thereby facilitating a comprehensive exploration of their behaviors and distinctions.",https://dr.lib.iastate.edu/handle/20.500.12876/erLKmxpv,Computer science,"classification,machine learning,Social Bots,social networks,Twitter bots",Social bots,thesis
N/A,"Ge, Jun",2020-11-06T02:24:26.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/97793,,,Software obfuscation by CFI-hiding scheme and self-modifying scheme,thesis
"Leslie Miller,Leslie Miller,Sarah M. Nusser","Lokasari, Andre",2018-08-22T15:39:10.000,"<p>The effectiveness of software interface design is a lot of the times tied to the user of the software.  In particular, it is tied to how users comprehend the spatial stimuli presented to them through the software interface (spatial information encoding).  Thus, the goal of the research is to understand the aspects of different levels of spatial ability on a spatial software interface for creating a theoretical structure of spatial ability on a simple task as a stepping stone to understand a more complex demanding task.  Why is this important?  The importance of knowing the different spatial ability aspect relates to how one can design software interfaces to suit different levels of spatial ability.;To examine the impact of spatial ability on using user interfaces, we developed a set of user interfaces that required the user to interpret an instruction and choose the matching buttons.  In total three interfaces were developed ranging from simply matching colors to an interface loosely based on the address canvassing task used by the Census Bureau to identify the correct location of housing units.  A cognitive model was developed for the interfaces that incorporated spatial ability.  The model was implemented using ACT/R and evaluated against user studies involving 63 participants.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/69061,Computer Sciences,Computer science,Spatial ability cognitive model with ACT-R 6.0,thesis
N/A,"Bhatt, Rushi",2020-11-22T06:40:10.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/98445,,,Spatial learning and localization in rodents: Behavioral simulations and results,thesis
Samik Basu,"Rajagopal Padmanabhan, Madhavan",2021-01-16T18:24:32.000,"<p>In the modern day, social networks have become an integral part of how people communicate information and ideas. Consequently, leveraging the network to maximize information spread is a science that is applied in viral marketing, political propaganda. In social networks, an idea/information starts from a small group of users (known as seed users) and is propagated through the network via connections of the seed users. There are limitations on the number of seed users that can be convinced to adopt a certain idea. Therefore, the problem exists in finding a small set of users who can maximally spread an idea/information. This is known as the influence maximization problem. While this problem has been studied extensively, the presence of potential adversarial users and their impact on the information spread, has not been considered in existing solutions.In this thesis, we study the problem of spreading information to Target users while limiting the spread from reaching adversarial(Non Target) users. To this end, we consider a hard constraint - the objective is to maximize the information spread among the Target users while the number of Non-Target users to whom the information reaches is limited by a hard constraint. We design two algorithms - Natural Greedy and Multi Greedy with efficient RIS based implementations. We run our solutions on real world social networks to study the information spread. Finally, we evaluate the quality of our solutions on different models of diffusion and network settings.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/94536,,"Influence Maximization,Information Diffusion,Social Network Analysis,Submodular Optimization",Spreading information in social networks containing adversarial users,dissertation
Ali Jannesari,"Burke, Matthew",2020-06-26T19:59:53.000,"<p>It has been shown that image segmentation models can be improved with an adversarial loss. Additionally, previous analysis of adversarial examples in image classification has shown that image datasets contain features that are not easily recognized by humans. This work investigates the effect of using a second adversarial loss to further improve image segmentation. The proposed model uses two generative adversarial networks stacked together, where the first generator takes an image as input and generates a segmentation map. The second generator then takes this predicted segmentation map as input and predicts the errors relative to the ground truth segmentation map. If these errors contained additional features that are not easily recognized by humans, they could possibly be learned by a discriminator. The proposed model did not consistently show significant improvement over a single generative adversarial model, casting doubt about the existence of such features.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/32166,,"generative adversarial network,image segmentation,machine learning",Stacked generative adversarial networks for learning additional features of image segmentation maps,thesis
N/A,"Crenshaw, John",2018-08-25T01:58:10.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/77073,Computer Sciences,"Least squares,Error analysis (Mathematics),Computer science",Starting approximations for rational fraction least-squares fitting,dissertation
Jack H. Lutz,"Summers, Scott",2018-08-23T12:05:20.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/68380,Computer Sciences,Computer science,Strict self-assembly of discrete Sierpinski triangles,thesis
Jin Tian,"Chen, Yetian",2018-08-11T13:47:17.000,"<p>Bayesian networks are a class of probabilistic graphical models that have been widely used in various tasks for probabilistic inference and causal modeling. A Bayesian network provides a compact, flexible, and interpretable representation of a joint probability distribution. When the network structure is unknown but there are observational data at hand, one can try to learn the network structure from the data. This is called structure discovery.</p>
<p>Structure discovery in Bayesian networks is a host of several interesting problem variants. In the optimal Bayesian network learning problem (we call this structure learning), one aims to find a Bayesian network that best explains the data and then utilizes this optimal Bayesian network for predictions or inferences. In others, we are interested in finding the local structural features that are highly probable (we call this structure discovery). Both structure learning and structure discovery are considered very hard because existing approaches to these problems require highly intensive computations.</p>
<p>In this dissertation, we develop algorithms to achieve more accurate, efficient and scalable structure discovery in Bayesian networks and demonstrate these algorithms in applications of systems biology and educational data mining. Specifically, this study is conducted in five directions.</p>
<p>First of all, we propose a novel heuristic algorithm for Bayesian network structure learning that takes advantage of the idea of curriculum learning and learns Bayesian network structures by stages. We prove theoretical advantages of our algorithm and also empirically show that it outperforms the state-of-the-art heuristic approach in learning Bayesian network structures.</p>
<p>Secondly, we develop an algorithm to efficiently enumerate the k-best equivalence classes of Bayesian networks where Bayesian networks in the same equivalence class are equally expressive in terms of representing probability distributions. We demonstrate our algorithm in the task of Bayesian model averaging. Our approach goes beyond the maximum-a-posteriori (MAP) model by listing the most likely network structures and their relative likelihood and therefore has important applications in causal structure discovery.</p>
<p>Thirdly, we study how parallelism can be used to tackle the exponential time and space complexity in the exact Bayesian structure discovery. We consider the problem of computing the exact posterior probabilities of directed edges in Bayesian networks. We present a parallel algorithm capable of computing the exact posterior probabilities of all possible directed edges with optimal parallel space efficiency and nearly optimal parallel time efficiency. We apply our algorithm to a biological data set for discovering the yeast pheromone response pathways.</p>
<p>Fourthly, we develop novel algorithms for computing the exact posterior probabilities of ancestor relations in Bayesian networks. Existing algorithm assumes an order-modular prior over Bayesian networks that does not respect Markov equivalence. Our algorithm allows uniform prior and respects the Markov equivalence. We apply our algorithm to a biological data set for discovering protein signaling pathways.</p>
<p>Finally, we introduce Combined student Modeling and prerequisite Discovery (COMMAND), a novel algorithm for jointly inferring a prerequisite graph and a student model from student performance data. COMMAND learns the skill prerequisite relations as a Bayesian network, which is capable of modeling the global prerequisite structure and capturing the conditional independence between skills. Our experiments on simulations and real student data suggest that COMMAND is better than prior methods in the literature. COMMAND is useful for designing intelligent tutoring systems that assess student knowledge or that offer remediation interventions to students.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29861,"Artificial Intelligence and Robotics,Computer Sciences","Bayesian networks,parallel algorithms,prerequisite graphs,probabilistic graphical models,structure discovery,student modeling",Structure Discovery in Bayesian Networks: Algorithms and Applications,dissertation
"Jin Tian,Huaiqing Wu","He, Ru",2018-08-11T19:10:28.000,"<p>Statistical learning refers to a set of methodologies for modeling and understanding data. In this thesis we will present our work for two research problems in statistical learning.</p>
<p>The first research problem is learning Bayesian network structures from data. We will present three different approaches for this learning problem. The first approach is to propose a dynamic programming algorithm that can compute the exact posterior probability of any modular feature of a Bayesian network with any general structure prior. The second approach is to develop a dynamic programming algorithm for obtaining the k best Bayesian network structures and then use these k best network structures to compute the posterior probabilities of hypotheses of interest based on Bayesian model averaging. The third approach is to develop new algorithms to efficiently sample Bayesian network structures according to the exact structure posterior and then use these sampled structures to construct estimators for the posterior of any feature. These three approaches all use dynamic programming techniques to learn Bayesian network structures from data.</p>
<p>The second research problem is session identification and analysis for the domain of people search within a professional social network. We will present our work for this research problem based on the data from LinkedIn social network. Two important refinements are proposed to address the drawbacks of the content-based method, one of two main session identification methods commonly used in real applications. We describe the underlying rationale of our refinements and then empirically show that the content-based method equipped with our refinements is able to achieve an excellent identification performance in the domain. Finally, based on our refined content-based session identification method, the corresponding session analysis is performed and the profession-oriented nature of the domain is illustrated.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28006,"Artificial Intelligence and Robotics,Computer Sciences","Bayesian networks,Session analysis,Structure learning",Structure learning in Bayesian networks and session analysis of people search within a professional social network,dissertation
Wensheng Zhang,"Lu, Xuejia",2020-01-02T18:32:14.000,"<p>VANET is an emerging mobile ad hoc network paradigm that facilitates vehicle-to-vehicle and vehicle-to-infrastructure communication. The most important application of the VANET is for driving safety. Road condition-awareness is critical for driving safety. Existing VANET-based systems usually assume drivers detect and report safety related road conditions, which however may be untrue because, drivers may not be willing to perform these duties, or such duties may distract drivers and thus make driving even unsafe. Therefore, automatic detection without human intervention is desired. As the first contribution of this thesis work, an automatic road condition detection system has been designed based on the idea of collecting and analysing the footprints of vehicles to infer anomaly. It has also been studied how to utilize inexpensive roadside devices, such as sensors, to facilitate the information collection and analysis, especially in the absence of connectivity between vehicles.</p>
<p>Due to the difficulty of conducting large-scale experiments on real roads, simulation plays an important role in VANET research. To make simulation close to the reality, it is desired to include detailed and realistic simulation of vehicle behaviour under various road conditions, and this is especially needed for studies targeted at driving safety. In the past, however, the simulation of vehicle behaviours are often overly simplified and implemented as a trivial extension of the network simulator. As a second contribution of this thesis work, a detailed and realistic simulator of vehicle behaviour has been developed based on the car-following and lane-changing models.</p>
<p>As the simulation of vehicle behaviour and that of communication behaviour are different tasks, they should be implemented separately for better modularity and meanwhile they should be seamlessly integrable. As another contribution of this thesis work, the online and seamless integration of vehicle behaviour simulator and network simulator has been studied. Specifically, a set of APIs has been designed and implemented atop the vehicular behaviour simulator to facilitate its integration with network simulator. Being a concrete example, the integration of ns2 and SUMO, an open-source vehicular behaviour simulator, has been implemented, and applied to simulate an electric vehicular network.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24533,Computer Sciences,"Integration,Safety,Vehicular Network,Vehicular Simulator",Study on vehicular network application and simulation,thesis
"Aduri, Pavan,Basu, Samik,Liu, Jia,Li, Qi,Dorius, Shawn","Fu, Xiaoyun",N/A,"Diffusion of information in social networks has been the focus of intense research in recent past decades due to its significant impact in shaping public discourse through group/individual influence. Existing research primarily models influence as a binary property of entities: influenced or not influenced. While this is a useful abstraction, it discards the notion of degree of influence, i.e., certain individuals may be influenced ``more'' than others. We introduce the notion of \emph{attitude}, which, as described in social psychology, is the degree by which an entity is influenced by the information. Intuitively, attitude captures the number of distinct neighbors of an entity influencing the latter.

We present an information diffusion model (Attitude-IC model) that quantifies the degree of influence, i.e., the attitude of individuals, in a social network.  With this model, we formulate and study the attitude maximization problem.  We prove that the function for computing attitude is monotonic and sub-modular, and the attitude maximization problem is NP-Hard. We present a greedy algorithm for maximization with an approximation guarantee of $(1-1/e)$. In the context of the Attitude-IC model, we study two other problems with the aim of investigating the scenarios where attaining individuals with high attitudes is objectively more important than maximizing the attitude of the entire network. In the first problem, we introduce the notion of \emph{actionable attitude}; intuitively, individuals with actionable attitude are likely to ``act'' on their attained attitude.  We show that the function for computing actionable attitude, unlike that for computing attitude, is non-submodular and however is \emph{approximately submodular}. We present an approximation algorithm for maximizing actionable attitude in a network. In the second problem, we consider identifying the number of individuals in the network with attitudes above a certain value, a threshold. In this context, the function for computing the number of individuals with attitude above a given threshold induced by a seed set is \emph{neither submodular nor supermodular}. We present heuristics for realizing the solution to the problem. We experimentally evaluate our algorithms and study empirical properties of the attitude of nodes in networks, such as spatial and value distribution of high attitude nodes.

While attitude maximization introduces new aspects and insight into the basic information diffusion models, it still focuses on maximizing the “influence” measure of the seed set over the \emph{whole} network without differentiating the individuals by their affiliation to different groups. To take this into account, we study the {\em Group Influence Maximization Problem} which seeks to influence each group in the network \emph{fairly}. We then generalize this problem to the {\em Multi-objective Optimization Problem with Cardinality Constraint in the context of $\delta$-approximate Oracles} and show that it is possible to ensure a $(1-1/e)^2 - 3\delta$ approximation guarantee. As the group influence maximization in online social networks is an instance of this optimization problem with cardinality constraint and $\delta$-oracles, we apply the results to it. We develop a prototype implementation of our solution strategy for the group influence maximization problem on networks of different sizes and experimentally justify the effectiveness and scalability of our strategy. 

To further explore fairness in influence over social networks, we study the {\em Representative Submodular Maximization Problem}, where the objective is to select a \emph{fair} portion of seed nodes from each group of the network to maximally propagate the information to the whole network. A natural greedy algorithm for this problem has an approximation ratio of $\frac{1}{2}$ (for any $k$). We show that the natural greedy algorithm's approximation ratio can not be improved, and we design a new approximation algorithm for this problem with improved approximation ratios. When $k=2$, our algorithm achieves an approximation ratio of $\frac{2}{3}$, for $k= 3$ the approximation ratio is $\frac{3}{5}$. For general $k\leq n/2$, the approximation ratio is $\frac{k}{2k-1}$. Our algorithm makes $O(kn)$ calls to the submodular function $f$, which is asymptotically the same as the natural greedy algorithm. We experimentally validate our algorithm, and extend the results to the more general {\em Submodular Maximization Problem under Partition Matroid Constraint}.",https://dr.lib.iastate.edu/handle/20.500.12876/kv7kJBpv,Computer science,"multiple objectives,partition matroid,social influence maximization,submodular optimization",Submodular optimization problems in the context of social influence,dissertation
N/A,"Chen, Duhong",2020-08-05T18:36:49.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/97293,,,Supertree construction by matrix representation with flip,thesis
"Fernandez-Baca, David,Aduri, Pavan,Eulenstein, Oliver,Huang, Xiaoqiu,Martin, Ryan","Liu, Lei",N/A,"Supertree construction problems synthesize source phylogenetic trees with overlapping taxon sets into one single supertree that contains all information from the source trees. Naturally, there exist conflicts or data shortages among the source trees. These obstacles make it difficult for a method to construct a high quality supertree efficiently.

Two supertree construction problems are studied first, namely compatibility testing problem and agreement testing problem. The compatibility testing algorithm takes as an input a profile $\P = \{\T_1, \T_2, \dots, \T_k\}$, and it targets at finding a tree $\T$ which takes the union of the taxon sets of the source trees, and each source tree $\T_i$ can be obtained from restriction of $\T$ to taxon set of $\T_i$ through proper edge contractions. We conducted an empirical experimental studies with the state-of-the-art compatibility testing algorithm and demonstrated that the algorithm was quite efficient with the support of auxiliary display graphs and dynamic graph connectivity data structure.

The agreement testing algorithm adds more requirements to compatibility testing problem. Given a profile $\P = \{\T_1, \T_2, \dots, \T_k\}$, the agreement testing problem asks whether an agreement tree $\T$ exists, such that the taxon sets of the agreement tree $\T$ is the union of taxon sets of the source trees such that the restriction of $\T$ to the taxon set of $\T_i$ is isomorphic to $\T_i$. We propose the first agreement testing algorithm that solved the agreement testing problem of profiles containing internal labels. Our algorithm runs in  $\Oh(n k (\sum_{i \in [k]} d_i + \log^2(nk)))$ time, where $n$ is the total number of distinct taxa in \P, $k$ is the number of trees in \P, and $d_i$ is the maximum number of children of a node in $\T_i$.

In practice, profiles that are compatible or agree are rare, and neither compatibility testing algorithms nor agreement testing algorithms can return supertrees. Based on our previous studies, we propose a new divide-and-conquer algorithm that can build up supertrees in $\mathcal{O}(kn^3\log^2(nk))$ time, where $n$ is the total number of taxa and $k$ is the total number of source trees. In order to improve the quality of the constructed supertrees, we also proposed several heuristics to achieve the goals. Our empirical experimental studies show that our method is competitive with other methods on a wide range of datasets, and indeed outperforms other methods on most of the real biological datasets we tested.",https://dr.lib.iastate.edu/handle/20.500.12876/3wxaNOxv,Computer science,"Agreement,Compatibility,Dynamic Graph Connectivity,Graph Algorithms,Phylogenetic tree,Supertree Construction",Supertree construction with display graphs and dynamic graph connectivity,dissertation
"Lu Ruan,Wensheng Zhang","Feng, Taiming",2018-08-11T17:24:39.000,"<p>Network survivability, reflecting the ability of a network to maintain an acceptable level of service during and after failures, is an important requirement for WDM optical networks due to the ultra-high capacity. The most common network failure is the link failure which could cause enormous data loss and lots of service disruption to Internet users. Although single-link failures are the most common failure scenarios, double-link failures can occur in some cases and cause more severe problem. Compared to unicast sessions, multicast sessions suffer more seriously from link failures because a link may carry traffic to multiple destinations rather than to a single destination. Hence, multicast sessions demand more effective and efficient protection against link failures. With the increasing demand for access bandwidth, the access networks draw more attention. The hybrid wireless-optical broadband-access network (WOBAN) is a promising architecture for future access networks because it combines the high capacity of optical communication and the flexibility and cost-effectiveness of a wireless network.</p>
<p>First, we consider the problem of protecting unicast connections against double link failures. The basic idea is to use two p-Cycles, with link-disjoint protection segments, to protect each working link. To utilize spare capacity more efficiently, we also propose a new hybrid protection/restoration scheme to handle two-link failures. Our scheme uses protection to ensure that most of the affected demands can be restored using the pre-planned backup paths upon a two-link failure. For the demands not restorable with protection, we use dynamic restoration to find new backup paths for them.</p>
<p>Second, we propose protection schemes for multicast sessions under one link failure. An intelligent p-Cycle (IpC) scheme is presented to provide p-Cycle protection for dynamic multicast sessions. When a multicast request arrives, a multicast tree is computed for it and then the IpC scheme is used to compute a set of high efficient p-Cycles on-demand to protect each link on the multicast tree. Then we propose a p-cycle-based path protection scheme and a PXT-based path protection scheme to provide protection for dynamic multicast sessions. Basically, to protect a multicast tree, we compute one p-Cycle and one PXT for each destination node v such that the p-Cycle and the PXT can be used to restore the traffic to v when a link failure occurs on the path from the source node to v.</p>
<p>Finally, we propose a new protection scheme for the hybrid wireless-optical broadband-access network(WOBAN). The scheme is cost-effective in that it does not require the PONs to have self-protecting capability. Based on the proposed protection scheme, we define the maximum protection with minimum cost(MPMC) problem and present one ILP solution approach to the MPMC problem. Then we prove the MPMC problem is NP-Hard and provide one heuristic algorithm for the MPMC problem.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25904,Computer Sciences,"Access Networks,Backbone Networks,Optical,Survivability",Survivability schemes for optical backbone and access networks,dissertation
Lu Ruan,"Xiao, Nan",2018-08-11T10:12:59.000,"<p>Compared with traditional WDM network, OFDM-based flexible optical networks are able to provide better spectral efficiency due to its flexible allocation of requests on finer granularity subcarriers. Survivability is a crucial issue in OFDM-based networks, although little work has been done in this topic. In this thesis, a survivable multipath provisioning scheme is presented, which provides flexible protection levels to individual demands in OFDM-based flexible optical networks. We also define the static Survivable Multipath Routing and Spectrum Allocation (SM-RSA) problem which aims to accommodate a given set of demands with minimum spectral utilization. We show that the static SM-RSA problem is NP-hard and provide ILP formulation for it. Also, an efficient heuristic algorithm is given to solve the problem. Our simulation results of both ILP solution and heuristic method show that the proposed multipath provisioning scheme achieves better spectral efficiency than the traditional single path provisioning scheme.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27115,Computer Sciences,"Flexible optical networks,Multipath Provisioning,OFDM",Survivable multipath provisioning in OFDM-based exible optical networks,thesis
"Jin Tian,Wei Le","Agrawal, Shubham",2018-08-11T08:28:43.000,"<p>Compiler error messages facilitate software development and debugging by providing cause and location of the error but due to various compiler bugs and inconsistencies it often fails its purpose and negatively affect performance of both novice and experienced programmers. An errant semicolon or brace can result in many errors reported throughout the program. This study tries to statistically analyze open source code base to predict real errors from different type of compiler error messages. It also tries to auto-fix these errors.</p>
<p>At the high level, this study handles two cases (1) when one error is present in code, (2) when two different errors are present in the code. We start with collecting different type of random error messages for both the cases by random error generation in C projects. We developed different models using document clustering, probabilistic topic modeling and multi-label classification algorithms for training and predicting real errors using collected error messages for both the cases.</p>
<p>Our empirical evaluation on open-source projects has shown that our model correctly predicts the real error in almost 95% cases, when only one error exists in program. In case of two errors, model correctly predicts at least one error in almost 91% cases and both the errors in almost 39% cases.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29836,Computer Sciences,,Syntax errors identification from compiler error messages using ML techniques,thesis
N/A,"Dowling, Wayne",2018-08-15T16:09:27.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/78610,Graphics and Human Computer Interfaces,"Computer Science,Computer Graphics",TAG: A Terminal for Animated Graphics. Its design and simulation,dissertation
"Zarecor, Kimberly,Canniffe, Bernard,Evans, Peter,Gilbert, Stephen,Leuth, Patience","Polito, Francesca",N/A,"Technology and innovation are reshaping our societies and economies at an accelerated pace, requiring new skills for individuals to thrive in economy and society and new educational models to support these changes. In response, world organizations seek solutions rooted in sustainable human development to ensure that future educational models are just and provide individuals with real opportunity to in their pursuit of a meaningful life. The United States is not an exception and has also put forth educational goals aimed at addressing 21st century skill needs most relevant for success in today’s technology driven, global society through STEM educational initiatives. However, efforts to attain these goals in the United States through traditional education have encountered substantial challenges and remains low, particularly among women, and historically underrepresented and disadvantaged communities. The purpose of this study was to understand why learners sought additional education following their traditional education, to switch careers and enter STEM fields, rather than enter STEM fields directly following their traditional educational journeys including elementary, high school and through university. 
This research aims to explore the application of the Capability Approaches to educational technology design, and how it may be used as a lens to detect vulnerabilities in our educational infrastructure that hinder individuals from actualizing their capabilities. It seeks to also explore a bootcamp educational model as a potential educational design approach to STEM education.
Many individuals aspire to have careers that are both fulfilling and long-lasting. However, they often face challenges in pursuing careers that truly resonate with their personal values and contribute to their overall happiness. This is a growing concern as the rapid pace of technology may require that individuals possess skills that enable them to reskill and upskill more frequently in one's lifetime. Failure to address barriers that hinder an individual’s ability to actualize one’s capabilities may have a widening effect on skills gaps that perpetuate economic disadvantage as we move to a reliance on technology in this effort. 
These studies utilized narrative case studies to examine the experiences of learners to illustrate their individuals' perspectives on the process of self-discovery throughout their traditional educational journeys, concluding in their participation in a UX boot camp. The study found that while most individuals had interests and desire to enter STEM fields, they were challenged by lack of opportunity to develop skill, knowledge, and confidence, necessary for them to self-actualize their academic and career pursuits in STEM fields. Additionally, from a capability perspective, the structure, pedagogy, support, and curriculum of the bootcamp was able to meet human development needs of learners in a condensed time frame, and impart 21st century skills of active learning, communication, collaboration, problem solving, analytics, creativity, and innovation. Thus, it holds potential for improved skill development and STEM learning, providing a valuable perspective that allows educators, researchers, educational technology designers, and policy makers to more effectively create educational strategies that align with the goals of 21st century education and sustainable human development.",https://dr.lib.iastate.edu/handle/20.500.12876/2vaZLoDr,"Educational technology,Engineering,Sustainability","Capability Approach,Education,Educational Philosophy,Educational Technology,Engineering,STEM",The Capability Approach: Addressing learners' ability to pursue STEM interests and careers.,dissertation
"Masha Sosonkina,Ying Cai,Mark Gordon","Peng, Fang",2018-08-23T07:21:11.000,"<p>GAMESS, a quantum chemistry program for electronic structure calculations, has been freely shared by high-performance application scientists for over twenty years. It provides a rich set of functionalities and can be run on a variety of parallel platforms through a distributed data interface. While a chemistry computation is sophisticated and hard to develop, the resource sharing among different chemistry packages will accelerate the development of new computations and encourage the cooperation of scientists from universities and laboratories. Common Component Architecture (CCA) offers an environment that allows scientific packages to dynamically interact with each other through components, which enable dynamic coupling of GAMESS with other chemistry packages, such as MPQC and NWChem. Conceptually, a computation can be constructed with ""plug-and-play"" components from scientific packages and require more than componentizing functions/subroutines of interest, especially for large-scale scientific packages with a long development history. In this research, we present our efforts to construct components for GAMESS that conform to the CCA specification. The goal is to enable the fine-grained interoperability between three quantum chemistry programs, GAMESS, MPQC and NWChem, via components. We focus on one of the three packages, GAMESS; delineate the structure of GAMESS computations, followed by our approaches to its component development. Then we use GAMESS as the driver to interoperate integral components from the other two packages, and show the solutions for interoperability problems along with preliminary results. To justify the versatility of the design, the Tuning and Analysis Utility (TAU) components have been coupled with GAMESS and its components, so that the performance of GAMESS and its components may be analyzed for a wide range of system parameters.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/68227,Computer Sciences,Computer science,The component-based application for GAMESS,thesis
Wei Le,"Dominguez Perez, Danilo",2019-08-21T10:24:42.000,"<p>Mobile devices have become ubiquitous over the last years. Android, as the leading platform in the mobile ecosystem, have over 2.5 million apps published in Google Play Market. This enormous ecosystem creates a fierce competition between apps with similar functionality in which the low quality of apps has been shown to increase the churn rate considerably. Additionally, the complex event-driven, framework-based architecture that developers use to implement apps imposes several challenges and led to new varieties of code smells and bugs. There is a need for tools that assure the quality of apps such as program analysis and testing tools. One of the foundational challenges for developing these tools is the sequencing or ordering of callback methods invoked from external events (e.g. GUI events) and framework calls. Even for a small subset of callbacks, it has been shown that the current state-of-the-art tools fail to generate sequences of callbacks that match the runtime behavior of Android apps.</p>
<p>This thesis explores the construction and applications of new representations and program analyses for event-driven, framework-based mobile applications, specifically Android apps. In Android, we observe that the changes of control flow between entry points are mostly handled by the framework using callbacks. These callbacks can be executed synchronously and asynchronously when an external event happens (e.g. a click event) or a framework call is made. In framework-based systems, method calls to the framework can invoke sequences of callbacks. With the high overhead introduced by libraries such as the Android framework, most current tools for the analysis of Android apps have opted to skip the analysis of these libraries. Thus, these analyses missed the correct order of callbacks for each callback invoked in framework calls. This thesis presents a new specification called Predicate Callback Summary (PCS) to summarize how library or API methods invoke callbacks. PCSs enable inter-procedural analysis for Android apps without the overhead of analyzing the whole framework and help developers understand how their code (callback methods) is executed in the framework. We show that our static analysis techniques to summarize PCSs is accurate and scalable, considering the complexity of the millions of lines of code in the Android framework.</p>
<p>With PCSs summaries, we have information about the control flow of callbacks invoked in framework calls but lack information about how external events can execute callbacks. To integrate event-driven control flow behavior with control behavior generated from framework calls, we designed a novel program representation, namely Callback Control Flow Automata (CCFA). The design of CCFA is based on the Extended Finite State Machine (EFSM) model, which extends the Finite State Machine (FSM) by labeling transitions using information such as guards. In a CCFA, a state represents whether the execution path enters or exits a callback. The transition from one state to another represents the transfer of control flow between callbacks. We present an analysis to automatically construct CCFAs by combining two callback control flow representations developed from the previous research, namely, Window Transition Graphs (WTGs) and PCSs. To demonstrate the usefulness of our representation, we integrated CCFAs into two client analyses: a taint analysis using FLOWDROID, and a value-flow analysis that computes source and sink pairs of a program. Our evaluation shows that we can compute CCFAs efficiently and that CCFAs improved the callback coverages over WTGs. As a result of using CCFAs, we obtained 33 more true positive security leaks than FLOWDROID over a total of 55 apps we have run. With a low false positive rate, we found that 22.76\% of source-sink pairs we computed are located in different callbacks and that 31 out of 55 apps contain source-sink pairs spreading across components.</p>
<p>In the last part of this thesis, we use the CCFAs to develop a new family of coverage criteria based on callback sequences for more effective testing Android apps. We present 2 studies to help us identify what types of callbacks are important when detecting bugs. With the help of the empirical results, we defined 3 coverage criteria based on callback sequences. Our evaluation shows that our coverage criteria are a more effective metric than statement and GUI-based event coverage to guide test input generation.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31187,Computer Sciences,"android,mobile,program analysis,software engineering,static analysis,testing",The construction and applications of callback control flow graphs for event-driven and framework-based mobile apps,dissertation
Ying Cai,"Wang, Sen",2018-07-21T09:27:30.000,"<p>This thesis describes the design and implementation of a real-time criminal tracking system. A criminal under tracking is asked to wear a low energy Bluetooth device and carry a smartphone. The Bluetooth device is secure on his body (e.g., hand or foot) and communicates with the smartphone, which communicates with a central server through cellular networks. The smartphone monitors the status of the Bluetooth device and reports to the server in real time when the status changes (e.g., connection lost or device being taken off). Moreover, it monitors the criminal's movement and reports to the server whenever the criminal moves into an alert zone, a geographic region where the law enforcement wants the criminal's movement to be tracked. Compared to the existing tracking approaches, our system has the following desired features. 1) Scalable. Instead of having a criminal to report its location all the time, the system allows one to configure where and when the criminal needs to be tracked, thus minimizing both mobile communication cost and server processing cost. 2) Low-cost. The system uses only off-shelf components (e.g., Bluetooth device and smartphone) which communicate through regular wireless networks. 3) Secure. The communication between a Bluetooth device and the corresponding smartphone is authenticated through One Time Password.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/27799,Computer Sciences,"Bluetooth,smartphone,wireless networks",The design and implementation of a smartphone and Bluetooth-based criminal tracking system,thesis
N/A,"O'Neil, Thomas",2018-08-23T07:03:32.000,"<p>A multidimensional forest is a data structure which is a generalization of the conventionally-defined forest. One- and two-dimensional forests correspond to strings and conventional forests respectively. Regular forest grammars can be written to produce sets of n-dimensional forests, and a frontier operation can be applied to sets of n-dimensional forests to yield sets of strings. Thus, n-dimensional regular forest grammars define a class of string languages for each value of n;One-dimensional forest grammars yield exactly the regular languages. Two dimensional forest grammars yield all the context-free languages and perhaps some non-context-free languages which can be produced without copying substrings. Three-dimensional forests grammars yield all the IO macro languages and at least some of the OI macro languages. The frontier operation on three-dimensional forests has the same copying power as the derivation operation in macro grammars, but it has more deleting power. The enhanced deleting power of a three-dimensional forest grammar allows both IO and OI macro derivations to be simulated in a single formal system.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/65423,Computer Sciences,Computer science,The multidimensional forest languages,dissertation
N/A,"Hare, Justin",2020-11-09T01:10:42.000,"<p>The potential of animated characters in virtual environments is great. They can be used to populate an environment and give it a life-like feel, or to represent real users in networked collaborative environments, becoming a vehicle of human-to-human communication. Many researchers have worked with using animated characters in virtual environments for a variety of purposes. The approaches, for the most part, have been ad hoc solutions that were not designed for general-purpose use. Many issues involved in creating a reusable implementation that can be used in the wide variety of VR systems have not been addressed. This thesis discusses requirements for a character toolkit that can be used for many different purposes in VR applications, the design and implementation of it, problems encountered, and the experiences and knowledge gained from developing real applications with it.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97942,,Computer science,The vjAvatar library: a toolkit for development of avatars in virtual reality applications,thesis
G. M. Prabhu,"Wikstrom, Milton",2018-08-15T06:16:04.000,"<p>A crucial concern in software development is reducing program execution time. Parallel processing is often used to meet this goal. However, parallel processing efforts can lead to many pitfalls and problems. One such problem is to distribute the workload among processors in such a way that minimum execution time is obtained. The common approach is to use a load balancer to distribute equal or nearly equal quantities of workload on each processor. Unfortunately, this approach relies on a naive definition of load imbalance and often fails to achieve the desired goal. A more sophisticated definition should account for the affects of additional factors including communication delay costs, network contention, and architectural issues. Consideration of additional factors led us to the realization that optimal load distribution does not always result from equal load distribution. In this dissertation, we tackle the difficult problem of defining load imbalance. This is accomplished through the development of a parallel program model called the Generalized Work/Exchange Model. Associated with the model are equations for a restricted set of deterministically balanced programs that characterize idle time, elapsed time, and potential speedup. With the aid of the model, several common myths about load imbalance are exposed. A useful application called a load balancer enhancer is also presented which is applicable to the more general, quasi-static load unbalanced program.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/82923,Computer Sciences,Computer science,The work/exchange model: a generalized approach to dynamic load balancing,dissertation
N/A,"Nandakumar, Satyadev",2020-07-17T07:21:04.000,"<p>This thesis describes the tool support for the query component of the eXtensible Pattern Specification Language (XPSL). The XPSL framework is a part of the Knowledge-Centric Software (KCS) platform of tools for software analysis and transformation. XPSL provides a language for the specification of patterns. Currently, there is no tool support to perform software analysis and transformation patterns specified through XPSL. The objective of this research is to provide tool support for analysis. An analysis task is viewed by the tool as a query that can be executed to produce the appropriate results. The goal is to produce a tool which is extensible and easily maintainable. This thesis outlines the framework design of the query component of XPSL, wherein it is presented as a library of basic queries on patterns in code, together with a composition mechanism for writing queries of greater sophistication. The tool is implemented as a translator which takes an XPSL specification as input, and converts it into an equivalent query in a target language of choice. We consider XQuery and XSLT as possible target languages. We discuss the comparative merits and demerits of XSLT and XQuery as the target languages, and explain why our choice of XQuery as the target language is desirable. The pattern search is then done by an XQuery engine. The translation mechanism precisely defines of the semantics of execution of the query, and chooses the various data formats and the technologies for its stages. These are discussed in the thesis. We also do an empirical study of the efficacy and efficiency of the approach taken. Some queries which were executed demonstrate the fact that queries composed in XPSL and executed using the tool can go beyond what is possible in the current Aspect-Oriented Languages. We discuss the applicability of the tool to various software engineering paradigms. We also explore future extensions to the querying mechanism, and discuss the issues that may arise in adding a transformation component to the current framework.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/96884,,Computer science,The XPSL Query component: a framework for pattern searches in code,thesis
"Jack H. Lutz,James I. Lathrop","Patterson, Brian",2018-08-11T10:45:35.000,"<p>Our work is centered around topics where we provide a new model or approach to a well-known paradigm. We provide a new lens through which to view an area of research, providing access for new researchers and perspectives. After a brief orientation with common terms, we examine computation of real-valued sets, our general multi-resolution cellular automata (MRCA) simulator, how to prove languages are non-regular using Kolmogorov complexity, and how to show hidden variables are valuable in Bayesian networks.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/24426,Computer Sciences,"cellular automata,hidden variables,Kolmogorov complexity","Three topics in the theory of computing: Multi-resolution cellular automata, the Kolmogorov complexity characterization of regular languages, and hidden variables in Bayesian networks",dissertation
"Jannesari, Ali,Bao, Forrest Sheng,Aduri, Pavan","Yu, Sixing",N/A,"Deep neural networks (DNNs) have found widespread applications across many domains. However, deploying these models on devices with limited computational and storage capabilities, like mobile devices, poses significant challenges. Model compression, aiming to make these large models more efficient without significant performance loss, is an active research area. However, traditional model compression techniques often require expert knowledge and overlook the inherent structural information within DNNs. To address these challenges, this thesis proposes two novel techniques, Auto Graph Encoder-decoder Model Compression (AGMC) and Graph Neural Network with Reinforcement Learning (GNN-RL).

AGMC and GNN-RL harness the power of graph neural networks (GNNs) and reinforcement learning (RL) to extract structural information from DNNs, modeled as computational graphs, and automatically derive efficient compression policies. These policies are then used to guide the model compression process, resulting in compact yet effective DNNs.
AGMC combines a GNN-based DNN embedding mechanism with RL to learn and apply effective compression strategies. The results showcase the superiority of AGMC over traditional rule-based DNN embedding techniques, yielding improved performance and higher compression ratios. It outperforms both handcrafted and learning-based model compression approaches on over-parameterized and mobile-friendly DNNs. On over-parameterized DNNs like ResNet-56, our method surpasses previous state-of-the-art methods with higher accuracy. Furthermore, on compact DNNs like MobileNet-v2, AGMC achieves a higher compression ratio with minimal accuracy loss.
GNN-RL extends this work by introducing a novel multi-stage graph embedding technique to capture DNN topologies, along with RL to determine an optimal compression policy. The effectiveness of GNN-RL is demonstrated on a diverse set of DNNs, including the ResNet family, VGG-16, MobileNet-v1/v2, and ShuffleNet. GNN-RL achieved competitive results, providing higher compression ratios with less fine-tuning, significantly reducing the computational resources required while maintaining outstanding model performance. 
These methods pave the way for more automated and efficient model compression, enabling the deployment of complex DNNs on resource-constrained devices.",https://dr.lib.iastate.edu/handle/20.500.12876/EwpaeZDv,Computer science,"Artificial Intelligence,Machine Learning,Model Compression,Network Pruning",Topology-aware efficient and transferable model compression using graph representation and reinforcement learning,thesis
"Jannesari, Ali,Zhang, Wensheng,Cai, Ying","Mathur, Yamini",N/A,"In our current data-driven age, data has rapidly become an extremely powerful and almost indispensable asset across every field and industry. Tools like Apache Spark have been devised to accomplish the task of processing data by dividing them across different machines in a distributed manner. In Spark, multiple people can submit jobs simultaneously at any given instant. Furthermore, jobs can be submitted from a range of different sources, such as applications, people, or even cloud environments, because of the distributed architecture of Spark. Consequently, since various kinds of jobs can be arriving at any given instant from various environments, it is extremely important for data processing software like Apache Spark to constantly calibrate the allocation of resources and its performance during execution. The software has to ensure a fair share of computing resources for the execution of its different applications. At the same time, it also needs to ensure the overall performance is not impacted adversely. Consequently, an important challenge in data processing is to ensure that resources are allocated in a fair manner. To this end, there are numerous algorithms that have been proposed over time to optimize scheduling the submitted jobs.  Some of the popular heuristic methods include First In First Out(FIFO), Short Job First(SJF), Fair Scheduling, etc. However, most of the existing methodologies do not take into account the complex structure of each job before they make a decision for scheduling jobs. In this paper, we propose a methodology that takes in account the job as well as its dependency graph. Our methodology does not consider a collective of all the jobs at any given time. Our proposed methodology first embeds a node of the graph in conjunction with the dependent stage network. The dependent stage network is basically the components of the job that are dependent on the current node. Next, we pass the output through a Decision Network which is a neural network. The embeddings that have no dependency (schedulable nodes) are the ones that are passed through our Decision Network. The output of the Decision Network is a node that has to be scheduled at a given instant. We have achieved an 8% increase from the state of the art in terms of performance and a 9% decrease in the number of decisions made using the technique of Upside Down Reinforcement Learning(UDRL).",https://dr.lib.iastate.edu/handle/20.500.12876/3wxaeJev,Computer science,"Apache Spark,Graph Neural Networks,Graphinator,Job Scheduling,Reinforcement Learning,Upside Down Reinforcement Learning",Torgraphina: A scheduler for data processing during high-frequency job arrival using Upside Down Reinforcement Learning,thesis
N/A,"Brown, Walter",2018-08-15T15:38:10.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/80045,Computer Sciences,Computer science,Toward an optimizing compiler for a very high level language,dissertation
"Li, Qi,De Brabanter, Kris,Eulenstein, Oliver,Zhang, Zhu,Liu, Jia","Sabetpour, Nasim",N/A,"Having various sources to describe an object of interest may cause misinformation in real-world applications. This misinformation may come from conflicting sources (e.g., crowdsourcing platforms) or a reliable single source (e.g., fact verification). The conflict among different sources might be because of errors, out-of-date data, typos, missing records, etc. However, the misinformation from a single reliable source might be generated by a public figure like a politician or an individual user on their personal web page. Thus, identifying the true information is a challenge. To tackle this challenge, truth discovery, which aims to identify the truthfulness of an object, has emerged as a hot topic. There are two major approaches to truth discovery: 1- Truth discovery via aggregating multiple conflicting sources. 2- Truth discovery from a single source. The first approach applies aggregation to cancel out the source errors, and the latter does text reasoning to validate a statement using a reliable source.

We first adapt truth discovery aggregation to aggregate sequential labels and constituency parse trees. In the first scenario, we aim to aggregate sequence labels from crowds in the absence of ground truth. An optimization-based sequential label aggregation (AggSLC) method is proposed that infers the best set of annotation aggregated using labels provided by workers. Next, truth discovery is adapted to aggregate constituency parse trees, a common data structure for parsing sentences in multiple NLP applications. An optimization-based method (CPTAM) is presented to aggregate the constituency parse trees in the absence of ground truth. In our last work, our approach to do truth discovery is via a single source. We use textual data (e.g. Wikipedia articles) to verify the credibility of a claim in fact verification. The goal is to integrate textual corpus, to validate a given statement. We propose a joint Multi-Instance Learning (MIL) based model (FVMIL) which jointly performs the evidence classification and claim verification sub-tasks. Experiments conducted on various datasets show the effectiveness of the proposed strategies.",https://dr.lib.iastate.edu/handle/20.500.12876/jrl8W7Kr,Computer science,"Data Aggregation,Data Mining,Fact Verification,Machine Learning,Natural Language Processing,Truth Discovery",Toward complex data structure aggregation and truth discovery,dissertation
"Saifullah, Abusayeed,Jannesari, Ali,Zhang, Wensheng","Pivezhandi, Mohammad",N/A,"Parallel real-time energy-aware scheduling is a challenging open-ended problem dealing with distributing parallel tasks represented as directed acyclic graphs (DAGs) among multiple processing cores while maintaining energy efficiency and minimizing response times. The problem has been addressed with non-learning and learning-based heuristics, but little consideration has been given to individual core configurations and temperature behavior when allocating tasks to cores. Additionally, the proposed learnable approaches did not address reward calculation, tuning, or model selection strategies suitable for sample-efficient high-dimensional multi-core environment action spaces. Consequently, this research proposes a practical online learnable framework for estimating and adjusting the per-core frequency configuration and temperature at runtime. Our method uses a hierarchical multi-agent cooperative reinforcement learning model in which one agent observes the profiler data to determine the frequency and the number of cores, and another agent gives priority to the cores based on average and differences in temperature observation. A non-policy dueling double deep {\em Q}-network is used to train these two agents to avoid over-fitting and overestimation issues while maintaining the sample-taking efficiency, and the network is made as shallow as possible to reduce the latency overhead. The proposed approach is compared with all the available Linux governors and a Federated energy-aware scheduler based on important parameters such as response time, energy consumption, average temperature, etc. An OpenMP DAG benchmark was used as the basis for these experiments using Intel Xeon 12-core and Intel Core i7 4-core processors. The results of real-time experiments on the Intel Core i7 show up to 26\% improvement in energy consumption and 16\% improvement in execution time compared to all other governors, and in the Intel Xeon processor, we have more than $3^{\circ }C$ average temperature reduction compared to the state-of-the-art Linux governors.",https://dr.lib.iastate.edu/handle/20.500.12876/0zEyeV9z,Computer science,"directed acyclic graph,dueling double deep Q-network,energy- temperature aware scheduling,few-shot learning,hierarchical reinforcement learning,non-policy based learning",Toward real-time energy-aware automation of the resource scheduling using reinforcement learning,thesis
N/A,"Majeed, Femitha",2020-08-05T05:06:09.000,"<p>Securing a web server on an insecure operating system can often prove to be unsuccessful. This leads us to consider structuring an operating system architecture specially configured for a secure web server. The first half of the paper presents an analysis of some common attacks against a web server. In the second half, the paper focuses on ways to secure a web server. An essential phase in securing a web server consists of securing the operating system on which the server is run. This is important because compromising a flaw in the operating system might lead to an attack on the web server. Denial of Service (DOS) attack is one of the most common attacks that are aimed at the web server. It can be addressed to a large extent by using a proper resource control mechanism. We propose a security architecture design that integrates resource control and accountability into Mandatory Access Control (MAC) architecture. The implementation incorporates resource control into SELinux, which has MAC built into it. This is then integrated with Multi Agent Intrusion Detection System (MAIDS), which is a framework for an intrusion detection system that is modularly compatible with other detection systems. Integration with MAIDS is done to alert the system administrator whenever a DOS attack occurs. The MAIDS software will monitor the resource control mechanism to check whether a DOS attack has taken place or not. Finally, we present the design and implementation of a security tool that checks for configurations of the web server and the operating system on which it is run.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97523,,Computer science,Towards a secure web server,thesis
"Bao, Forrest Sheng,Basu, Samik,Tian, Jin,Zhang, Wensheng,Zheng, Mai","He, Youbiao",N/A,"Printed Circuit Boards (PCBs) serve as the foundation of electronic products, facilitating the physical and electrical integration of electronic components. PCB routing, a crucial step of design, computationally determines optimal paths for metal traces based on component placement and connectivity requirements. However, with advancing integrated circuit technology, the complexity of routing increases, leading to time-consuming and error-prone processes. Current PCB routing paradigms often separate routing into escape and area routing stages, but this isolation can result in suboptimal solutions. In addition, existing routing algorithms are typically specialized and lack adaptability to address evolving design needs. Moreover, the absence of standardized benchmarks impedes the assessment of new routing approaches, particularly those incorporating machine learning techniques. To tackle these challenges, this dissertation proposes novel routing algorithms and a comprehensive dataset of real-world PCB designs.

The research unfolds in three distinct parts. In the first part, we propose an end-to-end solution using Monte Carlo tree search (MCTS) and deep reinforcement learning (DRL). This approach employs a designed MCTS for circuit routing, guided by a deep reinforcement learning policy in the rollout. Notably, our method can be easily extended to routing cases with diverse routing constraints and optimization goals. Experimental results underscore the superiority of our approach, showcasing the highest success rates and minimized total wirelengths compared to traditional sequential A*-based routers on the test set.

In the second part, we introduce a pad-focused, net-by-net, two-stage PCB routing methodology. This comprises an MCTS-based global routing stage followed by an A*-based detailed routing stage. To minimize the gap between the proposed global and detailed routing, a polygon-based dynamic routable region partitioning mechanism is introduced to guarantee the existence of a detailed routing solution when a global routing solution exists. Our experiments demonstrate the outperformance of our approach against both state-of-the-art academic and non-academic routers, evident in terms of enhanced routability and reduced wirelength.

In the third part, we address the absence of standardized benchmarks in PCB routing by curating a dataset of community-endorsed, real-world, open-source PCB designs. This dataset, formatted in JSON, exclusively contains routing-related information represented through basic geometric objects and data structures. Notably, our dataset is 100 times larger than those used in recent PCB routing papers and is expandable through our versatile script supporting various input formats.
Additionally, we provide a highly adaptable reinforcement learning (RL) environment tailored for the development and testing of RL-based PCB routing algorithms. We anticipate that this dataset will bridge the gap between the ML and PCB communities, fostering research into automated circuit design using machine learning, including generative PCB routing.",https://dr.lib.iastate.edu/handle/20.500.12876/OrD865jr,"Computer science,Artificial intelligence","Dataset,Machine Learning,Monte Carlo tree search,Printed circuit board,Reinforcement learning,Routing",Towards automated PCB routing: Leveraging machine learning and heuristic techniques,dissertation
"Le, Wei,Tian, Jin,Quinn, Christopher","Rahman, Md Mahbubur",N/A,"Deep learning vulnerability detection has shown promising results in recent years. However, an important challenge that still blocks it from being very useful in practice is that the model is not robust under perturbation and it cannot generalize well over the out-of-distribution (OOD) data, e.g., applying a trained model to unseen projects in real world. We hypothesize that this is because the model learned non-robust features, e.g., variable names, that have spurious correlations with labels. When the perturbed and OOD datasets no longer have the same spurious features, the model prediction fails. To address the challenge, in this work, we introduced causality into deep learning vulnerability detection. Our approach CausalVul consists of two phases. First, we designed novel perturbations to discover spurious features that the model may use to make predictions. Second, we applied the causal learning algorithms, specifically, do-calculus, on top of existing deep learning models to systematically remove the use of spurious features and thus promote causal based prediction. Our results show that CausalVul consistently improved the model accuracy, robustness and OOD performance for all the state-of-the-art models and datasets we experimented. To the best of our knowledge, this is the first work that introduces do calculus based causal learning to software engineering models and shows it’s indeed useful for improving the model accuracy, robustness and generalization. Our replication package is located at https://figshare.com/s/0ffda320dcb96c249ef2.",https://dr.lib.iastate.edu/handle/20.500.12876/gwW7o9jw,Artificial intelligence,"Causality,Deep Learning,Machine Learning,Software Engineering,Spurious Features,Vulnerability Detection",Towards causal deep learning for vulnerability detection,thesis
"Rajan, Hridesh,Reecy, James,Basu, Samik,Fernandez-Baca, David,Huang, Xiaoqiu","Bagheri, Hamid",N/A,"As the cost of sequencing decreases, the amount of data being deposited into public repositories is
increasing rapidly. As sequencing data continues to accumulate in the online repositories, scientists can
increasingly use multi-tiered data to better answer biological questions. One main challenge that the public
biological repositories have is the problem of data quality of the metadata. Unfortunately, most public
databases do not have methods for identifying errors in their metadata, leading to the potential for error
propagation.
In order to do the cleaning at the large scale, scalable infrastructure and algorithms are needed to be
developed. In this dissertation, we built a domain-specific language and large-scale infrastructure, called BoaG,
to analyze the wealth of genomics data. We used the BoaG’s interface to reason about the provenance,
frequencies, and quality of annotations.
The second part of the dissertation focuses on the cleaning of the public repositories at scale. Most public
databases, such as non-redundant (NR), rely on user input and do not have methods for identifying errors in
the provided metadata, leading to the potential for error propagation. Previous research on a small subset
of the NR database analyzed misclassification based on sequence similarity. To the best of our knowledge,
the amount of taxonomic misclassification in the entire database has not been quantified. We proposed and
developed an automatic approach to detect and remove the suspicious taxonomic assignments and mispredicted
functional annotations.
We also addressed widely used sequence clustering information of the public databases. The usefulness
of clusters to explore different biological analyses has been shown for functional annotation, family classification,
systems biology, structural genomics, and phylogenetic analysis [73]. We utilized CD-HIT [33] to
cluster NR sequences at different similarity levels, i.e. 95%, 90%, 85%, down to 65%. To improve the data
quality of the clusters, we removed anomalies and then provided a confidence score based on the lineage of
all sequences within each cluster.
For the functional annotations, we utilized protein ontology (PRO) [58] and Gene Ontology [11] that
are knowledge-based graphs to detect potentially mispredicted functions. Ontologies have been utilized
to express knowledge. In this dissertation, we leveraged them to improve the quality of the public genomics
databases. We proposed a computational method that abstracts ontology graphs into a lower-dimensional
network representation that makes reasoning for inconsistencies among the list of functional annotations
easier.
We found that the BoaG infrastructure provided fewer lines of code, reduced storage size, and provided
automatic parallelization for the large-scale analyses on the NR dataset. The BoaG’s web-interface is also
implemented and is made publicly available for researchers to test different hypotheses and share them
among others. We have identified “29,175,336"" proteins in the NR database that have more than one distinct
taxonomic assignments, among which “2,238,230"" (7.6%) are potentially taxonomically misclassified. We
also found that the total number of potential misclassifications in clusters at 95% similarity, above the
genus level, is “3,689,089"" out of 88M clusters, which are 4% of the total clusters. This percentage of
misclassifications in NR has a significant impact due to the potential for error propagation in the downstream
analysis. This method proposed in this dissertation will be a valuable tool in cleaning up large-scale public
databases. The technique we proposed could be extended to address other kinds of annotation errors of the
public databases at scale.",https://dr.lib.iastate.edu/handle/20.500.12876/dvmqEmbv,"Computer science,Bioinformatics","Data Cleaning,Domain-Specific Language,Non-Redundant Protein,Taxonomic Assignments",Towards data cleaning in large public biological databases,dissertation
Robyn R. Lutz,"Xu, Yijia",2018-08-11T18:05:29.000,"<p>In software development, testing often takes more than half the total development time (Pan 1999). Test case design and execution of test procedures consume most of the testing time. Thus, automatically generating test cases and automatically detecting errors in test procedures prior to execution is highly advantageous. This thesis proposes a new approach to further automate test case design and the test procedure development process.</p>
<p>Several open-source products exist to automate test case design, but they have limitations including test cases that do not trace back to models; test cases that are not reusable for libraries; and limiting test cases to generation on their own test environment. This limits their support for the important, new avionics standard, DO-178C (RTCA 2012).</p>
<p>The first contribution of the thesis is a technique for test code generation that, compared to existing products, is faster, provides improved traceability to models, and supports reusable test procedures that can be generated on any testing environment. To address the current limitations, the new approach utilizes the Simulink Design Verifier and an open-source constraint solver to generate test cases. The technique allows each test case to be traced back to an expression and to the original model.</p>
<p>Detecting errors in manually written test procedures before testing starts is also critical to efficient verification. It can save hours or even days if errors are detected in the early test procedure design stage. However, analysis done here of a set of open source code analysis tools shows that they cannot detect type and attribute errors effectively.</p>
<p>The second contribution of the thesis is to develop a static code analyzer for Python code that detects bugs that could cause automated test procedures to crash. The analyzer converts a Python code to an abstract syntax tree and detects all type and attribute errors by performing a type-flow analysis. This approach provides improved accuracy over existing products.</p>
<p>Together, these two contributions, a test code generator with improved traceability and reusability, and a static code analyzer capable of handling more error types, can improve test process compatibility with DO-178C.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28925,Computer Sciences,Computer Science,Towards DO-178C compatible tool design,thesis
"Bao, Sheng,Li, Qi,Zhang, Wensheng","Luo, Ge",N/A,"Evaluating machine-generated summaries without a human-written reference summary has been a need for a long time. In this paper, we present a proof-of-concept study to a summary evaluation approach without the presence of reference summaries. By negative sampling, massive data in existing summarization datasets are transformed for training by pairing documents with corrupted reference summaries. Two learning schemes are explored: weakly supervised learning with explicit number labels and preference learning with inexplicit labels. Extensive experiments on several datasets show that our approaches can produce scores highly correlated with human ratings.",https://dr.lib.iastate.edu/handle/20.500.12876/EzR2ykgz,Computer science,"evaluation,metric,summarization",Towards end-to-end reference-free summarization evaluation via negative sampling,thesis
"Tavanapong, Wallapak,Lutz, Robyn R,Li, Qi","Mohammed, Abdurahman Ali",N/A,"Accurate cell counting in biomedical imaging is crucial for numerous clinical applications, such as disease diagnosis and treatment efficacy evaluation. Despite advancements in deep learning, the interpretability of these models in this domain is challenging. This thesis introduces CountXplain, a novel post-hoc interpretation method designed for density map estimation models in cell counting. CountXplain uniquely interprets feature maps from pre-trained models, offering insights into their decision-making processes without compromising accuracy. Unlike existing interpretability methods focused on image classification, CountXplain caters specifically to density map estimation, a key technique in cell counting. It differs from recent concept-based interpretation methods for regression models, which do not directly suit density map estimation. 

The effectiveness of CountXplain was validated on two diverse, publicly available cell counting datasets: VGG and DCC. These datasets were chosen to demonstrate the method's applicability across different cell-counting scenarios. In addition, CountXplain's performance was evaluated using both the CSRNet and FCRN-A architectures to ensure broad applicability.

To quantitatively validate the effectiveness of the identified prototypes, a one-tailed sign test was used, checking whether these prototypes have effectively learned relevant features. Furthermore, a comprehensive visual analysis was conducted for qualitative evaluation. This involved examining the prototypes' alignment with key features in cell counting. These rigorous evaluations underscore CountXplain's potential to enhance the interpretability of deep learning models in biomedical imaging, aiding researchers and practitioners.

In conclusion, CountXplain addresses a significant gap in the interpretability of density map estimation models for cell counting. By providing detailed insights into the decision-making processes of these models, it fosters greater trust and transparency, crucial for clinical and research applications in biomedical imaging.",https://dr.lib.iastate.edu/handle/20.500.12876/PrMBma3z,"Computer science,Artificial intelligence","Artificial Intelligence,Cell Counting,Deep Learning,Explainable AI",Towards interpretable density map-based cell counting models,thesis
N/A,"Jing, Yaping",2020-07-29T14:11:19.000,"<p>Design patterns encapsulate accumulated software design knowledge and implementation experience. While they exist, in general, as informal guidelines that describe reusable design solutions to recurring problems in software development, they are not available as code abstractions and thus not ready to be incorporated into a concrete system implementation. In order to propagate the design patterns' reusability from design phase to implementation phase, this work presents an approach that differentiates between code-reusable and non-code-reusable design patterns. Using this classification, we develop a method that allows for representing code-reusable design patterns as reusable design components. These design components, when applied to application-specific design participants via well-defined protocols and/or interfaces, output concrete pattern-specific code ready to be incorporated into concrete system implementations. Through experimentation on GoF's design pattern catalog, we have identified 12 design patterns that can be classified as code-reusable and promoted them to reusable design components. These design components, although implemented in Java, exhibit very similar properties as shown in Hannemann and Kiczales' AspectJ implementation of design patterns in terms of reusability and pluggability.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/96932,,Computer science,Towards the representation of design patterns as design components,thesis
"Stephen Gilbert,Carl Chang","Sulaiman, Adel",2019-11-04T21:58:44.000,"<p>Virtual reality (VR) training has been applied to multiple different tasks and compared to conventional training methods. The aim of this research is to evaluate training in a large-scale CAVE-based virtual environment (VE) for a location-based mobile application designed to support an address verification task via walking. This training required a simulated neighborhood environment, a user interface for VE navigation, and communication between the VE and the mobile application. In this research, four main elements interacted together to provide the user experience: the VE, the mobile application, the task, and the user’s spatial ability.</p>
<p>The first part of the research examined the impact of training in VR. The training was applied in two different environments with two different groups (in the field and VR). The effectiveness of training was measured using a field test for both training groups. There were statistically significant improvements in both training groups after training. There were no significant differences between the two training groups in performance (time, distance, task errors) in the testing session.</p>
<p>The second part evaluated the interaction with the mobile application by assessing the impact of training on the use of the mobile application and the usability issues experienced with the app. Usability was measured using quantitative (taps in the application) and qualitative (usability questionnaire) methods. Results showed statistically significant improvements in app interaction in both real-world and virtual training groups where both groups reduced taps significantly in the testing session compared to the training session. There were no significant differences between the two training groups in taps in the testing session. The usability questionnaire documented issues related to feedback and map design. However, the questionnaire results showed an overall satisfaction of the usefulness and the information quality of the mobile app.</p>
<p>The last part of the study evaluated the interaction with the virtual environment by examining users’ sense of presence and perceived distance traveled. The sense of presence was measured using a presence questionnaire. The perceived distance compared participants’ perceived travel distance with their actual distance traveled. Results showed that participants had a relatively high sense of presence in the CAVE. It was also found that participants underperceived their distance traveled in VR.</p>
<p>This research found the large-scale CAVE-based virtual environment (VE) valuable in training and evaluation for the location-based mobile application designed to support an address verification task. These results will enable users from both academia and industry with location-based mobile applications to apply sufficient training in the context in the large-scale CAVE-based virtual environment (VE). Findings in this research enable researchers and practitioners in user experience (UX) to apply valuable evaluations of usability and interaction for location-based mobile applications using a large-scale CAVE-based virtual environment (VE).</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31756,Computer Sciences,"Computer Science,Human-Computer Interaction,Training,User experience,Virtual Environment,Virtual Reality",Training and evaluation in a large-scale virtual environment for a location-based mobile application,dissertation
"Cai, Ying,Tavanapong, Wallapak,Zhang, Wensheng","Idris, Azeez",N/A,"Supervised deep learning image classifiers continue to achieve human-level performance in various domains partly due to large labeled image datasets and increased computing power. However, a persistent problem of training supervised deep learning image classifiers is the limited availability of domain-specific annotated image datasets. This problem is particularly important in areas such as medicine due to the rarity of certain diseases and the cost and expertise needed for data annotation. Data augmentation, transfer learning, few-shot learning, and improved training and scaling strategies are some approaches proposed to solve this challenge.  

This thesis proposes a new training strategy that consists of four steps to reduce model misclassification. 1) The first step uses weakly supervised learning to understand which classes are most confused with each other. 2) A new class containing synthetic training data highlighting the class confusion is introduced. This class is called the not-sure class. Random Cropping (RCP) and Equal MixUp (EQM) are two methods used for creating the synthetic training data for the not-sure class. These synthetic training data are created using pairs of images from different classes. The not-sure algorithm is introduced to determine which pair of classes to consider. 3) The model is trained with the existing limited and new synthetic training data. 4) Lastly, the final model is fine-tuned using transfer learning.  

We tested our new training strategy using open medical and non-medical datasets. The proposed training strategy improves classification accuracy, precision, recall, and f-1 score. Our experimental results show that the RCP method improved classification performance for most datasets while the EQM method works better for some cases. The model improvements are 0.6% to 5.6% for the medical datasets and 3.9% to 4.4% for the non-medical datasets across the evaluated performance metrics. 

The ablation study reveals the impact of various components of the proposed training strategy, including using data augmentation during training and using not-sure data synthesized randomly rather than created by the not-sure algorithm. We find that data augmentation improves classification performance, and using the not-sure algorithm offers the best model performance when the image resolutions are not small.",https://dr.lib.iastate.edu/handle/20.500.12876/3wxak4gv,Computer science,"Deep Learning,Limited Labeled Data",Training Strategy for Limited Labeled Data by Learning from Confusion,thesis
Johnny Wong,"Kumar, Muruganandan",2018-08-17T13:44:00.000,"<p>The main issues that need to be resolved before applying transaction concepts to advanced applications such as CAD/CAM are a transaction model with a suitable control mechanism, and a new data model;A complex object is modeled as an hierarchy of interfaces and an implementation. This structuring facilitates storage of multiple object version and their subsequent reuse in design databases. Other concepts needed for data modeling in design databases are implementation and instantiation which are supported by our model. One of the main issues in design environment is change propagation. Two techniques--parametric instantiation and conditional instantiation--are provided that will alleviate the problems associated with change propagation;The design transactions are composed of subtransactions which are managed by the site where the project transactions are executed. While access to the design objects are controlled by the access protocol based on locking, they are not necessarily two-phased--only the access to the composite object hierarchy are serialized by following a tree protocol. The design database manages and controls the top-level project transactions. Hypothetical transaction, allows designers to explore alternate designs that may not be appropriate for future use. Because it is envisioned that most of the transactions in design environments will be transactions of this nature, support for the hypothetical transaction is provided with little overhead. A mechanism for upgrading a hypothetical transaction to a regular transaction is also developed;The project transaction can be decomposed into a hierarchy of subtransactions that model the behavior of design environments more closely. Independent commit transactions are introduced to reduce the cost of aborting long transactions; communicating interfaces between parent and child to capture the effect of multiple request-responses observed between users and application. Client-subcontractor relationship models the behavior of a user transaction that can invoke multiple applications to work on the same data set concurrently. The protocols needed for maintaining the database consistency are also presented along with the proof of correctness;The proposed transaction mechanisms and the data model will support design databases that are used to store structural data.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/82650,Computer Sciences,Computer science,Transaction and data models for design databases,dissertation
Johnny Wong,"ul Haque, Waqar",2018-08-23T10:00:36.000,"<p>Scheduling transactions in a real-time database requires an integrated approach in which the schedule does not only guarantee execution before the deadline, but also maintains data consistency. The problem has been studied under a common framework which considers both concurrency control issues and the real-time constraints in centralized and distributed transaction processing. A real-time transaction processing model has been defined for a centralized system. The proposed protocols use a unified approach to maximize concurrency while meeting real-time constraints at the same time. In order to test the behavior of the model and the proposed protocols, a real-time transaction processing testbed has been developed using discrete event simulation techniques. The results indicate that different protocols work better under different load scenarios and that the overall performance can be significantly enhanced by modifying the underlying system configuration. Among other system and transaction parameters, the effect of data partitioning, buffer management, preemption, disk contention, locking mode and multiprocessing has been studied;For the distributed environment, new concepts of real-time nested transactions and priority propagation have been proposed. Real-time nested transactions incorporate the deadline requirements in the hierarchical structure of nested transactions. Priority propagation addresses the issues related to transaction aborts in real-time nested transaction processing. The notion of priority ceiling has been used to avoid the priority inversion problem. The proposed protocols exhibit freedom from deadlock and have tightly bounded waiting period. Both of these properties make them very suitable for distributed real-time transaction processing environment.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/63589,Computer Sciences,Computer science,Transaction processing in real-time database systems,dissertation
"Jannesari, Ali,Lu, Chaoqun,Lutz, Robyn","Sarkar, Aishwarya",N/A,"Recent advances in remote sensing technologies have led to an explosive growth of geo-spatiotemporal data in fields like geology, ecology, hydrology, and astronomy. To effectively process this massive and complex data, scientists and researchers rely heavily on deep learning models. However, these models face two significant challenges: first, their purely data-driven approach often leads to sub-optimal performance due to the lack of domain knowledge about the system that generated the data. Second, the spatiotemporal features of these datasets vary significantly with location, requiring repetitive training of models for almost every new region and increasing the computational resources required to train them. In this thesis, we propose HydroDeep, a knowledge-guided deep neural network architecture that integrates a domain (process-based) model with deep convolutional neural networks (CNN) and long short-term memory (LSTM) networks to learn effectively from grid-based geo-spatiotemporal data. The results demonstrate that HydroDeep outperforms data-driven CNNs and LSTMs by 1.6% and 10.5%, respectively, in predicting complex spatiotemporal features. Four transfer learning approaches are also proposed that effectively transfer knowledge from one region to another, resulting in performance improvements of 9% to 108% in new regions with a 95\% reduction in training time. Finally, the merit of transfer learning as a tool for discovering and analyzing spatiotemporal variance between regions is investigated. Flood prediction, one of the most challenging problems involving complex geo-spatiotemporal data, is used as a use case. The findings have important implications for developing deep learning models that can better leverage domain knowledge and efficiently transfer knowledge across different regions.",https://dr.lib.iastate.edu/handle/20.500.12876/JwjbeZ4w,Computer science,"convolutional neural network,knowledge-guided deep learning,long short-term memory network,spatiotemporal,streamflow prediction,transfer learning",Transfer learning approaches for knowledge discovery in grid-based geo-spatiotemporal data,thesis
N/A,"Fuller, Douglas",2020-11-13T21:21:00.000,"<p>This thesis argues that a modular, source-to-source translation system for distributed-shared memory programming models would be beneficial to the high-performance computing community. It goes on to present a proof-of-concept example in detail, translating between Global Arrays (GA) and Unified Parallel C (UPC). Some useful extensions to UPC are discussed, along with how they are implemented in the proof-of-concept translator.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97913,,Computer science,Translation techniques for distributed-shared memory programming models,thesis
"Jannesari, Ali,Sheidaei, Azadeh,Gao, Hongyang","Hashemi, Mohammad Saber",N/A,"Degradation of bone, especially for astronauts exposed to microgravity conditions, is crucial for space exploration missions since the lower applied external forces accelerate the diminution in bone stiffness and strength substantially. Even though existing computational models and simulations help us understand this phenomenon and possibly restrict its effect in the future, they are time-consuming to simulate the changes in the bones, not just the bone microstructures, of each individual in detail. In this study, a robust yet fast computational method to predict and visualize bone degradation has been developed. The deep-learning part of it, TransVNet, can take in different 3D voxelized images and predict their evolution throughout months utilizing a hybrid 3D-CNN-VisionTransformer autoencoder architecture. Because of the limited available experimental data and the challenges of obtaining new samples, a digital twin dataset of diverse and initial bone-like microstructures was generated to train our TransVNet on the evolution of the 3D images through our previously developed degradation model for microgravity. The preliminary results show its satisfactory and high performance on the bone degradation dataset. Nevertheless, the presented AI framework can be used and customized for accelerating and generalizing more sophisticated degradation models or similar 3D image sequence prediction tasks.",https://dr.lib.iastate.edu/handle/20.500.12876/azJ4x9Kv,"Computer science,Artificial intelligence,Materials Science","AI-Accelerated Computational Physics,Bone Degradation and Microstructure Evolution,Cellular Microstructure Generator,Deep Learning,TransVNet,Vision Transformer",TransVNet: Predicting bone degradation using ViT and virtual dataset of cellular microstructures,thesis
"Liu, Jia,Rajan, Hridesh,Aduri, Pavankumar,Li, Qi,Zhang, Wensheng","Tian, Ye",N/A,"In recent years, ride-sharing systems have emerged as one of the quintessential examples of sharing economy that can effectively leverage excessive and under-utilized vehicle resources to address many challenges in modern transportation systems (e.g., CO$_2$ emissions, growing fuel prices) and achieve ``triple-win'' between the riders, enlisted drivers, and the ride-sharing platform.
Lying at the heart of most ride-sharing systems is the problem of joint trip-vehicle matching and routing optimization, which is highly challenging and results in this area remain rather limited. In this thesis, we studied both planning setting and real-time setting for ride-sharing problems. 

In the planning setting, available drivers and rider demands are given beforehand. We proposed an analytical framework to state the problem and reformulate it as a mixed-integer linear program, which can be solved by global optimization methods. 
To efficiently solve large-sized problem instances, we developed a memory-augmented time-expansion (MATE) approach which leverages the special problem structure to facilitate approximate (or even exact) algorithm designs. 

In the real-time setting, rider demands are not available in advance. We proposed a reinforcement learning formulation for ride-sharing that jointly optimizes the rewards and experiences from the perspectives of the drivers and riders, respectively. 
Then we developed a reactive model-free deep RL approach based on proximal policy optimization (PPO) to solve the joint trip-vehicle matching and routing optimization problem.
Finally, we conduct extensive simulations to analyze and verify the performance of our ride-sharing system using real-world datasets. 
Simulations results show that the proposed framework outperforms a greedy and the receding horizon control (RHC) algorithms under all testing demand patterns.",https://dr.lib.iastate.edu/handle/20.500.12876/OrD8EJor,Computer science,"Approximation Algorithm,Deep Reinforcement Learning,Ride-sharing,Trip-vehicle matching",Trip-vehicle matching and vehicle routing optimization for ride-sharing,dissertation
"Li, Qi,Eulenstein, Oliver,Zhang, Wensheng","Kulkarni, Adithya",N/A,"Heterogeneous data refers to the data with high variability of data types and formats. This data is acquired through different platforms which can follow different acquisition guidelines. Moreover, the information is collected at a large scale which can result in low quality, missing values, data redundancy, untruthfulness, etc. Different sources can have different interpretations of the data resulting in differences. Furthermore, sources can have varying reliabilities making it challenging to infer true labels. The challenge of identifying source reliabilities can be handled using the truth discovery idea, which uses optimization techniques to infer the true labels and source reliabilities. The focus of existing literature has been on binary, multi-class single label, and multi-class multi-label problems and takes no notice of sequential labels. Referring to the data types, existing literature focuses on numerical and categorical data and takes no notice of tree structure. This report focuses on true label inference from heterogeneous data sources for sequential labeling tasks and also on the tree structure. The truth discovery idea is first adapted to infer structure and label for tree structure; we refer to constituency parse trees, a common data structure for parsing sentences in multiple Natural Language Processing applications, for the purpose. Next, we adapt the truth discovery idea to infer true sequential labels. For both approaches, the true labels are inferred in the absence of ground truth by formulating an optimization framework such that the inferred labels that minimize the objective function are expected to be near to the true labels.",https://dr.lib.iastate.edu/handle/20.500.12876/avVO90dr,Computer science,"Data Mining,Heterogeneous Data Sources,Machine Learning,Natural Language Processing,Truth Discovery,Truth Inference",True label inference from heterogeneous data sources in Natural Language Processing,thesis
N/A,"Lu, Dingding",2020-07-17T07:19:44.000,"<p>Currently many safety-critical systems are being built. Safety-critical systems are those software systems where a single failure or hazard may cause catastrophic consequences. Therefore, safety is a property which must be satisfied for safety-critical systems. This research develops techniques to address two areas of software safety analysis in which structured methodologies have been lacking. The first contribution of the paper is to define a top-down, tree-based analysis technique, the Fault Contribution Tree Analysis (FCTA), that operates on the results of a product-family domain analysis. This paper then describes a method by which the FCTA of a product family can serve as a reusable asset in the building of new members of the family. Specifically, we describe both the construction of the fault contribution tree for a product family (domain engineering) and the reuse of the appropriately pruned fault contribution tree for the analysis of a new member of the product family (application engineering). The second contribution of the paper is to develop an analysis process which combines the different perspectives of system decomposition with hazard analysis methods to identify the safety-related scenarios. The derived safety-related scenarios are the detailed instantiations of system safety requirements that serve as input to future software architectural evaluation. The paper illustrates the two techniques with examples from applications to two product families in Chapter One and to a safety-critical system in Chapter Two.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/96859,,Computer science,Two techniques for software safety analysis,thesis
N/A,"Roeder, Robert",2018-08-15T08:38:35.000,N/A,https://dr.lib.iastate.edu/handle/20.500.12876/80099,Computer Sciences,Computer science,Type determination in an optimizing compiler for APL,dissertation
"Vasant Honavar,David-Fernandez Baca,Samik Basu","Vasile, Flavian",2018-08-23T00:28:52.000,"<p>Folksonomies - shared vocabularies generated by users through collective annotation (tagging) of web-based content, which are formally hypergraphs connecting users, tags and objects, are beginning to play an increasingly important role in social media. Effective use of folksonomies for organizing and locating web content, discovering and organizing user communities in order to facilitate the contact and collaboration between users who share parts of their interests and attitudes calls for effective methods for discovering coherent groupings of users, objects, and tags. We empirically compare the results of several folksonomy clustering methods using tensor decompositions such as PARAFAC, Tucker3 and HOSVD which are generalizations of principal component analysis and singular value decomposition with standard methods that use 2-dimensional projections of the original 3-way relationships. Our results suggest that the proposed methods overcome some of the limitations of 2-way decomposition methods in clustering folksonomies.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/68497,Computer Sciences,Computer science,Uncovering the structure of hypergraphs through tensor decomposition: an application to folksonomy analysis,thesis
Leslie Miller,"Meng, Tianyu",2018-08-11T08:55:07.000,"<p>As mobile devices have become more and more popular, we are seeing more location-based applications on handheld devices. Since the screen size of a handheld device is very limited, using a map on such kind of devices can be quite difficult. To make these applications easier to use, we want to use a situation model to provide user a better service. In this paper, we analyze user map operations and try to identify some useful patterns that have the potential to result in a significant saving of some unnecessary user operations when used in a situation-based system. Two user studies which are quite different are involved in this research. One is used to understand patterns and another one is used for validation. The result shows that the patterns identified in first user study are still valid in the second user study.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/28713,Computer Sciences,"Computer Science,Location-Based Software,Mobile devices,Pattern,Situation System,SITU System,Understanding and Identify Patterns",Understanding and identifying useful patterns in location-based software,thesis
"Le, Wei,Gao, Hongyang,Liu, Hailiang,Quinn, Christopher J,Wang, Zhengdao","Kloberdanz, Eliska",N/A,"Deep learning (DL) has become an integral part of solutions to various important problems, which is why ensuring the quality of DL systems is essential. One of the challenges of achieving reliability and robustness of DL software is to ensure that algorithm implementations are numerically stable. Numerical stability is a property of numerical algorithms, which governs how changes or errors introduced through inputs or during execution affect the accuracy of algorithm outputs. In numerically unstable algorithms, those errors are magnified and adversely affect the fidelity of algorithm’s outputs via incorrect or inaccurate results. In this thesis we analyze the numerical stability of DL algorithms to better understand and improve numerical stability of DL algorithms. First, we identify and analyze unstable numerical methods and their solutions in DL. Second, we learn assertions on inputs into DL functions that ensure their numerical stability. Third, we focus on neural network quantization, which we found to cause numerical stability issues. Specifically, we propose a new quantization algorithm that optimizes the trade-off between low-bit representation and loss of precision. Next, we focus on analyzing the numerical stability of residual networks by leveraging their dynamic systems interpretation. In particular, we propose that residual networks behave as stiff numerically unstable ordinary differential equations. Finally, we introduce a novel numerically stable solver for neural ordinary differential equations.",https://dr.lib.iastate.edu/handle/20.500.12876/kv7kJRpv,Computer science,"artificial intelligence,deep learning,neural networks,numerical analysis,numerical stability,robustness",Understanding and improving numerical stability of deep learning algorithms,dissertation
"Rajan, Hridesh,Le, Wei,Miner, Andrew,Mitra, Simanta,Liu, Jia","Biswas, Sumon",N/A,"Machine learning (ML) algorithms are increasingly being used in critical decision making software such as criminal sentencing, hiring employees, approving bank loans, college admission systems, which affect human lives directly. Algorithmic fairness of these ML based software has become a major concern in the recent past. Many incidents have been reported where ML models discriminated people based on their protected attributes e.g., race, sex, age, religious belief, etc. Research has been conducted to test and mitigate unfairness in ML models. However, there is a large gap between the theory of ML fairness and how the property can be ensured in practice. Similar to analyzing traditional software defects, fairness has to be engineered in ML software to minimize and eventually guarantee bias-free decisions. In this dissertation, we are the first to introduce compositional reasoning of group fairness in ML pipeline and propose individual fairness verification technique for neural networks. Towards that goal, first, we conducted a large-scale empirical study to understand unfairness issues in open-source ML models. A number of definitions of algorithmic fairness have been proposed in the literature and many bias mitigation techniques have been proposed. Group fairness property ensures that the protected groups (e.g., male-vs-female, young-vs-old, etc.) get similar treatment in the prediction. On the other hand, individual fairness states that any two similar individuals should be predicted similarly irrespective of their protected attributes. Often an accuracy-fairness tradeoff is experienced when a mitigation algorithm is applied. We evaluated fairness of models collected from Kaggle and investigated their root causes, compared the performance of mitigation algorithms and their impacts on accuracy. The study suggests many algorithmic components in ML pipeline as the root cause of bias, which leads to our work on compositional reasoning of fairness.

For ML tasks, it is a common practice to build a pipeline that includes an ordered set of stages from acquisition, to preprocessing, to modeling, and so on. However, no research has been conducted to measure fairness of a specific stage or data transformer operators in the pipeline. The existing metrics measure the fairness of the pipeline holistically. We proposed causal reasoning in ML pipeline to measure and instrument fairness of data preprocessing stages. We leveraged existing metrics to define component-specific fairness and localize fairness issues in the pipeline. We also showed how the local fairness of a preprocessing stage composes in the global fairness of the pipeline. In addition, we used the fairness composition to choose appropriate downstream transformer that mitigates unfairness. Although we could identify and localize unfairness in the ML model, providing formal guarantees of fairness is challenging because of the complex decision-making process. Therefore, we proposed Fairify, an approach to verify individual fairness property in neural networks (NN). Fairify leverages white-box access and neural pruning to provide certification or counterexample. The key idea is that many neurons in the NN always remain inactive for certain  smaller parts of the input domain. So, Fairify applies input partitioning and then prunes the NN for each partition to make them amenable to verification. In this work, we proposed the first SMT-based fairness verification that can answer targeted fairness queries with relaxations as well as provide counterexamples.",https://dr.lib.iastate.edu/handle/20.500.12876/qzXB1NMv,Computer science,"algorithmic fairness,causal reasoning,machine learning,software engineering,verification",Understanding and reasoning fairness in machine learning pipelines,dissertation
James Lathrop,"Doty, Karen",2020-06-26T20:08:48.000,"<p>Augmented and mixed reality experiences are increasingly accessible due to advances in technology in both professional and daily settings. Technology continues to evolve into multiple different forms, including tablet experiences in the form of augmented reality (AR) and mixed reality (MR) using wearable heads-up displays (HUDs). Currently, standards for best usability practices continue to evolve for MR HUD two-dimensional user interfaces (2D UI) and three-dimensional user interfaces (3D UI). Therefore, research on evolving usability practices will serve as guidance for future development of MR HUD applications.</p>
<p>The objective of this dissertation is to understand what gestures users intuitively make to respond to a MR environment while wearing a HUD. The Microsoft HoloLens is a wearable HUD that can be used for MR. The Microsoft HoloLens contains two core gestures that were developed to interact with holographic interfaces in MR. Although current gestures can be learned to generate successful outcomes, this dissertation provides a better understanding of which gestures are intuitive to new users of a MR environment.</p>
<p>To understand which gestures are intuitive to users, 74 participants without any experience with MR attempted to make gestures within a wearable MR HUD environment. The results of this study show that previous technology experience can influence gesture choice; however, gesture choice also depends on the goal of the interaction scenario. Results suggest that a greater number of programmed gestures are needed in order to best utilize all tools available in wearable HUDs in MR. Results of this dissertation suggest that five new gestures should be created, with three of these gestures serving to reflect a connection between MR interaction and current gesture-based technology. Additionally, results suggest that two new gestures should be created that reflect a connection between gestures for MR and daily movements in the physical world space.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/32263,,"Augmented,Gestures,Holograms,Technology,Virtual,Wearable",Understanding intuitive gestures in wearable mixed reality environments,thesis
Lu Ruan,"Kabala, Jinu",2020-09-23T19:13:07.000,"<p>Autonomous Systems (AS) in the Internet use BGP to perform inter-domain routing.  A set of import and export policies at an AS make up the routing table of an AS. Since AS relationships are not publicly available, several studies have proposed heuristic algorithms for inferring AS relationships using publicly available BGP data. Content Delivery Network (CDN) servers placed around the world cater to the needs of clients that access their content. Since, the majority of the Internet traffic today is content delivery traffic, it is important to study the efficiency of the routing paths from users to content servers which are not under the control of content providers. Netflix and Akamai are two major CDN providers. The user experience depends upon the performance of CDN servers. Hence, it is important for CDNs to choose the ideal server when a user requests content from its network. Due to lack of authentication of routes in BGP, prefixes are prone to being hijacked by ASes to which the prefixes do not belong. The mechanisms used to address this is to detect the hijack after it has happened and react to it. A more preventive mechanism is necessary to prevent it from happening in the first place.  A recent work proposed a list of serial hijackers that would enable such a solution. Unfortunately, the ground truth of serial hijackers is very small.</p>
<p>We try to understand the Internet AS Topology using BGP routing data received from neighbors of an AS. We present a machine learning approach to edge type inference in AS graphs. We use our method to train classifiers for three AS graphs derived from different data sources--a BGP graph, a trace-route graph, and an IRR graph. The classifier annotated the edges into p2c and p2p edge types. We merge the three individual graphs to obtain a combined graph and propose a method to compute edge types in the combined graph. We analyze the characteristics of the three individual graphs and the combined graph and show that combining the three individual graphs gives us a significantly more complete view of both the p2p and p2c ecosystems in the Internet. We also present a method to compute the customer cones of peering networks using PCH data.</p>
<p>We conduct a case study of Netflix to understand the efficiency of the AS paths from various access ISPs to Netflix servers deployed at IXPs in different regions of the world. We discover inefficient AS paths in Europe, North America, and South America. Paths in South America are especially inefficient as many of them leave the continent. We also analyze long paths in each region, explore their causes, and propose ways to avoid long paths. We analyze the performance variation in accessing content on Akamai servers at different times of the day at different client and server locations by using active measurements. We measure the latency of paths from residential ISP users using RIPE Atlas probes, along with throughput and packet loss from a non-residential user using httping measurements. Based on our observations, we propose a server selection strategy that picks a low latency server or maximum throughput server based on predicted value of  latency or throughput that matches the actual value with an accuracy of over 98%. The optimum server based on the measured throughput or latency is not always the geographically closest server. We also observe that the Akamai server choice does not always pick the minimum latency nor maximum throughput server, and we outline ways to improve their strategy.</p>
<p>We try to make the process of gathering the serial hijacker ground truth easier than manually going through the available mailing list by using a document classifier that can classify sources of interest from which the serial hijacker information can be derived from. The resulting classifier can identify the document sources of such BGP hijacking information with 89% accuracy. We further examine how to create an end-to-end tool to extract serial BGP AS Hijackers by using a BGP Hijacker detector that has an accuracy of approximately 87%.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/94306,,"AS Relationships,AS Topology,BGP Hijack,Content Delivery,Internet Structure,Server Selection",Understanding the internet AS topology and its applications,dissertation
"Rajan, Hridesh,Miner, Andrew,Mitra, Simanta","Biswas, Sumon",N/A,"Machine learning models are increasingly being used in important decision-making software such as approving bank loans, recommending criminal sentencing, hiring employees, and so on. It is important to ensure the fairness of these models so that no discrimination is made based on protected attributes (e.g., race, sex, age) while decision making. Many such algorithmic fairness issues of machine learning software have been reported in the recent past. Research has been conducted to measure unfairness and mitigate that to a certain extent. What unfairness issues exist in open-source models and how do the mitigation techniques perform? In this thesis, we have focused on the empirical evaluation of fairness and mitigations on real-world machine learning models. We have created a benchmark of 40 top-rated models from Kaggle used for 5 different tasks, and then using a comprehensive set of fairness metrics, evaluated their fairness. Then, we have applied 7 mitigation techniques on these models and analyzed the fairness, mitigation results, and impacts on performance. We have found that some model optimization techniques result in inducing unfairness in the models. On the other hand, although there are some fairness control mechanisms in machine learning libraries, they are not documented. The mitigation algorithms also exhibit common patterns such as mitigation in the post-processing is often costly (in terms of performance) and mitigation in the pre-processing stage is preferred in most cases. We have also presented different trade-off choices of fairness mitigation decisions. Our study suggests future research directions to reduce the gap between theoretical fairness aware algorithms and the software engineering methods to leverage them in practice.",https://dr.lib.iastate.edu/handle/20.500.12876/WwPg1aOz,Computer science,"fairness,machine learning,models",Understanding unfairness and its mitigation in open-source machine learning models,thesis
Vasant Honavar,"Tu, Kewei",2018-08-12T01:26:09.000,"<p>Probabilistic grammars define joint probability distributions over sentences and their grammatical structures. They have been used in many areas, such as natural language processing, bioinformatics and pattern recognition, mainly for the purpose of deriving grammatical structures from data (sentences). Unsupervised approaches to learning probabilistic grammars induce a grammar from unannotated sentences, which eliminates the need for manual annotation of grammatical structures that can be laborious and error-prone. In this thesis we study unsupervised learning of probabilistic context-free grammars and probabilistic dependency grammars, both of which are expressive enough for many real-world languages but remain tractable in inference. We investigate three different approaches.</p>
<p>The first approach is a structure search approach for learning probabilistic context-free grammars. It acquires rules of an unknown probabilistic context-free grammar through iterative coherent biclustering of the bigrams in the training corpus. A greedy procedure is used in our approach to add rules from biclusters such that each set of rules being added into the grammar results in the largest increase in the posterior of the grammar given the training corpus. Our experiments on several benchmark datasets show that this approach is competitive with existing methods for unsupervised learning of context-free grammars.</p>
<p>The second approach is a parameter learning approach for learning natural language grammars based on the idea of unambiguity regularization. We make the observation that natural language is remarkably unambiguous in the sense that each natural language sentence has a large number of possible parses but only a few of the parses are syntactically valid. We incorporate this prior information into parameter learning by means of posterior regularization. The resulting algorithm family contains classic EM and Viterbi EM, as well as a novel softmax-EM algorithm that can be implemented with a simple and efficient extension to classic EM. Our experiments show that unambiguity regularization improves natural language grammar learning, and when combined with other techniques our approach achieves the state-of-the-art grammar learning results.</p>
<p>The third approach is grammar learning with a curriculum. A curriculum is a means of presenting training samples in a meaningful order. We introduce the incremental construction hypothesis that explains the benefits of a curriculum in learning grammars and offers some useful insights into the design of curricula as well as learning algorithms. We present results of experiments with (a) carefully crafted synthetic data that provide support for our hypothesis and (b) natural language corpus that demonstrate the utility of curricula in unsupervised learning of real-world probabilistic grammars.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/26677,Artificial Intelligence and Robotics,grammar induction,Unsupervised learning of probabilistic grammars,dissertation
Carl K. Chang,"Yang, Xinxin",2019-11-04T22:02:27.000,"<p>Software development decisions are made based on the information collected during the development process. Information delay is one of the major reasons that developers make wrong decisions. Slack time is important for development team to be creative and productive. Shortened information delay and properly allocated slack time can improve project performance. System dynamics modeling can provide insights at both macro and micro level. It includes quantitative representation, feedback and control system.</p>
<p>The development of the model was guided by hypothesis that employees make inappropriate decisions about work intensity because they receive inaccurate information about the project status. The effects of independent and dependent variables on the model have been analyzed separately.</p>
<p>The primary contribution of this research is to propose a strategy for organizations to handle the information delay and accelerate the availability of accurate information as simple as changing the behavior of the project. There is zero cost change through the process. All the changes can apply to a traditional model happen to be exactly the same change in Scrum.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31802,Computer Sciences,,Using a system dynamics simulation model to explore the effect of information delays on software development,thesis
Carl Chang,"Dong, Wei",2021-06-11T00:46:51.000,<p>Dr. Kim van Oorschot introduced a dynamic model to simulate Agile Software Development. This thesis study is aimed at applying the System Dynamics modeling technique to verify Oorschot’s simulation result to see if it corresponds well with what we can see happening in the real world. We found that the reported behaviors of several variables in Oorschot’s model do not actually reproduce the behavior of Agile software development in the real world. This research would help us build a more precise dynamic model to simulate Agile software development for further investigations.</p>,https://dr.lib.iastate.edu/handle/20.500.12876/GvqXyOEw,,"Agile,System Dynamics",Using a system dynamics simulation model to explore the validity of dynamics of Agile software development,thesis
Jin Tian,"Sharma, Abhineet",2018-08-11T18:23:08.000,"<p>In this thesis, I address an important problem of estimating the structure of Bayesian network models using Bayesian model averaging approach. Bayesian networks are probabilistic graphical models which are widely used for probabilistic inference and causal modeling. Learning the structure of Bayesian networks can reveal insights into the causal structure of the underlying domain. Owing to the super exponential structure space, it is a challenging task to find the most suitable network model that explains the data. The problem is worsened when the amount of available data is modest, as there might be numerous models with non negligible posterior. Therefore, we are interested in the calculation of posterior of a feature like presence of an edge from one particular node to another or a particular set being a parent of a specific node. The contribution of this thesis includes a Markov Chain Monte Carlo simulation approach to sample network structures from a posterior and then using Bayesian model averaging approach to estimate the posterior of various features.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/30197,Computer Sciences,"Bayesian model averaging,bayesian networks,structure learning in bayesian networks",Using node ordering to improve Structure MCMC for Bayesian Model Averaging,thesis
"Keren, Nir,Stone, Richard,Simpson, Stephen","Yrjo, Tavion",N/A,"Ensuring the safety of our workforce requires substantial training and education of 
workers. This thesis investigates the potential of priming students with gamification and 
entertainment techniques in a virtual environment to enhance the training for occupational noise 
risk assessments. A simulator titled AssessVR, a PC-based application was created to enhance 
the learning experience of students and safety workers in identifying hazards and assessing risks. 
This simulator enables students to engage in hazard recognition, develop evaluation strategies, 
and perform risk assessment and analysis by utilizing the data generated within the simulation. 
Two studies, a pilot and an enhanced study, were conducted to assess priming learners' 
effectiveness in enhancing occupational noise risk assessment. The results of the pilot study 
point to the need for enhancing the gamification of the simulator.

Consequently, AssessVR was redesigned as a serious game. In the second study, the 
gamified AssessVR was used to prime students before they engaged in the risk assessment with a 
non-gamified, AssessVR. An additional group of students was primed with a non-gamified 
AssessVR as a benchmark or the assessment of the gamification approach.

The results demonstrated that the gamified-based priming significantly improved 
students' exposure assessments, a critical component of risk assessments. In their assessment, 
students primed with a gamified approach referenced exposure standards more frequently than 
those in a traditional utilitarian group. However, not all risk assessment dimensions were 
improved through gamification.",https://dr.lib.iastate.edu/handle/20.500.12876/7wbOjRPv,"Occupational safety,Educational technology","Education,Priming,Risk Assessment,Safety Training,Serious Games,Virtual Reality",Using serious games to prime safety students for occupational noise exposure risk assessment,thesis
"Cohen, Myra B.,Lutz, Robyn R.,Miner, Andrew S.","Marsh, Alexis L",N/A,"While scientific software has become increasingly important to research, the ability to fully understand the behavior of these tools has remained limited. This is, in part, due to the difficulties in developing tests for these tools. Consider computational biology software. Not only do we often lack an oracle, there are multiple ways results deviating from the expected can arise. This includes bugs, issues with the input, or misunderstandings on the user end. Further, it is often the case that there are many open source tools available to perform the same task and it is difficult for users to determine what tools to use. Despite the existence of data-sharing principles such as FAIR, little attention has been paid to ensuring computational tools provide a platform to compare experimental results and data across different studies. Tools often are lightly documented, non-extensible, and provide different levels of accuracy in their representation. This lack of standardization and rigor reduces the potential power for new insight and discovery and makes it more difficult for biologists to experiment, compare, and trust results between different studies. To address these issues, we focused on a set of  four flux balance analysis tools, which are commonly used in both computational biology and metabolic engineering, and four models. We used both metamorphic and differential testing including tests focusing both on basic functionality and ones requiring domain knowledge.

We found not all tests could be run on all tools. Of the 3,738 metamorphic tests ran, we observed 1,185 failures.  We found nine different types of faults.  Further, even when using metamorphic-differential testing, we still saw high rates of failures (overall fail rate of 54.92%). Finally, we had multiple faults confirmed by developers.

Next, we evaluated the tools from a biologist (user) perspective using a set of seven models with a focus on the comparability between tools and a goal of developing a set of principles similar to FAIR (which are data sharing principles). In our study, we noted a range of differences including in the workflows, network metrics, and biological behavior. Using what we observed, we developed a set of principles we called CORE (Comparable, Open, Reliable and Extensible). Our goal is that these principles will make the tools more user-friendly (decreasing the number of tools that are re-made) as well as more biologically accurate.",https://dr.lib.iastate.edu/handle/20.500.12876/WwPgER7z,Bioinformatics,"Computational biology,Flux balance analysis,Software engineering,Software testing",Using software testing techniques for bioinformatics tools,thesis
Samik Basu,"Nalla, Shiva",2018-08-11T08:34:46.000,"<p>Asynchronous systems with message-passing communication paradigm have made major inroads in many application domains in service-oriented computing, secure and safe operating systems and in general, distributed systems. Asynchrony and concurrency in these systems bring in new challenges in verification of correctness properties. In particular, the high-level behavior of message-passing asynchronous systems is modeled as communicating finite-state machines (CFSMs) with unbounded communication buffers/channels. It has been proven that, in general, state-space exploration based automatic verification of CFSMs is undecidable - specifically, reachability and boundedness problems for CFSMs are undecidable. In this context, we focus on an important path-based property for CFSMs, namely well-formedness - every message sent can be eventually consumed. We show that well-formedness is undecidable as well, and present decidable sub-classes for which verification of well-formedness can be automated. We implemented the algorithm for verifying the well-formedness for the decidable subclass, and present our results using several case studies such as service choreographies and Singularity OS contracts.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/29570,Computer Sciences,"Asynchronous systems,automatic verification,boundedness,synchronizability,undecidability,well-formedness",Verification of Well-formedness in Message-Passing Asynchronous Systems modeled as Communicating Finite-State Machines,thesis
Hridesh Rajan,"Hanna, Youssef",2018-08-11T11:14:44.000,"<p>Verifying sensor network security protocol implementations using testing/simulation might leave some flaws undetected. Formal verification techniques have been very successful in detecting faults in security protocol specifications; however, they generally require building a formal description (model) of the protocol. Building accurate models is hard, thus hindering the application of formal verification. In this work, a framework for automating formal verification of sensor network security protocols is presented. The framework Slede extracts models from protocol implementations and verifies them against generated intruder models. Slede was evaluated by verifying two sensor network security protocol implementations. Security flaws in both protocols were detected.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/25377,Computer Sciences,"program verification,security protocols,sensor networks",Verifying sensor network security protocol implementations,thesis
N/A,"Gu, Xiaoyang",2020-08-05T05:01:50.000,"<p>Computer security has long been one of the most important research areas in computer science. In recent years, the rapid growth in Internet based industry has raised the importance of computer security to an unprecedented level. However, at the same time, profit driven commercial software development always leaves security concerns behind the quick incorporation of new functionalities. Therefore, the need to improve the security of these products is very urgent now. Microsoft Windows 2000, as one of the most popular operating systems, also needs to be improved. Especially, because of the unavailability of the necessary documentation and source code, few third party research and development have been done for Windows 2000 operating system kernel. In this paper, we introduce WinLomac, a prototype security enhancement software for Windows 2000 operating system that enforces Low Water Mark integrity model based Mandatory Access Control in the kernel.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/97232,,Computer science,WinLomac: Low Water Mark integrity protection for Windows 2000,thesis
Gianfranco Ciardo,"Jiang, Chuan",2019-08-21T12:49:32.000,"<p>Hardware and software systems are widely used in applications where failure is prohibitively costly or even unacceptable. The main obstacle to make such systems more reliable and capable of more complex and sensitive tasks is our limited ability to design and implement them with sufficiently high degree of confidence in their correctness under all circumstances. As an automated technique that verifies the system early in the design phase, model checking explores the state space of the system exhaustively and rigorously to determine if the system satisfies the specifications and detect fatal errors that may be missed by simulation and testing. One essential advantage of model checking is the capability to generate witnesses and counterexamples. They are simple and straightforward forms to prove an existential specification or falsify a universal specification. Beside enhancing the credibility of the model checker's conclusion, they either strengthen engineers' confidence in the system or provide hints to reveal potential defects.</p>
<p>In this dissertation, we focus on symbolic model checking with specifications expressed in computation tree logic (CTL), which describes branching-time behaviors of the system, and investigate the witness generation techniques for the existential fragment of CTL, i.e., ECTL, covering both decision-diagram-based and SAT-based.</p>
<p>Since witnesses provide important debugging information and may be inspected by engineers, smaller ones are always preferable to ease their interpretation and understanding. To the best of our knowledge, no existing witness generation technique guarantees the minimality for a general ECTL formula with nested existential CTL operators. One contribution of this dissertation is to fill this gap with the minimality guarantee. With the help of the saturation algorithm, our approach computes the minimum witness size for the given ECTL formula in every state, stored as an additive edge-valued multiway decision diagrams (EV+MDD), a variant of the well-known binary decision diagram (BDD), and then builds a minimum witness. Though computationally intensive, this has promising applications in reducing engineers' workload.</p>
<p>SAT-based model checking, in particular, bounded model checking, reduces a model checking problem problem into a satisfiability problem and leverages a SAT solver to solve it. Another contribution of this dissertation is to improve the translation of bounded semantics of ECTL into propositional formulas. By realizing the possibility of path reuse, i.e., a state may build its own witness by reusing its successor's, we may generate a significantly smaller formula, which is often easier for a SAT solver to answer, and thus boost the performance of bounded model checking.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/31216,Computer Sciences,"computation tree logic,decision diagram,formal verification,model checking,SAT solver,witness generation",Witness generation in existential CTL model checking,dissertation
"Ciardo, Gianfranco,Miner, Andrew,Basu, Samik,Aduri, Pavankumar,Lathrop, James","Meskell, Nathan Riley",N/A,"Binary decision diagrams (BDDs) have been a huge success story in hardware and software
verification. In various extended forms, they are increasingly applied to a wide range of
combinatorial problems. However, BDDs are a heuristic to encode Boolean functions of a fixed
number of Boolean variables, so they cannot work well (i.e., require polynomial space) in all cases.
We investigate the possible sizes, in particular the worst-case size and shape, of several BDD
variants, including those with complement or swap flags. This work expands upon work
previously done without complement or swap flags, and gives new combinatoric ways to count the
exact distribution of BDD encoding sizes. Finally, this paper explores the expansion of such
techniques into non-Boolean functions - specifically functions on the naturals",https://dr.lib.iastate.edu/handle/20.500.12876/Nveo5E2z,Computer science,"Binary Decision Diagrams,Combinatorics,Decision Diagrams,Dynamic Programming,Encoding Boolean Functions,Worst-Case Analysis",Worst-case and median encoding sizes of binary decision diagrams,thesis
N/A,"Lin, Yuan",2020-07-17T07:19:03.000,"<p>XML is a Markup language, designed for structure data exchange over the web. Its flexibility in representing heterogeneous data and its similarity to ASN.1 make it a good way to represent and store biology data. BIND is a biological database that has a version in XML format and the schema was designed in XML DTD. We redesigned the schema using XML Schema to show that using the new comer XML Schema for schema specification is much better. In order to extract useful information from large amount of heterogeneous biology data in XML, a good query language is essential. We presented a use case for querying meaningful biology information in BIND using the XML query language XQuery and showed how well the XQuery can be applied to query extremely complex biological data.</p>",https://dr.lib.iastate.edu/handle/20.500.12876/96846,,Computer science,XML schema of biomolecular interaction network database and an XQuery use case,thesis
"Jannesari, Ali,Li, Qi,Prabhu, Gurpur,Just, John","Sundareswaran, Ramakrishnan",N/A,"Unsupervised disentangled representation learning is a long-standing problem in computer vision. This thesis aims to tackle this problem and proposes a deep learning framework for performing image clustering. More specifically, this work proposes a novel framework for performing image clustering from deep embeddings by combining instance-level contrastive learning with a deep embedding based cluster center predictor. Our approach jointly learns representations and predicts cluster centers in an end-to-end manner. This is accomplished via a three-pronged approach that combines a clustering loss, an instance-wise contrastive loss, and an anchor loss. Our fundamental intuition is that using an ensemble loss that incorporates instance-level features and a clustering procedure focusing on semantic similarity reinforces learning better representations in the latent space. We observe that our method performs exceptionally well on popular vision datasets when evaluated using standard clustering metrics such as Normalized Mutual Information (NMI), in addition to producing geometrically well-separated cluster embeddings as defined by the Euclidean distance. Our framework performs on par with widely accepted clustering methods and outperforms the state-of-the-art contrastive learning method on the CIFAR-10 dataset with an NMI score of 0.772, a 7-8% improvement on the strong baseline.",https://dr.lib.iastate.edu/handle/20.500.12876/azJ4ybGv,"Computer science,Artificial intelligence","Contrastive Learning,Image Clustering,Machine Learning,Representation Learning,Self-supervised Learning",Yet another image clustering framework using deep learning,thesis
